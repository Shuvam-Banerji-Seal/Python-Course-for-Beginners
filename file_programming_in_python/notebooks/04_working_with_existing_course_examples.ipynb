{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Working with Existing Course Examples\n",
    "\n",
    "This notebook demonstrates the concepts from the original course materials, showing how they fit into our comprehensive understanding of file programming."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîÑ File Modes in Practice (from class01.py)\n",
    "\n",
    "Let's revisit the original examples and understand them better with our new knowledge:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import our examples module\n",
    "import sys\n",
    "sys.path.append('../examples')\n",
    "\n",
    "from basic_file_operations import (\n",
    "    demo_write_mode, demo_append_mode, demo_exclusive_mode,\n",
    "    demo_binary_write_mode, demo_read_plus_write_mode\n",
    ")\n",
    "\n",
    "# Run the original demonstrations with our enhanced understanding\n",
    "print(\"üîÑ Running Original Course Examples with Enhanced Understanding\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "demo_write_mode()\n",
    "demo_append_mode()\n",
    "demo_exclusive_mode()\n",
    "demo_binary_write_mode()\n",
    "demo_read_plus_write_mode()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä Performance Analysis (from class02.py)\n",
    "\n",
    "The original course included a performance comparison between pickle and text files. Let's expand on this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import json\n",
    "import time\n",
    "import os\n",
    "\n",
    "def enhanced_pickle_vs_text_comparison():\n",
    "    \"\"\"Enhanced version of the original pickle vs text comparison\"\"\"\n",
    "    \n",
    "    # Create more complex test data\n",
    "    test_data = {\n",
    "        'user_info': {\n",
    "            'name': 'John Doe',\n",
    "            'age': 30,\n",
    "            'email': 'john@example.com',\n",
    "            'preferences': ['python', 'data_science', 'machine_learning']\n",
    "        },\n",
    "        'session_data': {\n",
    "            'login_time': time.time(),\n",
    "            'actions': ['login', 'view_dashboard', 'edit_profile', 'logout'],\n",
    "            'settings': {'theme': 'dark', 'notifications': True}\n",
    "        },\n",
    "        'large_dataset': list(range(10000)),  # Simulate larger data\n",
    "        'metadata': {\n",
    "            'version': '2.1.0',\n",
    "            'created_by': 'file_programming_course',\n",
    "            'encoding': 'utf-8'\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    print(\"üìä Enhanced Pickle vs Text File Comparison\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Test pickle operations\n",
    "    pickle_file = '../sample_files/enhanced_data.pkl'\n",
    "    \n",
    "    # Pickle write\n",
    "    start_time = time.time()\n",
    "    with open(pickle_file, 'wb') as f:\n",
    "        pickle.dump(test_data, f)\n",
    "    pickle_write_time = time.time() - start_time\n",
    "    \n",
    "    # Pickle read\n",
    "    start_time = time.time()\n",
    "    with open(pickle_file, 'rb') as f:\n",
    "        pickle_loaded = pickle.load(f)\n",
    "    pickle_read_time = time.time() - start_time\n",
    "    \n",
    "    # Test JSON operations\n",
    "    json_file = '../sample_files/enhanced_data.json'\n",
    "    \n",
    "    # JSON write\n",
    "    start_time = time.time()\n",
    "    with open(json_file, 'w', encoding='utf-8') as f:\n",
    "        json.dump(test_data, f, indent=2)\n",
    "    json_write_time = time.time() - start_time\n",
    "    \n",
    "    # JSON read\n",
    "    start_time = time.time()\n",
    "    with open(json_file, 'r', encoding='utf-8') as f:\n",
    "        json_loaded = json.load(f)\n",
    "    json_read_time = time.time() - start_time\n",
    "    \n",
    "    # Get file sizes\n",
    "    pickle_size = os.path.getsize(pickle_file)\n",
    "    json_size = os.path.getsize(json_file)\n",
    "    \n",
    "    # Display results\n",
    "    print(f\"‚è±Ô∏è Performance Results:\")\n",
    "    print(f\"   Pickle write: {pickle_write_time:.6f} seconds\")\n",
    "    print(f\"   JSON write:   {json_write_time:.6f} seconds\")\n",
    "    print(f\"   Pickle read:  {pickle_read_time:.6f} seconds\")\n",
    "    print(f\"   JSON read:    {json_read_time:.6f} seconds\")\n",
    "    \n",
    "    print(f\"\\nüíæ File Size Comparison:\")\n",
    "    print(f\"   Pickle file: {pickle_size:,} bytes\")\n",
    "    print(f\"   JSON file:   {json_size:,} bytes\")\n",
    "    print(f\"   Size ratio:  {json_size/pickle_size:.2f}x (JSON vs Pickle)\")\n",
    "    \n",
    "    print(f\"\\nüîç Data Integrity Check:\")\n",
    "    print(f\"   Pickle data matches: {pickle_loaded == test_data}\")\n",
    "    print(f\"   JSON data matches:   {json_loaded == test_data}\")\n",
    "    \n",
    "    # Show the actual file contents (first few lines)\n",
    "    print(f\"\\nüìÑ File Content Preview:\")\n",
    "    print(f\"   Pickle (binary): {open(pickle_file, 'rb').read()[:50]}...\")\n",
    "    \n",
    "    with open(json_file, 'r', encoding='utf-8') as f:\n",
    "        json_preview = f.read()[:200]\n",
    "    print(f\"   JSON (text): {json_preview}...\")\n",
    "\n",
    "enhanced_pickle_vs_text_comparison()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîç File Seeking Operations (from class_03.py)\n",
    "\n",
    "The original course touched on file seeking. Let's explore this more thoroughly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from advanced_file_techniques import demonstrate_file_seeking, performance_comparison_with_seeking\n",
    "\n",
    "print(\"üéØ Enhanced File Seeking Demonstration\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Run the enhanced seeking demonstrations\n",
    "demonstrate_file_seeking()\n",
    "performance_comparison_with_seeking()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìù File Analysis Homework Solutions\n",
    "\n",
    "Let's solve the homework problems from the original course with proper implementations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from file_analysis_homework import (\n",
    "    count_words, count_lines, count_characters, count_sentences,\n",
    "    count_vowels, count_consonants, count_uppercase_letters,\n",
    "    count_digits, count_spaces, count_tabs, count_newlines,\n",
    "    count_punctuation, analyze_file_comprehensive\n",
    ")\n",
    "\n",
    "# Create a comprehensive test file\n",
    "test_content = \"\"\"Hello World! This is a comprehensive test file.\n",
    "It contains UPPERCASE and lowercase letters, numbers like 123 and 456.\n",
    "There are punctuation marks: periods, commas, exclamation points!\n",
    "Some lines have\\ttabs\\tand   multiple   spaces.\n",
    "We also have vowels (a, e, i, o, u) and consonants (b, c, d, f, g...).\n",
    "\n",
    "This file tests all our analysis functions.\n",
    "Can you count everything correctly? Let's see!\"\"\"\n",
    "\n",
    "test_file = '../sample_files/comprehensive_test.txt'\n",
    "with open(test_file, 'w', encoding='utf-8') as f:\n",
    "    f.write(test_content)\n",
    "\n",
    "print(\"üìä Comprehensive File Analysis (Homework Solutions)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Run comprehensive analysis\n",
    "analyze_file_comprehensive(test_file)\n",
    "\n",
    "print(\"\\nüéØ Individual Function Tests:\")\n",
    "functions_to_test = [\n",
    "    ('Words', count_words),\n",
    "    ('Lines', count_lines),\n",
    "    ('Characters', count_characters),\n",
    "    ('Sentences', count_sentences),\n",
    "    ('Vowels', count_vowels),\n",
    "    ('Consonants', count_consonants),\n",
    "    ('Uppercase', count_uppercase_letters),\n",
    "    ('Digits', count_digits),\n",
    "    ('Spaces', count_spaces),\n",
    "    ('Tabs', count_tabs),\n",
    "    ('Newlines', count_newlines),\n",
    "    ('Punctuation', count_punctuation)\n",
    "]\n",
    "\n",
    "for name, func in functions_to_test:\n",
    "    result = func(test_file)\n",
    "    print(f\"   {name:12}: {result:4,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üåü Demonstrating File Extension Independence\n",
    "\n",
    "Let's prove that file extensions don't define content, as mentioned in the original course:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def demonstrate_extension_independence():\n",
    "    \"\"\"Show that file extensions don't define content\"\"\"\n",
    "    \n",
    "    print(\"üé≠ File Extension Independence Demonstration\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Create the same content with different extensions\n",
    "    content = \"This is the same content, regardless of extension!\"\n",
    "    \n",
    "    files_with_different_extensions = [\n",
    "        '../sample_files/same_content.txt',\n",
    "        '../sample_files/same_content.random_world',\n",
    "        '../sample_files/same_content.fake_video',\n",
    "        '../sample_files/same_content.not_an_image',\n",
    "        '../sample_files/same_content.whatever'\n",
    "    ]\n",
    "    \n",
    "    # Write the same content to all files\n",
    "    for filepath in files_with_different_extensions:\n",
    "        with open(filepath, 'w', encoding='utf-8') as f:\n",
    "            f.write(content)\n",
    "    \n",
    "    print(\"‚úÖ Created files with identical content but different extensions:\")\n",
    "    \n",
    "    # Read and verify all files have the same content\n",
    "    for filepath in files_with_different_extensions:\n",
    "        with open(filepath, 'r', encoding='utf-8') as f:\n",
    "            read_content = f.read()\n",
    "        \n",
    "        extension = filepath.split('.')[-1]\n",
    "        matches = read_content == content\n",
    "        print(f\"   .{extension:15} ‚Üí Content matches: {matches} ‚úÖ\")\n",
    "    \n",
    "    print(\"\\nüí° Key Insight: The extension doesn't change the content!\")\n",
    "    print(\"   Programs determine file type by examining the actual content,\")\n",
    "    print(\"   not by trusting the extension.\")\n",
    "    \n",
    "    # Demonstrate with binary content too\n",
    "    print(\"\\nüî¢ Same demonstration with binary content:\")\n",
    "    \n",
    "    binary_content = b'\\x89PNG\\r\\n\\x1a\\n'  # PNG file signature\n",
    "    binary_files = [\n",
    "        '../sample_files/fake_png.txt',\n",
    "        '../sample_files/fake_png.doc',\n",
    "        '../sample_files/fake_png.mp3'\n",
    "    ]\n",
    "    \n",
    "    for filepath in binary_files:\n",
    "        with open(filepath, 'wb') as f:\n",
    "            f.write(binary_content)\n",
    "    \n",
    "    for filepath in binary_files:\n",
    "        with open(filepath, 'rb') as f:\n",
    "            read_binary = f.read()\n",
    "        \n",
    "        extension = filepath.split('.')[-1]\n",
    "        is_png_signature = read_binary.startswith(b'\\x89PNG')\n",
    "        print(f\"   .{extension:3} file ‚Üí Contains PNG signature: {is_png_signature} ‚úÖ\")\n",
    "    \n",
    "    print(\"\\nüéØ Conclusion: A file's content is independent of its extension!\")\n",
    "\n",
    "demonstrate_extension_independence()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîß Advanced Binary Operations\n",
    "\n",
    "Building on the binary file concepts from the original course:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from advanced_file_techniques import demonstrate_binary_operations, demonstrate_struct_operations\n",
    "\n",
    "print(\"üî¢ Advanced Binary File Operations\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Run binary demonstrations\n",
    "demonstrate_binary_operations()\n",
    "demonstrate_struct_operations()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìà Memory-Efficient File Processing\n",
    "\n",
    "For large files, we need efficient processing techniques:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def demonstrate_efficient_processing():\n",
    "    \"\"\"Show memory-efficient file processing techniques\"\"\"\n",
    "    \n",
    "    print(\"üöÄ Memory-Efficient File Processing\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    # Create a moderately large file for demonstration\n",
    "    large_file = '../sample_files/processing_demo.txt'\n",
    "    \n",
    "    print(\"üìù Creating demonstration file...\")\n",
    "    with open(large_file, 'w', encoding='utf-8') as f:\n",
    "        for i in range(5000):\n",
    "            f.write(f\"Line {i+1:04d}: This is sample content for processing demonstration.\\n\")\n",
    "    \n",
    "    file_size = os.path.getsize(large_file)\n",
    "    print(f\"‚úÖ Created file with {file_size:,} bytes\")\n",
    "    \n",
    "    # Method 1: Process line by line (memory efficient)\n",
    "    print(\"\\nüîÑ Method 1: Line-by-line processing (memory efficient)\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    line_count = 0\n",
    "    word_count = 0\n",
    "    char_count = 0\n",
    "    \n",
    "    with open(large_file, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            line_count += 1\n",
    "            word_count += len(line.split())\n",
    "            char_count += len(line)\n",
    "    \n",
    "    line_time = time.time() - start_time\n",
    "    \n",
    "    print(f\"   Processed {line_count:,} lines\")\n",
    "    print(f\"   Found {word_count:,} words\")\n",
    "    print(f\"   Counted {char_count:,} characters\")\n",
    "    print(f\"   Time taken: {line_time:.3f} seconds\")\n",
    "    \n",
    "    # Method 2: Process in chunks (for binary or when you need more control)\n",
    "    print(\"\\nüîÑ Method 2: Chunk-based processing\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    chunk_size = 4096  # 4KB chunks\n",
    "    total_bytes = 0\n",
    "    chunk_count = 0\n",
    "    \n",
    "    with open(large_file, 'rb') as f:\n",
    "        while True:\n",
    "            chunk = f.read(chunk_size)\n",
    "            if not chunk:\n",
    "                break\n",
    "            total_bytes += len(chunk)\n",
    "            chunk_count += 1\n",
    "    \n",
    "    chunk_time = time.time() - start_time\n",
    "    \n",
    "    print(f\"   Processed {chunk_count:,} chunks\")\n",
    "    print(f\"   Total bytes: {total_bytes:,}\")\n",
    "    print(f\"   Time taken: {chunk_time:.3f} seconds\")\n",
    "    \n",
    "    # Method 3: Generator-based processing (most memory efficient)\n",
    "    print(\"\\nüîÑ Method 3: Generator-based processing (most efficient)\")\n",
    "    \n",
    "    def process_file_generator(filepath):\n",
    "        \"\"\"Generator that yields processed lines\"\"\"\n",
    "        with open(filepath, 'r', encoding='utf-8') as f:\n",
    "            for line_num, line in enumerate(f, 1):\n",
    "                # Process each line and yield results\n",
    "                words = line.split()\n",
    "                yield {\n",
    "                    'line_number': line_num,\n",
    "                    'word_count': len(words),\n",
    "                    'char_count': len(line),\n",
    "                    'first_word': words[0] if words else ''\n",
    "                }\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    total_words = 0\n",
    "    total_chars = 0\n",
    "    processed_lines = 0\n",
    "    \n",
    "    for line_info in process_file_generator(large_file):\n",
    "        total_words += line_info['word_count']\n",
    "        total_chars += line_info['char_count']\n",
    "        processed_lines += 1\n",
    "        \n",
    "        # Show progress every 1000 lines\n",
    "        if processed_lines % 1000 == 0:\n",
    "            print(f\"   Processed {processed_lines:,} lines...\")\n",
    "    \n",
    "    generator_time = time.time() - start_time\n",
    "    \n",
    "    print(f\"   Final results: {processed_lines:,} lines, {total_words:,} words, {total_chars:,} chars\")\n",
    "    print(f\"   Time taken: {generator_time:.3f} seconds\")\n",
    "    \n",
    "    print(f\"\\nüìä Performance Summary:\")\n",
    "    print(f\"   Line-by-line: {line_time:.3f}s\")\n",
    "    print(f\"   Chunk-based:  {chunk_time:.3f}s\")\n",
    "    print(f\"   Generator:    {generator_time:.3f}s\")\n",
    "    \n",
    "    # Clean up\n",
    "    os.remove(large_file)\n",
    "    print(f\"\\nüßπ Cleaned up demonstration file\")\n",
    "\n",
    "demonstrate_efficient_processing()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéØ Key Takeaways from Original Course Integration\n",
    "\n",
    "1. **File modes matter** - Understanding when to use 'r', 'w', 'a', 'x', and their combinations\n",
    "2. **Performance considerations** - Pickle vs JSON, seeking vs full reads\n",
    "3. **File extensions are just hints** - Content determines file type, not extension\n",
    "4. **Memory efficiency** - Process large files line-by-line or in chunks\n",
    "5. **Binary vs text** - Know when to use each mode\n",
    "6. **Error handling** - Always use context managers and handle exceptions\n",
    "\n",
    "## üîú What's Next?\n",
    "\n",
    "The original course materials provide a solid foundation. In the next notebooks, we'll explore:\n",
    "- Modern file handling with `pathlib`\n",
    "- Working with structured data (CSV, JSON)\n",
    "- Advanced binary file formats\n",
    "- File system operations and management\n",
    "\n",
    "This integration shows how fundamental concepts build into advanced file programming skills!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}