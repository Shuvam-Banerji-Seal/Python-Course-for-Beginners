{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python Strings: The Basics\n",
    "\n",
    "A string in Python is a sequence of characters. They are **immutable**, meaning once a string is created, it cannot be changed. Any operation that appears to modify a string actually creates a new string.\n",
    "\n",
    "Strings can be created using single quotes (`'...'`), double quotes (`\"...\"`), or triple quotes (`'''...'''` or `\"\"\"...\"\"\"`). Triple quotes are useful for multi-line strings and docstrings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- String Introduction ---\n",
      "s1: Hello, World!\n",
      "s2: Python is fun.\n",
      "s3:\n",
      "This is a\n",
      "multi-line\n",
      "string.\n",
      "s4:\n",
      "Another\n",
      "multi-line\n",
      "string.\n",
      "Original string ID: 139988986137600\n",
      "New string ID: 139988986138080\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "# --- Introduction ---\n",
    "print(\"--- String Introduction ---\")\n",
    "s1 = 'Hello, World!'\n",
    "s2 = \"Python is fun.\"\n",
    "s3 = \"\"\"This is a\n",
    "multi-line\n",
    "string.\"\"\"\n",
    "s4 = '''Another\n",
    "multi-line\n",
    "string.'''\n",
    "\n",
    "print(f\"s1: {s1}\")\n",
    "print(f\"s2: {s2}\")\n",
    "print(f\"s3:\\n{s3}\") # \\n is needed here because print itself adds a newline for f-string content\n",
    "print(f\"s4:\\n{s4}\")\n",
    "\n",
    "# Immutability example\n",
    "my_string = \"apple\"\n",
    "print(f\"Original string ID: {id(my_string)}\")\n",
    "# my_string[0] = \"A\" # This would cause a TypeError: 'str' object does not support item assignment\n",
    "my_string = \"Apple\" # This creates a NEW string \"Apple\" and assigns it to my_string\n",
    "print(f\"New string ID: {id(my_string)}\")\n",
    "print(\"-\" * 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## String Operations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Concatenation (+)\n",
    "Joining two or more strings together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Concatenation ---\n",
      "Full name: Ada Lovelace\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "print(\"--- Concatenation ---\")\n",
    "first_name = \"Ada\"\n",
    "last_name = \"Lovelace\"\n",
    "full_name = first_name + \" \" + last_name\n",
    "print(f\"Full name: {full_name}\")\n",
    "print(\"-\" * 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Repetition (*)\n",
    "Repeating a string multiple times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Repetition ---\n",
      "Repeated: -_--_--_--_--_-\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "print(\"--- Repetition ---\")\n",
    "separator = \"-_-\"\n",
    "repeated_separator = separator * 5\n",
    "print(f\"Repeated: {repeated_separator}\")\n",
    "print(\"-\" * 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Membership (`in`, `not in`)\n",
    "Checking if a substring exists within a string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Membership ---\n",
      "'fox' in sentence: True\n",
      "'cat' in sentence: False\n",
      "'cat' not in sentence: True\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "print(\"--- Membership ---\")\n",
    "sentence = \"The quick brown fox jumps over the lazy dog.\"\n",
    "print(f\"'fox' in sentence: {'fox' in sentence}\")\n",
    "print(f\"'cat' in sentence: {'cat' in sentence}\")\n",
    "print(f\"'cat' not in sentence: {'cat' not in sentence}\")\n",
    "print(\"-\" * 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Slicing (`[start:end:step]`)\n",
    "Extracting a part of the string.\n",
    "*   `start`: The starting index (inclusive). Defaults to 0.\n",
    "*   `end`: The ending index (exclusive). Defaults to the end of the string.\n",
    "*   `step`: The increment. Defaults to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Slicing ---\n",
      "Original: PythonProgramming\n",
      "text[0]: P\n",
      "text[6]: P\n",
      "text[-1]: g\n",
      "text[0:6]: Python\n",
      "text[:6]: Python\n",
      "text[6:]: Programming\n",
      "text[-11:]: Programming\n",
      "text[::2]: PtoPormig\n",
      "text[::-1]: gnimmargorPnohtyP\n",
      "text[1:10:3]: yor\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "print(\"--- Slicing ---\")\n",
    "text = \"PythonProgramming\"\n",
    "# P y t h o n P r o g r a m m i n g\n",
    "# 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 (indices)\n",
    "#-7-6-5-4-3-2-1 (negative indices from end)\n",
    "\n",
    "print(f\"Original: {text}\")\n",
    "print(f\"text[0]: {text[0]}\")\n",
    "print(f\"text[6]: {text[6]}\")\n",
    "print(f\"text[-1]: {text[-1]}\")\n",
    "print(f\"text[0:6]: {text[0:6]}\")\n",
    "print(f\"text[:6]: {text[:6]}\")\n",
    "print(f\"text[6:]: {text[6:]}\")\n",
    "print(f\"text[-11:]: {text[-11:]}\")\n",
    "print(f\"text[::2]: {text[::2]}\")\n",
    "print(f\"text[::-1]: {text[::-1]}\")\n",
    "print(f\"text[1:10:3]: {text[1:10:3]}\")\n",
    "print(\"-\" * 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Traversing a String using Loops"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. `for` loop (character by character)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Traversing with for loop (char by char) ---\n",
      "L##o##o##p##i##n##g##\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "print(\"--- Traversing with for loop (char by char) ---\")\n",
    "word = \"Looping\"\n",
    "for char in word:\n",
    "    print(char, end=\"##\") # print each char followed by a space\n",
    "print(\"\\n\" + \"-\" * 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30330aec",
   "metadata": {},
   "source": [
    "# Python String Manipulation and NLTK Introduction\n",
    "\n",
    "This notebook covers fundamental Python string operations and provides an introduction to the Natural Language Toolkit (NLTK)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. `for` loop with `range` (index-based)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Traversing with for loop (index-based) ---\n",
      "Index 0: I\n",
      "Index 1: n\n",
      "Index 2: d\n",
      "Index 3: e\n",
      "Index 4: x\n",
      "Index 5: e\n",
      "Index 6: d\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "print(\"--- Traversing with for loop (index-based) ---\")\n",
    "word = \"Indexed\"\n",
    "for i in range(len(word)):\n",
    "    print(f\"Index {i}: {word[i]}\")\n",
    "print(\"-\" * 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. `while` loop (index-based)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Traversing with while loop ---\n",
      "W-h-i-l-e-I-t-\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "print(\"--- Traversing with while loop ---\")\n",
    "word = \"WhileIt\"\n",
    "index = 0\n",
    "while index < len(word):\n",
    "    print(word[index], end=\"-\")\n",
    "    index += 1\n",
    "print(\"\\n\" + \"-\" * 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Built-in Functions/Methods\n",
    "\n",
    "**Note:**\n",
    "*   **Functions** like `len()` are called directly on the object (e.g., `len(my_string)`).\n",
    "*   **Methods** are functions associated with an object and are called using dot notation (e.g., `my_string.upper()`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. `len()`\n",
    "Function: Returns the length of the string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- len() ---\n",
      "String: 'Hello', Length: 5\n"
     ]
    }
   ],
   "source": [
    "print(\"--- len() ---\")\n",
    "s = \"Hello\"\n",
    "print(f\"String: '{s}', Length: {len(s)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. `capitalize()`\n",
    "Method: Converts the first character to uppercase and the rest to lowercase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- capitalize() ---\n",
      "Original: 'python is fun.', Capitalized: 'Python is fun.'\n",
      "Original: 'PYTHON IS FUN.', Capitalized: 'Python is fun.'\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- capitalize() ---\")\n",
    "s = \"python is fun.\"\n",
    "print(f\"Original: '{s}', Capitalized: '{s.capitalize()}'\")\n",
    "s2 = \"PYTHON IS FUN.\"\n",
    "print(f\"Original: '{s2}', Capitalized: '{s2.capitalize()}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. `title()`\n",
    "Method: Converts the first character of each word to uppercase and others to lowercase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- title() ---\n",
      "Original: 'welcome to the jungle', Titled: 'Welcome To The Jungle'\n",
      "Original: 'they're bill's friends', Titled: 'They'Re Bill'S Friends'\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- title() ---\")\n",
    "s = \"welcome to the jungle\"\n",
    "print(f\"Original: '{s}', Titled: '{s.title()}'\")\n",
    "s2 = \"they're bill's friends\"\n",
    "print(f\"Original: '{s2}', Titled: '{s2.title()}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. `lower()`\n",
    "Method: Converts all characters to lowercase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- lower() ---\n",
      "Original: 'HELLO WORLD', Lowered: 'hello world'\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- lower() ---\")\n",
    "s = \"HELLO WORLD\"\n",
    "print(f\"Original: '{s}', Lowered: '{s.lower()}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. `upper()`\n",
    "Method: Converts all characters to uppercase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- upper() ---\n",
      "Original: 'hello world', Uppered: 'HELLO WORLD'\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- upper() ---\")\n",
    "s = \"hello world\"\n",
    "print(f\"Original: '{s}', Uppered: '{s.upper()}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. `count()`\n",
    "Method: Returns the number of non-overlapping occurrences of a substring.\n",
    "`count(substring, start=0, end=len(string))`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- count() ---\n",
      "String: 'abracadabra'\n",
      "Count of 'a': 5\n",
      "Count of 'abra': 2\n",
      "Count of 'a' from index 3: 4\n",
      "Count of 'a' from index 3 to 7: 2\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- count() ---\")\n",
    "s = \"abracadabra\"\n",
    "print(f\"String: '{s}'\")\n",
    "print(f\"Count of 'a': {s.count('a')}\")\n",
    "print(f\"Count of 'abra': {s.count('abra')}\")\n",
    "print(f\"Count of 'a' from index 3: {s.count('a', 3)}\")\n",
    "print(f\"Count of 'a' from index 3 to 7: {s.count('a', 3, 7)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. `find()`\n",
    "Method: Returns the lowest index of a substring. Returns -1 if not found.\n",
    "`find(substring, start=0, end=len(string))`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- find() ---\n",
      "String: 'hello world, world of wonders'\n",
      "Find 'world': 6\n",
      "Find 'world' after index 7: 13\n",
      "Find 'python': -1\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- find() ---\")\n",
    "s = \"hello world, world of wonders\"\n",
    "print(f\"String: '{s}'\")\n",
    "print(f\"Find 'world': {s.find('world')}\")\n",
    "print(f\"Find 'world' after index 7: {s.find('world', 7)}\")\n",
    "print(f\"Find 'python': {s.find('python')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. `index()`\n",
    "Method: Like `find()`, but raises a `ValueError` if the substring is not found.\n",
    "`index(substring, start=0, end=len(string))`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- index() ---\n",
      "String: 'hello world'\n",
      "Index of 'world': 6\n",
      "Error finding 'python': substring not found\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- index() ---\")\n",
    "s = \"hello world\"\n",
    "print(f\"String: '{s}'\")\n",
    "print(f\"Index of 'world': {s.index('world')}\")\n",
    "try:\n",
    "    print(f\"Index of 'python': {s.index('python')}\")\n",
    "except ValueError as e:\n",
    "    print(f\"Error finding 'python': {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. `endswith()`\n",
    "Method: Returns `True` if the string ends with the specified suffix.\n",
    "`endswith(suffix, start=0, end=len(string))`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- endswith() ---\n",
      "String: 'document.txt'\n",
      "Ends with '.txt': True\n",
      "Ends with 'doc': False\n",
      "Substring 'document.t' ends with '.t': True\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- endswith() ---\")\n",
    "filename = \"document.txt\"\n",
    "print(f\"String: '{filename}'\")\n",
    "print(f\"Ends with '.txt': {filename.endswith('.txt')}\")\n",
    "print(f\"Ends with 'doc': {filename.endswith('doc')}\")\n",
    "print(f\"Substring 'document.t' ends with '.t': {filename.endswith('.t', 0, 10)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10. `startswith()`\n",
    "Method: Returns `True` if the string starts with the specified prefix.\n",
    "`startswith(prefix, start=0, end=len(string))`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- startswith() ---\n",
      "String: 'https://www.example.com'\n",
      "Starts with 'https://': True\n",
      "Starts with 'www' from index 8: True\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- startswith() ---\")\n",
    "url = \"https://www.example.com\"\n",
    "print(f\"String: '{url}'\")\n",
    "print(f\"Starts with 'https://': {url.startswith('https://')}\")\n",
    "print(f\"Starts with 'www' from index 8: {url.startswith('www', 8)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11. `isalnum()`\n",
    "Method: Returns `True` if all characters are alphanumeric (letters or numbers) and there is at least one character."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- isalnum() ---\n",
      "'Python3'.isalnum(): True\n",
      "'Python 3'.isalnum(): False\n",
      "'Python@'.isalnum(): False\n",
      "''.isalnum(): False\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- isalnum() ---\")\n",
    "s1 = \"Python3\"\n",
    "s2 = \"Python 3\" # contains space\n",
    "s3 = \"Python@\"  # contains symbol\n",
    "s4 = \"\"         # empty\n",
    "print(f\"'{s1}'.isalnum(): {s1.isalnum()}\")\n",
    "print(f\"'{s2}'.isalnum(): {s2.isalnum()}\")\n",
    "print(f\"'{s3}'.isalnum(): {s3.isalnum()}\")\n",
    "print(f\"'{s4}'.isalnum(): {s4.isalnum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 12. `isalpha()`\n",
    "Method: Returns `True` if all characters are alphabetic and there is at least one character."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- isalpha() ---\n",
      "'Python'.isalpha(): True\n",
      "'Python3'.isalpha(): False\n",
      "'Python '.isalpha(): False\n",
      "''.isalpha(): False\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- isalpha() ---\")\n",
    "s1 = \"Python\"\n",
    "s2 = \"Python3\" # contains digit\n",
    "s3 = \"Python \" # contains space\n",
    "s4 = \"\"        # empty\n",
    "print(f\"'{s1}'.isalpha(): {s1.isalpha()}\")\n",
    "print(f\"'{s2}'.isalpha(): {s2.isalpha()}\")\n",
    "print(f\"'{s3}'.isalpha(): {s3.isalpha()}\")\n",
    "print(f\"'{s4}'.isalpha(): {s4.isalpha()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 13. `isdigit()`\n",
    "Method: Returns `True` if all characters are digits and there is at least one character."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- isdigit() ---\n",
      "'12345'.isdigit(): True\n",
      "'123.45'.isdigit(): False\n",
      "'123a'.isdigit(): False\n",
      "''.isdigit(): False\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- isdigit() ---\")\n",
    "s1 = \"12345\"\n",
    "s2 = \"123.45\" # contains period\n",
    "s3 = \"123a\"   # contains letter\n",
    "s4 = \"\"       # empty\n",
    "print(f\"'{s1}'.isdigit(): {s1.isdigit()}\")\n",
    "print(f\"'{s2}'.isdigit(): {s2.isdigit()}\")\n",
    "print(f\"'{s3}'.isdigit(): {s3.isdigit()}\")\n",
    "print(f\"'{s4}'.isdigit(): {s4.isdigit()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 14. `islower()`\n",
    "Method: Returns `True` if all cased characters are lowercase and there is at least one cased character."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- islower() ---\n",
      "'python'.islower(): True\n",
      "'Python'.islower(): False\n",
      "'python3'.islower(): True\n",
      "'PYTHON'.islower(): False\n",
      "'123'.islower(): False\n",
      "''.islower(): False\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- islower() ---\")\n",
    "s1 = \"python\"\n",
    "s2 = \"Python\"\n",
    "s3 = \"python3\"\n",
    "s4 = \"PYTHON\"\n",
    "s5 = \"123\"    # no cased characters\n",
    "s6 = \"\"       # empty\n",
    "print(f\"'{s1}'.islower(): {s1.islower()}\")\n",
    "print(f\"'{s2}'.islower(): {s2.islower()}\")\n",
    "print(f\"'{s3}'.islower(): {s3.islower()}\")\n",
    "print(f\"'{s4}'.islower(): {s4.islower()}\")\n",
    "print(f\"'{s5}'.islower(): {s5.islower()}\")\n",
    "print(f\"'{s6}'.islower(): {s6.islower()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 15. `isupper()`\n",
    "Method: Returns `True` if all cased characters are uppercase and there is at least one cased character."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- isupper() ---\n",
      "'PYTHON'.isupper(): True\n",
      "'Python'.isupper(): False\n",
      "'PYTHON3'.isupper(): True\n",
      "'python'.isupper(): False\n",
      "'123'.isupper(): False\n",
      "''.isupper(): False\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- isupper() ---\")\n",
    "s1 = \"PYTHON\"\n",
    "s2 = \"Python\"\n",
    "s3 = \"PYTHON3\"\n",
    "s4 = \"python\"\n",
    "s5 = \"123\"\n",
    "s6 = \"\"\n",
    "print(f\"'{s1}'.isupper(): {s1.isupper()}\")\n",
    "print(f\"'{s2}'.isupper(): {s2.isupper()}\")\n",
    "print(f\"'{s3}'.isupper(): {s3.isupper()}\")\n",
    "print(f\"'{s4}'.isupper(): {s4.isupper()}\")\n",
    "print(f\"'{s5}'.isupper(): {s5.isupper()}\")\n",
    "print(f\"'{s6}'.isupper(): {s6.isupper()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 16. `isspace()`\n",
    "Method: Returns `True` if all characters are whitespace and there is at least one character."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- isspace() ---\n",
      "' \t\n",
      "'.isspace(): True\n",
      "'  Hello  '.isspace(): False\n",
      "''.isspace(): False\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- isspace() ---\")\n",
    "s1 = \" \\t\\n\" # space, tab, newline\n",
    "s2 = \"  Hello  \"\n",
    "s3 = \"\"\n",
    "print(f\"'{s1}'.isspace(): {s1.isspace()}\")\n",
    "print(f\"'{s2}'.isspace(): {s2.isspace()}\")\n",
    "print(f\"'{s3}'.isspace(): {s3.isspace()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 17. `lstrip()`\n",
    "Method: Returns a copy of the string with leading whitespace removed.\n",
    "`lstrip(chars)` - Optional argument `chars` specifies a string containing characters to remove from the left."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- lstrip() ---\n",
      "Original: 'END   Hello World   END'\n",
      "lstrip(): 'ENDHello World   END'\n",
      "Original with chars: 'xyxxyyHello World'\n",
      "lstrip('xy'): 'Hello World'\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- lstrip() ---\")\n",
    "s = \"   Hello World   \"\n",
    "print(f\"Original: 'END{s}END'\")\n",
    "print(f\"lstrip(): 'END{s.lstrip()}END'\")\n",
    "\n",
    "s_chars = \"xyxxyyHello World\"\n",
    "print(f\"Original with chars: '{s_chars}'\")\n",
    "print(f\"lstrip('xy'): '{s_chars.lstrip('xy')}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 18. `rstrip()`\n",
    "Method: Returns a copy of the string with trailing whitespace removed.\n",
    "`rstrip(chars)` - Optional argument `chars` specifies a string containing characters to remove from the right."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- rstrip() ---\n",
      "Original: 'END   Hello World   END'\n",
      "rstrip(): 'END   Hello WorldEND'\n",
      "Original with chars: 'Hello Worldxyxxyy'\n",
      "rstrip('xy'): 'Hello World'\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- rstrip() ---\")\n",
    "s = \"   Hello World   \"\n",
    "print(f\"Original: 'END{s}END'\")\n",
    "print(f\"rstrip(): 'END{s.rstrip()}END'\")\n",
    "\n",
    "s_chars = \"Hello Worldxyxxyy\"\n",
    "print(f\"Original with chars: '{s_chars}'\")\n",
    "print(f\"rstrip('xy'): '{s_chars.rstrip('xy')}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 19. `strip()`\n",
    "Method: Returns a copy of the string with leading and trailing whitespace removed.\n",
    "`strip(chars)` - Optional argument `chars` specifies a string containing characters to remove from both ends."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n--- strip() ---\")\n",
    "s = \"   Hello World   \"\n",
    "print(f\"Original: 'END{s}END'\")\n",
    "print(f\"strip(): 'END{s.strip()}END'\")\n",
    "\n",
    "s_chars = \"xyxxyyHello Worldxyxxyy\"\n",
    "print(f\"Original with chars: '{s_chars}'\")\n",
    "print(f\"strip('xy'): '{s_chars.strip('xy')}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 20. `replace()`\n",
    "Method: Returns a copy with all occurrences of a substring replaced by another.\n",
    "`replace(old, new, count=-1)` - `count` is optional, max number of replacements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n--- replace() ---\")\n",
    "s = \"one two one three one four\"\n",
    "print(f\"Original: '{s}'\")\n",
    "print(f\"Replace 'one' with '1': '{s.replace('one', '1')}'\")\n",
    "print(f\"Replace 'one' with '1' (max 2 times): '{s.replace('one', '1', 2)}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 21. `join()`\n",
    "Method: Joins elements of an iterable (like a list of strings) into a single string, with the string itself as the separator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n--- join() ---\")\n",
    "words = [\"Hello\", \"World\", \"Python\"]\n",
    "separator = \" \"\n",
    "print(f\"Words: {words}, Separator: '{separator}'\")\n",
    "print(f\"Joined: '{separator.join(words)}'\")\n",
    "\n",
    "separator_dash = \"-\"\n",
    "print(f\"Words: {words}, Separator: '{separator_dash}'\")\n",
    "print(f\"Joined: '{separator_dash.join(words)}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 22. `partition()`\n",
    "Method: Splits the string at the first occurrence of a separator.\n",
    "Returns a 3-tuple: `(part_before_separator, separator, part_after_separator)`.\n",
    "If separator not found, returns `(original_string, '', '')`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n--- partition() ---\")\n",
    "s = \"name=value\"\n",
    "print(f\"String: '{s}'\")\n",
    "print(f\"Partition by '=': {s.partition('=')}\")\n",
    "\n",
    "s2 = \"no_separator_here\"\n",
    "print(f\"String: '{s2}'\")\n",
    "print(f\"Partition by '=': {s2.partition('=')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 23. `split()`\n",
    "Method: Splits the string into a list of substrings.\n",
    "`split(sep=None, maxsplit=-1)`\n",
    "*   If `sep` is not specified or `None`, splits by whitespace and discards empty strings.\n",
    "*   If `sep` is specified, splits by that separator.\n",
    "*   `maxsplit` is the maximum number of splits to do."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n--- split() ---\")\n",
    "s = \"apple banana cherry\"\n",
    "print(f\"String: '{s}'\")\n",
    "print(f\"Split by space (default): {s.split()}\")\n",
    "\n",
    "s_csv = \"apple,banana,cherry,date\"\n",
    "print(f\"String CSV: '{s_csv}'\")\n",
    "print(f\"Split by ',': {s_csv.split(',')}\")\n",
    "print(f\"Split by ',' (maxsplit=1): {s_csv.split(',', 1)}\")\n",
    "print(f\"Split by ',' (maxsplit=2): {s_csv.split(',', 2)}\")\n",
    "\n",
    "s_multi_space = \"word1  word2   word3\"\n",
    "print(f\"String multi-space: '{s_multi_space}'\")\n",
    "print(f\"Split (default): {s_multi_space.split()}\")\n",
    "print(\"-\" * 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca927a32",
   "metadata": {},
   "source": [
    "## Introduction to NLTK (Natural Language Toolkit)\n",
    "\n",
    "NLTK is a powerful Python library for working with human language data (text). It provides tools for many Natural Language Processing (NLP) tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installation\n",
    "\n",
    "First, you need to install NLTK if you haven't already. You can do this by running the following command in your terminal:\n",
    "```bash\n",
    "pip install nltk\n",
    "```\n",
    "Or, you can run the following cell (uncomment the line):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in /home/shuvam/codes/Python-Course-for-Beginners/.venv/lib/python3.13/site-packages (3.9.1)\n",
      "Requirement already satisfied: click in /home/shuvam/codes/Python-Course-for-Beginners/.venv/lib/python3.13/site-packages (from nltk) (8.2.1)\n",
      "Requirement already satisfied: joblib in /home/shuvam/codes/Python-Course-for-Beginners/.venv/lib/python3.13/site-packages (from nltk) (1.5.1)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /home/shuvam/codes/Python-Course-for-Beginners/.venv/lib/python3.13/site-packages (from nltk) (2024.11.6)\n",
      "Requirement already satisfied: tqdm in /home/shuvam/codes/Python-Course-for-Beginners/.venv/lib/python3.13/site-packages (from nltk) (4.67.1)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NLTK Resource Downloads\n",
    "\n",
    "After installation, NLTK requires you to download some data resources (corpora, models, etc.) for its modules to work. The cell below attempts to download necessary resources like 'punkt' (for tokenization), 'averaged_perceptron_tagger' (for POS tagging), 'stopwords', 'wordnet', and 'omw-1.4' (for lemmatization). \n",
    "\n",
    "**Note:** This might take a few moments the first time you run it. You generally only need to do this once per environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- NLTK Resource Check/Download ---\n",
      "Resource 'punkt' already downloaded.\n",
      "Resource 'averaged_perceptron_tagger' already downloaded.\n",
      "Resource 'stopwords' already downloaded.\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'nltk.downloader' has no attribute 'DownloadError'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mLookupError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 15\u001b[39m\n\u001b[32m     14\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m     \u001b[43mnltk\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresource_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     16\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mResource \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresource_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m already downloaded.\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/codes/Python-Course-for-Beginners/.venv/lib/python3.13/site-packages/nltk/data.py:579\u001b[39m, in \u001b[36mfind\u001b[39m\u001b[34m(resource_name, paths)\u001b[39m\n\u001b[32m    578\u001b[39m resource_not_found = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m579\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[31mLookupError\u001b[39m: \n**********************************************************************\n  Resource \u001b[93mwordnet\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('wordnet')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/wordnet\u001b[0m\n\n  Searched in:\n    - '/home/shuvam/nltk_data'\n    - '/home/shuvam/codes/Python-Course-for-Beginners/.venv/nltk_data'\n    - '/home/shuvam/codes/Python-Course-for-Beginners/.venv/share/nltk_data'\n    - '/home/shuvam/codes/Python-Course-for-Beginners/.venv/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 17\u001b[39m\n\u001b[32m     15\u001b[39m     nltk.data.find(resource_path)\n\u001b[32m     16\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mResource \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresource_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m already downloaded.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m17\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[43mnltk\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdownloader\u001b[49m\u001b[43m.\u001b[49m\u001b[43mDownloadError\u001b[49m:\n\u001b[32m     18\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mDownloading \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresource_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m resource for NLTK...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     19\u001b[39m     nltk.download(resource_name, quiet=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[31mAttributeError\u001b[39m: module 'nltk.downloader' has no attribute 'DownloadError'"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "print(\"--- NLTK Resource Check/Download ---\")\n",
    "\n",
    "resources_to_download = {\n",
    "    'punkt': 'tokenizers/punkt',\n",
    "    'averaged_perceptron_tagger': 'taggers/averaged_perceptron_tagger',\n",
    "    'stopwords': 'corpora/stopwords',\n",
    "    'wordnet': 'corpora/wordnet',\n",
    "    'omw-1.4': 'corpora/omw-1.4' # For WordNetLemmatizer, Open Multilingual Wordnet\n",
    "}\n",
    "\n",
    "for resource_name, resource_path in resources_to_download.items():\n",
    "    try:\n",
    "        nltk.data.find(resource_path)\n",
    "        print(f\"Resource '{resource_name}' already downloaded.\")\n",
    "    except nltk.downloader.DownloadError:\n",
    "        print(f\"Downloading '{resource_name}' resource for NLTK...\")\n",
    "        nltk.download(resource_name, quiet=True)\n",
    "        print(f\"'{resource_name}' downloaded.\")\n",
    "    except Exception as e: # Catch any other lookup errors\n",
    "        print(f\"Could not find or download '{resource_name}'. Error: {e}\")\n",
    "        print(f\"Attempting download directly for '{resource_name}'...\")\n",
    "        try:\n",
    "            nltk.download(resource_name, quiet=True)\n",
    "            print(f\"'{resource_name}' downloaded successfully after retry.\")\n",
    "        except Exception as download_e:\n",
    "            print(f\"Failed to download '{resource_name}' even after retry. Error: {download_e}\")\n",
    "\n",
    "print(\"NLTK resources check/download process completed.\")\n",
    "print(\"-\" * 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1129e12c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/shuvam/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic NLTK Operations\n",
    "\n",
    "Let's look at a few common NLP tasks using NLTK:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Tokenization\n",
    "Breaking text into smaller units (words or sentences)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- NLTK: Tokenization ---\n",
      "Sentences:\n",
      "1. Hello Mr. Smith, how are you doing today?\n",
      "2. The weather is great, and Python is awesome.\n",
      "3. The sky is pinkish-blue.\n",
      "\n",
      "Words (lowercase):\n",
      "['hello', 'mr.', 'smith', ',', 'how', 'are', 'you', 'doing', 'today', '?', 'the', 'weather', 'is', 'great', ',', 'and', 'python', 'is', 'awesome', '.', 'the', 'sky', 'is', 'pinkish-blue', '.']\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "print(\"\\n--- NLTK: Tokenization ---\")\n",
    "text_sample = \"Hello Mr. Smith, how are you doing today? The weather is great, and Python is awesome. The sky is pinkish-blue.\"\n",
    "\n",
    "# Sentence Tokenization\n",
    "sentences = sent_tokenize(text_sample)\n",
    "print(\"Sentences:\")\n",
    "for i, sentence in enumerate(sentences):\n",
    "    print(f\"{i+1}. {sentence}\")\n",
    "\n",
    "# Word Tokenization\n",
    "words = word_tokenize(text_sample.lower()) # Often good to lowercase first\n",
    "print(\"\\nWords (lowercase):\")\n",
    "print(words)\n",
    "print(\"-\" * 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Stop Word Removal\n",
    "Removing common words (like \"the\", \"is\", \"in\") that often don't carry significant meaning for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- NLTK: Stop Word Removal ---\n",
      "Original words: ['hello', 'mr.', 'smith', ',', 'how', 'are', 'you', 'doing', 'today', '?', 'the', 'weather', 'is', 'great', ',', 'and', 'python', 'is', 'awesome', '.', 'the', 'sky', 'is', 'pinkish-blue', '.', 'duytika', 'has', 'made', 'a', 'well', 'rounded', 'python', 'code', 'to', 'compute', 'the', 'determinants', 'for', '2x2', 'and', '3x3', 'matrices', 'along', 'with', 'the', 'implementation', 'of', 'the', 'cramer', '’', 's', 'rule', '.']\n",
      "Stop words (English sample - first 10): ['had', \"wasn't\", 'own', 'd', \"i'll\", 'weren', \"shan't\", 'be', 's', \"mustn't\"]\n",
      "Filtered words: ['hello', 'smith', 'today', 'weather', 'great', 'python', 'awesome', 'sky', 'duytika', 'made', 'well', 'rounded', 'python', 'code', 'compute', 'determinants', '2x2', '3x3', 'matrices', 'along', 'implementation', 'cramer', 'rule']\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "print(\"\\n--- NLTK: Stop Word Removal ---\")\n",
    "stop_words_english = set(stopwords.words('english')) # Use a set for faster lookups\n",
    "# You can add custom stop words:\n",
    "# stop_words_english.update(['mr.', ',', '?']) # Example\n",
    "\n",
    "# Using the text_sample from the previous cell\n",
    "text_sample = \"Hello Mr. Smith, how are you doing today? The weather is great, and Python is awesome. The sky is pinkish-blue. Duytika has made a well rounded python code to compute the determinants for 2x2 and 3x3 matrices along with the implementation of the Cramer’s rule.\"\n",
    "words_from_sample = word_tokenize(text_sample.lower())\n",
    "\n",
    "filtered_words = [word for word in words_from_sample if word.isalnum() and word not in stop_words_english]\n",
    "# isalnum() helps remove punctuation tokens as well\n",
    "\n",
    "print(\"Original words:\", words_from_sample)\n",
    "print(\"Stop words (English sample - first 10):\", list(stop_words_english)[:10])\n",
    "print(\"Filtered words:\", filtered_words)\n",
    "print(\"-\" * 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Stemming\n",
    "Reducing words to their root or stem form. It's a crude heuristic process that chops off word endings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- NLTK: Stemming ---\n",
      "Original: ['program', 'programs', 'programmer', 'programming', 'programmers', 'running', 'flies', 'democracy']\n",
      "Stemmed (Porter): ['program', 'program', 'programm', 'program', 'programm', 'run', 'fli', 'democraci']\n",
      "\n",
      "Stemmed filtered words from sample:\n",
      "['hello', 'smith', 'today', 'weather', 'great', 'python', 'awesom', 'sky', 'duytika', 'made', 'well', 'round', 'python', 'code', 'comput', 'determin', '2x2', '3x3', 'matric', 'along', 'implement', 'cramer', 'rule']\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "print(\"\\n--- NLTK: Stemming ---\")\n",
    "ps = PorterStemmer()\n",
    "words_to_stem = [\"program\", \"programs\", \"programmer\", \"programming\", \"programmers\", \"running\", \"flies\", \"democracy\"]\n",
    "stemmed_words = [ps.stem(w) for w in words_to_stem]\n",
    "\n",
    "print(f\"Original: {words_to_stem}\")\n",
    "print(f\"Stemmed (Porter): {stemmed_words}\")\n",
    "\n",
    "# Example with our filtered words from the previous cell\n",
    "# (Assuming 'filtered_words' is available from the stop word removal cell)\n",
    "if 'filtered_words' in locals(): # Check if filtered_words exists\n",
    "    stemmed_filtered = [ps.stem(w) for w in filtered_words]\n",
    "    print(\"\\nStemmed filtered words from sample:\")\n",
    "    print(stemmed_filtered)\n",
    "else:\n",
    "    print(\"\\nRun the 'Stop Word Removal' cell first to generate 'filtered_words' for this example.\")\n",
    "print(\"-\" * 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Lemmatization\n",
    "Similar to stemming, but it reduces words to their actual dictionary form (lemma). It's more sophisticated as it considers the part of speech (POS). By default, WordNetLemmatizer assumes words are nouns if no POS tag is provided."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- NLTK: Lemmatization ---\n",
      "Original: ['cats', 'cacti', 'geese', 'rocks', 'running', 'better', 'ate']\n",
      "Lemmatized (default POS - noun): ['cat', 'cactus', 'goose', 'rock', 'running', 'better', 'ate']\n",
      "'running' (as verb): run\n",
      "'ate' (as verb): eat\n",
      "'better' (as adjective): good\n",
      "'better' (as adverb): well\n",
      "\n",
      "Lemmatized filtered words from sample (default POS):\n",
      "['hello', 'smith', 'today', 'weather', 'great', 'python', 'awesome', 'sky', 'duytika', 'made', 'well', 'rounded', 'python', 'code', 'compute', 'determinant', '2x2', '3x3', 'matrix', 'along', 'implementation', 'cramer', 'rule']\n",
      "\n",
      "Lemmatized filtered words from sample (with POS tags):\n",
      "['hello', 'smith', 'today', 'weather', 'great', 'python', 'awesome', 'sky', 'duytika', 'make', 'well', 'rounded', 'python', 'code', 'compute', 'determinants', '2x2', '3x3', 'matrix', 'along', 'implementation', 'cramer', 'rule']\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "# NLTK's WordNetLemmatizer needs a POS tag that is compatible with WordNet's POS tags.\n",
    "# We can create a helper function to map NLTK's POS tags (from pos_tag) to WordNet tags.\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "def get_wordnet_pos(treebank_tag):\n",
    "    \"\"\"Converts treebank POS tags to WordNet POS tags.\"\"\"\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN # Default to noun if no mapping found\n",
    "\n",
    "print(\"\\n--- NLTK: Lemmatization ---\")\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "words_to_lemmatize = [\"cats\", \"cacti\", \"geese\", \"rocks\", \"running\", \"better\", \"ate\"]\n",
    "\n",
    "print(f\"Original: {words_to_lemmatize}\")\n",
    "\n",
    "# Lemmatizing without POS tag (defaults to noun)\n",
    "lemmatized_default_pos = [lemmatizer.lemmatize(w) for w in words_to_lemmatize]\n",
    "print(f\"Lemmatized (default POS - noun): {lemmatized_default_pos}\")\n",
    "\n",
    "# Lemmatizing with specific POS tags\n",
    "print(f\"'running' (as verb): {lemmatizer.lemmatize('running', pos='v')}\")\n",
    "print(f\"'ate' (as verb): {lemmatizer.lemmatize('ate', pos='v')}\")\n",
    "print(f\"'better' (as adjective): {lemmatizer.lemmatize('better', pos='a')}\")\n",
    "print(f\"'better' (as adverb): {lemmatizer.lemmatize('better', pos='r')}\")\n",
    "\n",
    "# Example with our filtered words (defaulting to noun)\n",
    "# (Assuming 'filtered_words' is available from the stop word removal cell)\n",
    "if 'filtered_words' in locals():\n",
    "    lemmatized_filtered_default = [lemmatizer.lemmatize(w) for w in filtered_words]\n",
    "    print(\"\\nLemmatized filtered words from sample (default POS):\")\n",
    "    print(lemmatized_filtered_default)\n",
    "    \n",
    "    # For more accurate lemmatization, use POS tags\n",
    "    # First, get POS tags for the filtered words (using nltk.pos_tag)\n",
    "    # Note: pos_tag works best on original sentence structure, not just filtered words.\n",
    "    # This is just a demonstration on the filtered list.\n",
    "    # For a real task, you'd POS tag before stopword removal or on tokenized sentences.\n",
    "    tagged_filtered_words = nltk.pos_tag(filtered_words) \n",
    "    lemmatized_filtered_with_pos = [lemmatizer.lemmatize(word, get_wordnet_pos(tag)) for word, tag in tagged_filtered_words]\n",
    "    print(\"\\nLemmatized filtered words from sample (with POS tags):\")\n",
    "    print(lemmatized_filtered_with_pos)\n",
    "else:\n",
    "    print(\"\\nRun the 'Stop Word Removal' cell first to generate 'filtered_words' for this example.\")\n",
    "\n",
    "print(\"-\" * 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "abb47a57",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package tagsets_json to\n",
      "[nltk_data]     /home/shuvam/nltk_data...\n",
      "[nltk_data]   Unzipping help/tagsets_json.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('tagsets_json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Part-of-Speech (POS) Tagging\n",
    "Assigning grammatical tags (noun, verb, adjective, etc.) to each word. NLTK's `pos_tag` uses the Penn Treebank tagset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- NLTK: Part-of-Speech Tagging ---\n",
      "Tokens: ['The', 'quick', 'brown', 'fox', 'jumps', 'over', 'the', 'lazy', 'dog', '.', 'Duytika', 'is', 'a', 'good', 'programmer', '.', 'Duytika', 'has', 'made', 'a', 'well', 'rounded', 'python', 'code', 'to', 'compute', 'the', 'determinants', 'for', '2x2', 'and', '3x3', 'matrices', 'along', 'with', 'the', 'implementation', 'of', 'the', 'Cramer', '’', 's', 'rule', '.']\n",
      "POS Tags:\n",
      "DT: determiner\n",
      "    all an another any both del each either every half la many much nary\n",
      "    neither no some such that the them these this those\n",
      "('The', 'DT') - No description available\n",
      "JJ: adjective or numeral, ordinal\n",
      "    third ill-mannered pre-war regrettable oiled calamitous first separable\n",
      "    ectoplasmic battery-powered participatory fourth still-to-be-named\n",
      "    multilingual multi-disciplinary ...\n",
      "('quick', 'JJ') - No description available\n",
      "NN: noun, common, singular or mass\n",
      "    common-carrier cabbage knuckle-duster Casino afghan shed thermostat\n",
      "    investment slide humour falloff slick wind hyena override subhumanity\n",
      "    machinist ...\n",
      "('brown', 'NN') - No description available\n",
      "NN: noun, common, singular or mass\n",
      "    common-carrier cabbage knuckle-duster Casino afghan shed thermostat\n",
      "    investment slide humour falloff slick wind hyena override subhumanity\n",
      "    machinist ...\n",
      "('fox', 'NN') - No description available\n",
      "VBZ: verb, present tense, 3rd person singular\n",
      "    bases reconstructs marks mixes displeases seals carps weaves snatches\n",
      "    slumps stretches authorizes smolders pictures emerges stockpiles\n",
      "    seduces fizzes uses bolsters slaps speaks pleads ...\n",
      "('jumps', 'VBZ') - No description available\n",
      "IN: preposition or conjunction, subordinating\n",
      "    astride among uppon whether out inside pro despite on by throughout\n",
      "    below within for towards near behind atop around if like until below\n",
      "    next into if beside ...\n",
      "('over', 'IN') - No description available\n",
      "DT: determiner\n",
      "    all an another any both del each either every half la many much nary\n",
      "    neither no some such that the them these this those\n",
      "('the', 'DT') - No description available\n",
      "JJ: adjective or numeral, ordinal\n",
      "    third ill-mannered pre-war regrettable oiled calamitous first separable\n",
      "    ectoplasmic battery-powered participatory fourth still-to-be-named\n",
      "    multilingual multi-disciplinary ...\n",
      "('lazy', 'JJ') - No description available\n",
      "NN: noun, common, singular or mass\n",
      "    common-carrier cabbage knuckle-duster Casino afghan shed thermostat\n",
      "    investment slide humour falloff slick wind hyena override subhumanity\n",
      "    machinist ...\n",
      "('dog', 'NN') - No description available\n",
      ".: sentence terminator\n",
      "    . ! ?\n",
      "('.', '.') - No description available\n",
      "NNP: noun, proper, singular\n",
      "    Motown Venneboerger Czestochwa Ranzer Conchita Trumplane Christos\n",
      "    Oceanside Escobar Kreisler Sawyer Cougar Yvette Ervin ODI Darryl CTCA\n",
      "    Shannon A.K.C. Meltex Liverpool ...\n",
      "('Duytika', 'NNP') - No description available\n",
      "VBZ: verb, present tense, 3rd person singular\n",
      "    bases reconstructs marks mixes displeases seals carps weaves snatches\n",
      "    slumps stretches authorizes smolders pictures emerges stockpiles\n",
      "    seduces fizzes uses bolsters slaps speaks pleads ...\n",
      "('is', 'VBZ') - No description available\n",
      "DT: determiner\n",
      "    all an another any both del each either every half la many much nary\n",
      "    neither no some such that the them these this those\n",
      "('a', 'DT') - No description available\n",
      "JJ: adjective or numeral, ordinal\n",
      "    third ill-mannered pre-war regrettable oiled calamitous first separable\n",
      "    ectoplasmic battery-powered participatory fourth still-to-be-named\n",
      "    multilingual multi-disciplinary ...\n",
      "('good', 'JJ') - No description available\n",
      "NN: noun, common, singular or mass\n",
      "    common-carrier cabbage knuckle-duster Casino afghan shed thermostat\n",
      "    investment slide humour falloff slick wind hyena override subhumanity\n",
      "    machinist ...\n",
      "('programmer', 'NN') - No description available\n",
      ".: sentence terminator\n",
      "    . ! ?\n",
      "('.', '.') - No description available\n",
      "NNP: noun, proper, singular\n",
      "    Motown Venneboerger Czestochwa Ranzer Conchita Trumplane Christos\n",
      "    Oceanside Escobar Kreisler Sawyer Cougar Yvette Ervin ODI Darryl CTCA\n",
      "    Shannon A.K.C. Meltex Liverpool ...\n",
      "('Duytika', 'NNP') - No description available\n",
      "VBZ: verb, present tense, 3rd person singular\n",
      "    bases reconstructs marks mixes displeases seals carps weaves snatches\n",
      "    slumps stretches authorizes smolders pictures emerges stockpiles\n",
      "    seduces fizzes uses bolsters slaps speaks pleads ...\n",
      "('has', 'VBZ') - No description available\n",
      "VBN: verb, past participle\n",
      "    multihulled dilapidated aerosolized chaired languished panelized used\n",
      "    experimented flourished imitated reunifed factored condensed sheared\n",
      "    unsettled primed dubbed desired ...\n",
      "('made', 'VBN') - No description available\n",
      "DT: determiner\n",
      "    all an another any both del each either every half la many much nary\n",
      "    neither no some such that the them these this those\n",
      "('a', 'DT') - No description available\n",
      "RB: adverb\n",
      "    occasionally unabatingly maddeningly adventurously professedly\n",
      "    stirringly prominently technologically magisterially predominately\n",
      "    swiftly fiscally pitilessly ...\n",
      "('well', 'RB') - No description available\n",
      "JJ: adjective or numeral, ordinal\n",
      "    third ill-mannered pre-war regrettable oiled calamitous first separable\n",
      "    ectoplasmic battery-powered participatory fourth still-to-be-named\n",
      "    multilingual multi-disciplinary ...\n",
      "('rounded', 'JJ') - No description available\n",
      "NN: noun, common, singular or mass\n",
      "    common-carrier cabbage knuckle-duster Casino afghan shed thermostat\n",
      "    investment slide humour falloff slick wind hyena override subhumanity\n",
      "    machinist ...\n",
      "('python', 'NN') - No description available\n",
      "NN: noun, common, singular or mass\n",
      "    common-carrier cabbage knuckle-duster Casino afghan shed thermostat\n",
      "    investment slide humour falloff slick wind hyena override subhumanity\n",
      "    machinist ...\n",
      "('code', 'NN') - No description available\n",
      "TO: \"to\" as preposition or infinitive marker\n",
      "    to\n",
      "('to', 'TO') - No description available\n",
      "VB: verb, base form\n",
      "    ask assemble assess assign assume atone attention avoid bake balkanize\n",
      "    bank begin behold believe bend benefit bevel beware bless boil bomb\n",
      "    boost brace break bring broil brush build ...\n",
      "('compute', 'VB') - No description available\n",
      "DT: determiner\n",
      "    all an another any both del each either every half la many much nary\n",
      "    neither no some such that the them these this those\n",
      "('the', 'DT') - No description available\n",
      "NNS: noun, common, plural\n",
      "    undergraduates scotches bric-a-brac products bodyguards facets coasts\n",
      "    divestitures storehouses designs clubs fragrances averages\n",
      "    subjectivists apprehensions muses factory-jobs ...\n",
      "('determinants', 'NNS') - No description available\n",
      "IN: preposition or conjunction, subordinating\n",
      "    astride among uppon whether out inside pro despite on by throughout\n",
      "    below within for towards near behind atop around if like until below\n",
      "    next into if beside ...\n",
      "('for', 'IN') - No description available\n",
      "CD: numeral, cardinal\n",
      "    mid-1890 nine-thirty forty-two one-tenth ten million 0.5 one forty-\n",
      "    seven 1987 twenty '79 zero two 78-degrees eighty-four IX '60s .025\n",
      "    fifteen 271,124 dozen quintillion DM2,000 ...\n",
      "('2x2', 'CD') - No description available\n",
      "CC: conjunction, coordinating\n",
      "    & 'n and both but either et for less minus neither nor or plus so\n",
      "    therefore times v. versus vs. whether yet\n",
      "('and', 'CC') - No description available\n",
      "CD: numeral, cardinal\n",
      "    mid-1890 nine-thirty forty-two one-tenth ten million 0.5 one forty-\n",
      "    seven 1987 twenty '79 zero two 78-degrees eighty-four IX '60s .025\n",
      "    fifteen 271,124 dozen quintillion DM2,000 ...\n",
      "('3x3', 'CD') - No description available\n",
      "NNS: noun, common, plural\n",
      "    undergraduates scotches bric-a-brac products bodyguards facets coasts\n",
      "    divestitures storehouses designs clubs fragrances averages\n",
      "    subjectivists apprehensions muses factory-jobs ...\n",
      "('matrices', 'NNS') - No description available\n",
      "IN: preposition or conjunction, subordinating\n",
      "    astride among uppon whether out inside pro despite on by throughout\n",
      "    below within for towards near behind atop around if like until below\n",
      "    next into if beside ...\n",
      "('along', 'IN') - No description available\n",
      "IN: preposition or conjunction, subordinating\n",
      "    astride among uppon whether out inside pro despite on by throughout\n",
      "    below within for towards near behind atop around if like until below\n",
      "    next into if beside ...\n",
      "('with', 'IN') - No description available\n",
      "DT: determiner\n",
      "    all an another any both del each either every half la many much nary\n",
      "    neither no some such that the them these this those\n",
      "('the', 'DT') - No description available\n",
      "NN: noun, common, singular or mass\n",
      "    common-carrier cabbage knuckle-duster Casino afghan shed thermostat\n",
      "    investment slide humour falloff slick wind hyena override subhumanity\n",
      "    machinist ...\n",
      "('implementation', 'NN') - No description available\n",
      "IN: preposition or conjunction, subordinating\n",
      "    astride among uppon whether out inside pro despite on by throughout\n",
      "    below within for towards near behind atop around if like until below\n",
      "    next into if beside ...\n",
      "('of', 'IN') - No description available\n",
      "DT: determiner\n",
      "    all an another any both del each either every half la many much nary\n",
      "    neither no some such that the them these this those\n",
      "('the', 'DT') - No description available\n",
      "NNP: noun, proper, singular\n",
      "    Motown Venneboerger Czestochwa Ranzer Conchita Trumplane Christos\n",
      "    Oceanside Escobar Kreisler Sawyer Cougar Yvette Ervin ODI Darryl CTCA\n",
      "    Shannon A.K.C. Meltex Liverpool ...\n",
      "('Cramer', 'NNP') - No description available\n",
      "NNP: noun, proper, singular\n",
      "    Motown Venneboerger Czestochwa Ranzer Conchita Trumplane Christos\n",
      "    Oceanside Escobar Kreisler Sawyer Cougar Yvette Ervin ODI Darryl CTCA\n",
      "    Shannon A.K.C. Meltex Liverpool ...\n",
      "('’', 'NNP') - No description available\n",
      "NN: noun, common, singular or mass\n",
      "    common-carrier cabbage knuckle-duster Casino afghan shed thermostat\n",
      "    investment slide humour falloff slick wind hyena override subhumanity\n",
      "    machinist ...\n",
      "('s', 'NN') - No description available\n",
      "NN: noun, common, singular or mass\n",
      "    common-carrier cabbage knuckle-duster Casino afghan shed thermostat\n",
      "    investment slide humour falloff slick wind hyena override subhumanity\n",
      "    machinist ...\n",
      "('rule', 'NN') - No description available\n",
      ".: sentence terminator\n",
      "    . ! ?\n",
      "('.', '.') - No description available\n",
      "\n",
      "Nouns from the sentence: ['brown', 'fox', 'dog', 'Duytika', 'programmer', 'Duytika', 'python', 'code', 'determinants', 'matrices', 'implementation', 'Cramer', '’', 's', 'rule']\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "from nltk import pos_tag # Already imported nltk, but good for clarity\n",
    "# from nltk.tokenize import word_tokenize # Already imported\n",
    "\n",
    "print(\"\\n--- NLTK: Part-of-Speech Tagging ---\")\n",
    "sentence_for_pos = \"The quick brown fox jumps over the lazy dog. Duytika is a good programmer. Duytika has made a well rounded python code to compute the determinants for 2x2 and 3x3 matrices along with the implementation of the Cramer’s rule.\"\n",
    "tokens_for_pos = word_tokenize(sentence_for_pos)\n",
    "pos_tags = pos_tag(tokens_for_pos)\n",
    "\n",
    "print(\"Tokens:\", tokens_for_pos)\n",
    "print(\"POS Tags:\")\n",
    "for word, tag in pos_tags:\n",
    "    # nltk.help.upenn_tagset(tag) provides a description of the tag\n",
    "    # It can be verbose, so let's just print the tag for now, or a snippet.\n",
    "    tag_description_lines = nltk.help.upenn_tagset(tag)\n",
    "    if tag_description_lines:\n",
    "        tag_description = tag_description_lines.splitlines()[0] # Get the first line of description\n",
    "    else:\n",
    "        tag_description = \"No description available\"\n",
    "    print(f\"('{word}', '{tag}') - {tag_description}\")\n",
    "\n",
    "# Example: Find all nouns from the sentence\n",
    "nouns = [word for word, tag in pos_tags if tag.startswith('NN')] # NN, NNS, NNP, NNPS are noun tags\n",
    "print(\"\\nNouns from the sentence:\", nouns)\n",
    "print(\"-\" * 30)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
