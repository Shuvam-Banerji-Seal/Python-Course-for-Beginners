{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hugging Face Transformers Fundamentals: Local LLM Deployment\n",
    "\n",
    "Welcome to the comprehensive guide on using Hugging Face Transformers for local Large Language Model deployment! This notebook will teach you how to use the Transformers library as an alternative to Ollama for running models locally.\n",
    "\n",
    "## What You'll Learn\n",
    "\n",
    "- Installing and setting up Hugging Face Transformers\n",
    "- Loading different types of models (text generation, chat, code)\n",
    "- Memory optimization techniques and quantization\n",
    "- Performance comparison with Ollama\n",
    "- Hardware considerations and GPU acceleration\n",
    "- Hands-on examples with interactive widgets\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- Python 3.8 or higher\n",
    "- At least 8GB of RAM (16GB+ recommended for larger models)\n",
    "- Optional: CUDA-compatible GPU for acceleration\n",
    "- Internet connection for initial model downloads\n",
    "\n",
    "Let's dive in!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Installation and Setup\n",
    "\n",
    "First, let's install the required packages and check our system capabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "import subprocess\n",
    "import sys\n",
    "import platform\n",
    "\n",
    "def install_package(package):\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package])\n",
    "\n",
    "# Core packages for Transformers\n",
    "packages = [\n",
    "    'transformers>=4.30.0',\n",
    "    'torch>=2.0.0',\n",
    "    'accelerate>=0.20.0',\n",
    "    'bitsandbytes',  # For quantization\n",
    "    'ipywidgets',\n",
    "    'matplotlib',\n",
    "    'pandas',\n",
    "    'psutil',  # For system monitoring\n",
    "    'GPUtil'   # For GPU monitoring\n",
    "]\n",
    "\n",
    "print(\"Installing Hugging Face Transformers and dependencies...\")\n",
    "print(\"This may take a few minutes.\\n\")\n",
    "\n",
    "for package in packages:\n",
    "    try:\n",
    "        # Try importing the base package name\n",
    "        base_name = package.split('>=')[0].split('==')[0]\n",
    "        if base_name == 'bitsandbytes':\n",
    "            # Skip bitsandbytes check as it's optional\n",
    "            continue\n",
    "        __import__(base_name)\n",
    "        print(f\"✓ {base_name} is already installed\")\n",
    "    except ImportError:\n",
    "        print(f\"Installing {package}...\")\n",
    "        try:\n",
    "            install_package(package)\n",
    "            print(f\"✓ {package} installed successfully\")\n",
    "        except Exception as e:\n",
    "            print(f\"⚠️ Warning: Could not install {package}: {e}\")\n",
    "\n",
    "print(\"\\n🎉 Installation complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import torch\n",
    "import transformers\n",
    "from transformers import (\n",
    "    AutoTokenizer, \n",
    "    AutoModelForCausalLM, \n",
    "    pipeline,\n",
    "    BitsAndBytesConfig\n",
    ")\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, HTML, clear_output\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import psutil\n",
    "import time\n",
    "import gc\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Try to import GPU monitoring\n",
    "try:\n",
    "    import GPUtil\n",
    "    GPU_AVAILABLE = True\n",
    "except ImportError:\n",
    "    GPU_AVAILABLE = False\n",
    "    print(\"GPUtil not available - GPU monitoring disabled\")\n",
    "\n",
    "print(f\"🔧 PyTorch version: {torch.__version__}\")\n",
    "print(f\"🤗 Transformers version: {transformers.__version__}\")\n",
    "print(f\"🖥️ System: {platform.system()} {platform.release()}\")\n",
    "print(f\"🧠 CPU cores: {psutil.cpu_count()}\")\n",
    "print(f\"💾 RAM: {psutil.virtual_memory().total / (1024**3):.1f} GB\")\n",
    "\n",
    "# Check CUDA availability\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"🚀 CUDA available: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"🎮 GPU memory: {torch.cuda.get_device_properties(0).total_memory / (1024**3):.1f} GB\")\n",
    "else:\n",
    "    print(\"💻 CUDA not available - using CPU\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. System Resource Monitoring\n",
    "\n",
    "Let's create utilities to monitor system resources during model operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SystemMonitor:\n",
    "    \"\"\"Monitor system resources during model operations\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.measurements = []\n",
    "    \n",
    "    def get_memory_usage(self):\n",
    "        \"\"\"Get current memory usage\"\"\"\n",
    "        memory = psutil.virtual_memory()\n",
    "        return {\n",
    "            'ram_used_gb': memory.used / (1024**3),\n",
    "            'ram_percent': memory.percent,\n",
    "            'ram_available_gb': memory.available / (1024**3)\n",
    "        }\n",
    "    \n",
    "    def get_gpu_usage(self):\n",
    "        \"\"\"Get GPU usage if available\"\"\"\n",
    "        if not GPU_AVAILABLE or not torch.cuda.is_available():\n",
    "            return {'gpu_memory_used_gb': 0, 'gpu_memory_total_gb': 0, 'gpu_utilization': 0}\n",
    "        \n",
    "        try:\n",
    "            gpu = GPUtil.getGPUs()[0]\n",
    "            return {\n",
    "                'gpu_memory_used_gb': gpu.memoryUsed / 1024,\n",
    "                'gpu_memory_total_gb': gpu.memoryTotal / 1024,\n",
    "                'gpu_utilization': gpu.load * 100\n",
    "            }\n",
    "        except:\n",
    "            # Fallback to PyTorch CUDA info\n",
    "            allocated = torch.cuda.memory_allocated() / (1024**3)\n",
    "            total = torch.cuda.get_device_properties(0).total_memory / (1024**3)\n",
    "            return {\n",
    "                'gpu_memory_used_gb': allocated,\n",
    "                'gpu_memory_total_gb': total,\n",
    "                'gpu_utilization': 0\n",
    "            }\n",
    "    \n",
    "    def snapshot(self, label=\"\"):\n",
    "        \"\"\"Take a snapshot of current resource usage\"\"\"\n",
    "        timestamp = datetime.now()\n",
    "        memory_info = self.get_memory_usage()\n",
    "        gpu_info = self.get_gpu_usage()\n",
    "        \n",
    "        snapshot = {\n",
    "            'timestamp': timestamp,\n",
    "            'label': label,\n",
    "            **memory_info,\n",
    "            **gpu_info\n",
    "        }\n",
    "        \n",
    "        self.measurements.append(snapshot)\n",
    "        return snapshot\n",
    "    \n",
    "    def display_current(self):\n",
    "        \"\"\"Display current resource usage\"\"\"\n",
    "        snapshot = self.snapshot()\n",
    "        \n",
    "        print(f\"💾 RAM: {snapshot['ram_used_gb']:.1f}GB / {snapshot['ram_used_gb'] + snapshot['ram_available_gb']:.1f}GB ({snapshot['ram_percent']:.1f}%)\")\n",
    "        \n",
    "        if torch.cuda.is_available():\n",
    "            print(f\"🎮 GPU: {snapshot['gpu_memory_used_gb']:.1f}GB / {snapshot['gpu_memory_total_gb']:.1f}GB\")\n",
    "            if snapshot['gpu_utilization'] > 0:\n",
    "                print(f\"⚡ GPU Utilization: {snapshot['gpu_utilization']:.1f}%\")\n",
    "\n",
    "# Initialize system monitor\n",
    "monitor = SystemMonitor()\n",
    "print(\"📊 Current system status:\")\n",
    "monitor.display_current()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Model Loading and Management\n",
    "\n",
    "Let's create a comprehensive model manager for Hugging Face Transformers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformersManager:\n",
    "    \"\"\"Manage Hugging Face Transformers models with optimization options\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.models = {}\n",
    "        self.tokenizers = {}\n",
    "        self.pipelines = {}\n",
    "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        print(f\"🔧 Using device: {self.device}\")\n",
    "    \n",
    "    def load_model(self, model_name, use_quantization=False, load_in_8bit=False, load_in_4bit=False):\n",
    "        \"\"\"Load a model with various optimization options\"\"\"\n",
    "        print(f\"📥 Loading {model_name}...\")\n",
    "        \n",
    "        # Take memory snapshot before loading\n",
    "        monitor.snapshot(f\"Before loading {model_name}\")\n",
    "        \n",
    "        try:\n",
    "            # Configure quantization if requested\n",
    "            model_kwargs = {}\n",
    "            \n",
    "            if use_quantization and (load_in_8bit or load_in_4bit):\n",
    "                if load_in_4bit:\n",
    "                    quantization_config = BitsAndBytesConfig(\n",
    "                        load_in_4bit=True,\n",
    "                        bnb_4bit_compute_dtype=torch.float16,\n",
    "                        bnb_4bit_use_double_quant=True,\n",
    "                        bnb_4bit_quant_type=\"nf4\"\n",
    "                    )\n",
    "                    model_kwargs[\"quantization_config\"] = quantization_config\n",
    "                    print(\"🔧 Using 4-bit quantization\")\n",
    "                elif load_in_8bit:\n",
    "                    model_kwargs[\"load_in_8bit\"] = True\n",
    "                    print(\"🔧 Using 8-bit quantization\")\n",
    "            \n",
    "            # Set device map for multi-GPU or CPU\n",
    "            if self.device == \"cuda\":\n",
    "                model_kwargs[\"device_map\"] = \"auto\"\n",
    "                model_kwargs[\"torch_dtype\"] = torch.float16\n",
    "            \n",
    "            # Load tokenizer\n",
    "            tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "            if tokenizer.pad_token is None:\n",
    "                tokenizer.pad_token = tokenizer.eos_token\n",
    "            \n",
    "            # Load model\n",
    "            model = AutoModelForCausalLM.from_pretrained(\n",
    "                model_name,\n",
    "                **model_kwargs\n",
    "            )\n",
    "            \n",
    "            # Store references\n",
    "            self.models[model_name] = model\n",
    "            self.tokenizers[model_name] = tokenizer\n",
    "            \n",
    "            # Create pipeline for easy inference\n",
    "            pipe = pipeline(\n",
    "                \"text-generation\",\n",
    "                model=model,\n",
    "                tokenizer=tokenizer,\n",
    "                device=0 if self.device == \"cuda\" else -1\n",
    "            )\n",
    "            self.pipelines[model_name] = pipe\n",
    "            \n",
    "            # Take memory snapshot after loading\n",
    "            monitor.snapshot(f\"After loading {model_name}\")\n",
    "            \n",
    "            print(f\"✅ Successfully loaded {model_name}\")\n",
    "            monitor.display_current()\n",
    "            \n",
    "            return True\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error loading {model_name}: {str(e)}\")\n",
    "            return False\n",
    "    \n",
    "    def generate_text(self, model_name, prompt, max_length=100, temperature=0.7, do_sample=True):\n",
    "        \"\"\"Generate text using a loaded model\"\"\"\n",
    "        if model_name not in self.pipelines:\n",
    "            print(f\"❌ Model {model_name} not loaded\")\n",
    "            return None\n",
    "        \n",
    "        try:\n",
    "            start_time = time.time()\n",
    "            \n",
    "            # Generate text\n",
    "            result = self.pipelines[model_name](\n",
    "                prompt,\n",
    "                max_length=max_length,\n",
    "                temperature=temperature,\n",
    "                do_sample=do_sample,\n",
    "                pad_token_id=self.tokenizers[model_name].eos_token_id,\n",
    "                return_full_text=False\n",
    "            )\n",
    "            \n",
    "            end_time = time.time()\n",
    "            \n",
    "            generated_text = result[0]['generated_text']\n",
    "            \n",
    "            return {\n",
    "                'text': generated_text,\n",
    "                'duration': end_time - start_time,\n",
    "                'model': model_name\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error generating text: {str(e)}\")\n",
    "            return None\n",
    "    \n",
    "    def unload_model(self, model_name):\n",
    "        \"\"\"Unload a model to free memory\"\"\"\n",
    "        if model_name in self.models:\n",
    "            del self.models[model_name]\n",
    "            del self.tokenizers[model_name]\n",
    "            del self.pipelines[model_name]\n",
    "            \n",
    "            # Force garbage collection\n",
    "            gc.collect()\n",
    "            if torch.cuda.is_available():\n",
    "                torch.cuda.empty_cache()\n",
    "            \n",
    "            print(f\"🗑️ Unloaded {model_name}\")\n",
    "            monitor.display_current()\n",
    "    \n",
    "    def list_loaded_models(self):\n",
    "        \"\"\"List currently loaded models\"\"\"\n",
    "        return list(self.models.keys())\n",
    "\n",
    "# Initialize the Transformers manager\n",
    "tm = TransformersManager()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Interactive Model Loading Interface\n",
    "\n",
    "Let's create an interactive interface to load different types of models with various optimization options."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interactive model loading interface\n",
    "def create_model_loader():\n",
    "    \"\"\"Create an interactive interface for loading models\"\"\"\n",
    "    \n",
    "    # Popular small models for testing\n",
    "    model_options = {\n",
    "        'GPT-2 Small (124M)': 'gpt2',\n",
    "        'GPT-2 Medium (355M)': 'gpt2-medium',\n",
    "        'DistilGPT-2 (82M)': 'distilgpt2',\n",
    "        'Microsoft DialoGPT Small': 'microsoft/DialoGPT-small',\n",
    "        'CodeT5 Small': 'Salesforce/codet5-small',\n",
    "        'Custom Model': 'custom'\n",
    "    }\n",
    "    \n",
    "    # Create widgets\n",
    "    model_dropdown = widgets.Dropdown(\n",
    "        options=list(model_options.keys()),\n",
    "        value='GPT-2 Small (124M)',\n",
    "        description='Model:',\n",
    "        style={'description_width': 'initial'}\n",
    "    )\n",
    "    \n",
    "    custom_model_text = widgets.Text(\n",
    "        placeholder='Enter Hugging Face model name',\n",
    "        description='Custom:',\n",
    "        style={'description_width': 'initial'},\n",
    "        layout=widgets.Layout(display='none')\n",
    "    )\n",
    "    \n",
    "    quantization_checkbox = widgets.Checkbox(\n",
    "        value=False,\n",
    "        description='Use Quantization (requires GPU)',\n",
    "        style={'description_width': 'initial'}\n",
    "    )\n",
    "    \n",
    "    quantization_type = widgets.RadioButtons(\n",
    "        options=['4-bit', '8-bit'],\n",
    "        value='4-bit',\n",
    "        description='Type:',\n",
    "        layout=widgets.Layout(display='none')\n",
    "    )\n",
    "    \n",
    "    load_button = widgets.Button(\n",
    "        description='Load Model',\n",
    "        button_style='primary',\n",
    "        icon='download'\n",
    "    )\n",
    "    \n",
    "    unload_button = widgets.Button(\n",
    "        description='Unload All Models',\n",
    "        button_style='warning',\n",
    "        icon='trash'\n",
    "    )\n",
    "    \n",
    "    output_area = widgets.Output()\n",
    "    \n",
    "    # Event handlers\n",
    "    def on_model_change(change):\n",
    "        if change['new'] == 'Custom Model':\n",
    "            custom_model_text.layout.display = 'block'\n",
    "        else:\n",
    "            custom_model_text.layout.display = 'none'\n",
    "    \n",
    "    def on_quantization_change(change):\n",
    "        if change['new']:\n",
    "            quantization_type.layout.display = 'block'\n",
    "        else:\n",
    "            quantization_type.layout.display = 'none'\n",
    "    \n",
    "    def on_load_click(b):\n",
    "        with output_area:\n",
    "            clear_output()\n",
    "            \n",
    "            # Determine model name\n",
    "            if model_dropdown.value == 'Custom Model':\n",
    "                model_name = custom_model_text.value.strip()\n",
    "                if not model_name:\n",
    "                    print(\"❌ Please enter a custom model name\")\n",
    "                    return\n",
    "            else:\n",
    "                model_name = model_options[model_dropdown.value]\n",
    "            \n",
    "            # Load model with options\n",
    "            use_quant = quantization_checkbox.value\n",
    "            load_4bit = use_quant and quantization_type.value == '4-bit'\n",
    "            load_8bit = use_quant and quantization_type.value == '8-bit'\n",
    "            \n",
    "            success = tm.load_model(\n",
    "                model_name,\n",
    "                use_quantization=use_quant,\n",
    "                load_in_4bit=load_4bit,\n",
    "                load_in_8bit=load_8bit\n",
    "            )\n",
    "            \n",
    "            if success:\n",
    "                print(f\"\\n📋 Loaded models: {tm.list_loaded_models()}\")\n",
    "    \n",
    "    def on_unload_click(b):\n",
    "        with output_area:\n",
    "            clear_output()\n",
    "            loaded_models = tm.list_loaded_models().copy()\n",
    "            for model in loaded_models:\n",
    "                tm.unload_model(model)\n",
    "            print(\"🗑️ All models unloaded\")\n",
    "    \n",
    "    # Set up event handlers\n",
    "    model_dropdown.observe(on_model_change, names='value')\n",
    "    quantization_checkbox.observe(on_quantization_change, names='value')\n",
    "    load_button.on_click(on_load_click)\n",
    "    unload_button.on_click(on_unload_click)\n",
    "    \n",
    "    return widgets.VBox([\n",
    "        widgets.HTML(\"<h3>🤗 Model Loading Interface</h3>\"),\n",
    "        model_dropdown,\n",
    "        custom_model_text,\n",
    "        quantization_checkbox,\n",
    "        quantization_type,\n",
    "        widgets.HBox([load_button, unload_button]),\n",
    "        output_area\n",
    "    ])\n",
    "\n",
    "display(create_model_loader())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Text Generation Examples\n",
    "\n",
    "Now let's explore different types of text generation with the loaded models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interactive text generation interface\n",
    "def create_text_generator():\n",
    "    \"\"\"Create an interactive text generation interface\"\"\"\n",
    "    \n",
    "    # Get loaded models\n",
    "    loaded_models = tm.list_loaded_models()\n",
    "    \n",
    "    if not loaded_models:\n",
    "        return widgets.HTML(\"<p>❌ No models loaded. Please load a model first using the interface above.</p>\")\n",
    "    \n",
    "    # Create widgets\n",
    "    model_selector = widgets.Dropdown(\n",
    "        options=loaded_models,\n",
    "        description='Model:',\n",
    "        style={'description_width': 'initial'}\n",
    "    )\n",
    "    \n",
    "    prompt_text = widgets.Textarea(\n",
    "        value=\"Once upon a time, in a world where artificial intelligence\",\n",
    "        placeholder=\"Enter your prompt here...\",\n",
    "        description='Prompt:',\n",
    "        layout=widgets.Layout(width='100%', height='80px'),\n",
    "        style={'description_width': 'initial'}\n",
    "    )\n",
    "    \n",
    "    max_length_slider = widgets.IntSlider(\n",
    "        value=100,\n",
    "        min=20,\n",
    "        max=500,\n",
    "        step=10,\n",
    "        description='Max Length:',\n",
    "        style={'description_width': 'initial'}\n",
    "    )\n",
    "    \n",
    "    temperature_slider = widgets.FloatSlider(\n",
    "        value=0.7,\n",
    "        min=0.1,\n",
    "        max=2.0,\n",
    "        step=0.1,\n",
    "        description='Temperature:',\n",
    "        style={'description_width': 'initial'}\n",
    "    )\n",
    "    \n",
    "    generate_button = widgets.Button(\n",
    "        description='Generate Text',\n",
    "        button_style='success',\n",
    "        icon='magic'\n",
    "    )\n",
    "    \n",
    "    output_area = widgets.Output()\n",
    "    \n",
    "    def on_generate_click(b):\n",
    "        with output_area:\n",
    "            clear_output()\n",
    "            \n",
    "            model = model_selector.value\n",
    "            prompt = prompt_text.value\n",
    "            max_len = max_length_slider.value\n",
    "            temp = temperature_slider.value\n",
    "            \n",
    "            print(f\"🤖 Generating with {model}...\")\n",
    "            print(f\"📝 Prompt: {prompt}\")\n",
    "            print(f\"⚙️ Max Length: {max_len}, Temperature: {temp}\")\n",
    "            print(\"\\n\" + \"=\"*60 + \"\\n\")\n",
    "            \n",
    "            result = tm.generate_text(\n",
    "                model,\n",
    "                prompt,\n",
    "                max_length=max_len,\n",
    "                temperature=temp\n",
    "            )\n",
    "            \n",
    "            if result:\n",
    "                print(f\"Generated Text:\\n{result['text']}\")\n",
    "                print(f\"\\n⏱️ Generation time: {result['duration']:.2f} seconds\")\n",
    "                \n",
    "                # Calculate words per second\n",
    "                word_count = len(result['text'].split())\n",
    "                wps = word_count / result['duration'] if result['duration'] > 0 else 0\n",
    "                print(f\"📊 Words generated: {word_count} ({wps:.1f} words/sec)\")\n",
    "            else:\n",
    "                print(\"❌ Generation failed\")\n",
    "    \n",
    "    generate_button.on_click(on_generate_click)\n",
    "    \n",
    "    return widgets.VBox([\n",
    "        widgets.HTML(\"<h3>✨ Interactive Text Generation</h3>\"),\n",
    "        model_selector,\n",
    "        prompt_text,\n",
    "        widgets.HBox([max_length_slider, temperature_slider]),\n",
    "        generate_button,\n",
    "        output_area\n",
    "    ])\n",
    "\n",
    "# Create the text generator (will update when models are loaded)\n",
    "text_gen_widget = create_text_generator()\n",
    "display(text_gen_widget)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Performance Comparison: Transformers vs Ollama\n",
    "\n",
    "Let's create a comprehensive comparison between Hugging Face Transformers and Ollama approaches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance comparison utilities\n",
    "def run_performance_benchmark():\n",
    "    \"\"\"Run performance benchmarks on loaded models\"\"\"\n",
    "    \n",
    "    loaded_models = tm.list_loaded_models()\n",
    "    \n",
    "    if not loaded_models:\n",
    "        print(\"❌ No models loaded for benchmarking\")\n",
    "        return None\n",
    "    \n",
    "    # Test prompts of varying complexity\n",
    "    test_prompts = [\n",
    "        \"Hello, how are you?\",\n",
    "        \"Explain the concept of machine learning in simple terms.\",\n",
    "        \"Write a short story about a robot discovering emotions.\",\n",
    "        \"Create a Python function that calculates the factorial of a number.\"\n",
    "    ]\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    print(\"🔍 Running performance benchmark...\\n\")\n",
    "    \n",
    "    for model_name in loaded_models:\n",
    "        print(f\"Testing {model_name}...\")\n",
    "        \n",
    "        for i, prompt in enumerate(test_prompts, 1):\n",
    "            print(f\"  Prompt {i}/4: \", end=\"\")\n",
    "            \n",
    "            # Take memory snapshot before generation\n",
    "            before_snapshot = monitor.snapshot(f\"Before gen {model_name} P{i}\")\n",
    "            \n",
    "            # Generate text\n",
    "            result = tm.generate_text(\n",
    "                model_name, \n",
    "                prompt, \n",
    "                max_length=150, \n",
    "                temperature=0.7\n",
    "            )\n",
    "            \n",
    "            # Take memory snapshot after generation\n",
    "            after_snapshot = monitor.snapshot(f\"After gen {model_name} P{i}\")\n",
    "            \n",
    "            if result:\n",
    "                word_count = len(result['text'].split())\n",
    "                duration = result['duration']\n",
    "                words_per_sec = word_count / duration if duration > 0 else 0\n",
    "                \n",
    "                results.append({\n",
    "                    'Model': model_name,\n",
    "                    'Prompt': f\"Prompt {i}\",\n",
    "                    'Prompt_Text': prompt[:30] + \"...\",\n",
    "                    'Duration (s)': duration,\n",
    "                    'Words': word_count,\n",
    "                    'Words/sec': words_per_sec,\n",
    "                    'RAM_Before (GB)': before_snapshot['ram_used_gb'],\n",
    "                    'RAM_After (GB)': after_snapshot['ram_used_gb'],\n",
    "                    'GPU_Before (GB)': before_snapshot['gpu_memory_used_gb'],\n",
    "                    'GPU_After (GB)': after_snapshot['gpu_memory_used_gb']\n",
    "                })\n",
    "                \n",
    "                print(f\"{duration:.2f}s ({word_count} words, {words_per_sec:.1f} w/s)\")\n",
    "            else:\n",
    "                print(\"Failed\")\n",
    "        \n",
    "        print()\n",
    "    \n",
    "    if results:\n",
    "        df = pd.DataFrame(results)\n",
    "        \n",
    "        # Display summary statistics\n",
    "        print(\"📊 Performance Summary:\")\n",
    "        summary = df.groupby('Model').agg({\n",
    "            'Duration (s)': ['mean', 'std'],\n",
    "            'Words/sec': ['mean', 'std'],\n",
    "            'Words': 'mean'\n",
    "        }).round(2)\n",
    "        \n",
    "        print(summary.to_string())\n",
    "        \n",
    "        # Create visualizations\n",
    "        fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "        \n",
    "        # Response time by model\n",
    "        df.boxplot(column='Duration (s)', by='Model', ax=axes[0,0])\n",
    "        axes[0,0].set_title('Response Time Distribution by Model')\n",
    "        axes[0,0].set_xlabel('Model')\n",
    "        \n",
    "        # Words per second by model\n",
    "        df.boxplot(column='Words/sec', by='Model', ax=axes[0,1])\n",
    "        axes[0,1].set_title('Words per Second Distribution by Model')\n",
    "        axes[0,1].set_xlabel('Model')\n",
    "        \n",
    "        # Memory usage comparison\n",
    "        memory_data = df.groupby('Model')[['RAM_Before (GB)', 'RAM_After (GB)']].mean()\n",
    "        memory_data.plot(kind='bar', ax=axes[1,0])\n",
    "        axes[1,0].set_title('Average RAM Usage by Model')\n",
    "        axes[1,0].set_ylabel('RAM (GB)')\n",
    "        axes[1,0].tick_params(axis='x', rotation=45)\n",
    "        \n",
    "        # GPU memory usage (if available)\n",
    "        if torch.cuda.is_available():\n",
    "            gpu_data = df.groupby('Model')[['GPU_Before (GB)', 'GPU_After (GB)']].mean()\n",
    "            gpu_data.plot(kind='bar', ax=axes[1,1])\n",
    "            axes[1,1].set_title('Average GPU Memory Usage by Model')\n",
    "            axes[1,1].set_ylabel('GPU Memory (GB)')\n",
    "            axes[1,1].tick_params(axis='x', rotation=45)\n",
    "        else:\n",
    "            axes[1,1].text(0.5, 0.5, 'GPU not available', \n",
    "                          ha='center', va='center', transform=axes[1,1].transAxes)\n",
    "            axes[1,1].set_title('GPU Memory Usage')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    return None\n",
    "\n",
    "# Create benchmark interface\n",
    "benchmark_button = widgets.Button(\n",
    "    description='Run Performance Benchmark',\n",
    "    button_style='info',\n",
    "    icon='chart-bar'\n",
    ")\n",
    "\n",
    "benchmark_output = widgets.Output()\n",
    "\n",
    "def on_benchmark_click(b):\n",
    "    with benchmark_output:\n",
    "        clear_output()\n",
    "        benchmark_data = run_performance_benchmark()\n",
    "        if benchmark_data is not None:\n",
    "            print(\"\\n📈 Benchmark completed! Results displayed above.\")\n",
    "\n",
    "benchmark_button.on_click(on_benchmark_click)\n",
    "\n",
    "display(widgets.VBox([\n",
    "    widgets.HTML(\"<h3>⚡ Performance Benchmarking</h3>\"),\n",
    "    widgets.HTML(\"<p>Run comprehensive performance tests on loaded models:</p>\"),\n",
    "    benchmark_button,\n",
    "    benchmark_output\n",
    "]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Model Type Comparison\n",
    "\n",
    "Let's explore different types of models and their specific use cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model type comparison\n",
    "def create_model_comparison():\n",
    "    \"\"\"Create interface to compare different model types\"\"\"\n",
    "    \n",
    "    model_categories = {\n",
    "        'General Text Generation': {\n",
    "            'models': ['gpt2', 'distilgpt2'],\n",
    "            'description': 'General purpose text generation models',\n",
    "            'use_cases': ['Creative writing', 'Text completion', 'General conversation']\n",
    "        },\n",
    "        'Conversational': {\n",
    "            'models': ['microsoft/DialoGPT-small', 'microsoft/DialoGPT-medium'],\n",
    "            'description': 'Models optimized for dialogue and conversation',\n",
    "            'use_cases': ['Chatbots', 'Customer service', 'Interactive assistants']\n",
    "        },\n",
    "        'Code Generation': {\n",
    "            'models': ['Salesforce/codet5-small', 'microsoft/CodeGPT-small-py'],\n",
    "            'description': 'Models specialized for code generation and programming tasks',\n",
    "            'use_cases': ['Code completion', 'Code explanation', 'Programming assistance']\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Create comparison table\n",
    "    comparison_data = []\n",
    "    for category, info in model_categories.items():\n",
    "        for model in info['models']:\n",
    "            comparison_data.append({\n",
    "                'Category': category,\n",
    "                'Model': model,\n",
    "                'Description': info['description'],\n",
    "                'Primary Use Cases': ', '.join(info['use_cases'])\n",
    "            })\n",
    "    \n",
    "    df = pd.DataFrame(comparison_data)\n",
    "    \n",
    "    print(\"🔍 Model Type Comparison:\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    for category, group in df.groupby('Category'):\n",
    "        print(f\"\\n📂 {category}\")\n",
    "        print(\"-\" * 40)\n",
    "        for _, row in group.iterrows():\n",
    "            print(f\"  🤖 {row['Model']}\")\n",
    "            print(f\"     {row['Description']}\")\n",
    "            print(f\"     Use cases: {row['Primary Use Cases']}\")\n",
    "            print()\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Display model comparison\n",
    "model_comparison_df = create_model_comparison()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Memory Optimization Techniques\n",
    "\n",
    "Let's explore various memory optimization techniques for running larger models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Memory optimization demonstration\n",
    "def demonstrate_memory_optimization():\n",
    "    \"\"\"Demonstrate different memory optimization techniques\"\"\"\n",
    "    \n",
    "    print(\"🧠 Memory Optimization Techniques for Transformers\\n\")\n",
    "    \n",
    "    techniques = {\n",
    "        '4-bit Quantization': {\n",
    "            'description': 'Reduces model size by ~75% with minimal quality loss',\n",
    "            'memory_reduction': '~75%',\n",
    "            'quality_impact': 'Minimal',\n",
    "            'requirements': 'GPU with bitsandbytes support',\n",
    "            'code_example': '''\n",
    "# 4-bit quantization configuration\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\"\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"model_name\",\n",
    "    quantization_config=quantization_config,\n",
    "    device_map=\"auto\"\n",
    ")'''\n",
    "        },\n",
    "        '8-bit Quantization': {\n",
    "            'description': 'Reduces model size by ~50% with good quality retention',\n",
    "            'memory_reduction': '~50%',\n",
    "            'quality_impact': 'Low',\n",
    "            'requirements': 'GPU with bitsandbytes support',\n",
    "            'code_example': '''\n",
    "# 8-bit quantization\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"model_name\",\n",
    "    load_in_8bit=True,\n",
    "    device_map=\"auto\"\n",
    ")'''\n",
    "        },\n",
    "        'CPU Offloading': {\n",
    "            'description': 'Automatically manages model layers between GPU and CPU',\n",
    "            'memory_reduction': 'Variable',\n",
    "            'quality_impact': 'None (slower inference)',\n",
    "            'requirements': 'Accelerate library',\n",
    "            'code_example': '''\n",
    "# Automatic device mapping with CPU offloading\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"model_name\",\n",
    "    device_map=\"auto\",\n",
    "    offload_folder=\"./offload\"\n",
    ")'''\n",
    "        },\n",
    "        'Gradient Checkpointing': {\n",
    "            'description': 'Trades computation for memory during training/fine-tuning',\n",
    "            'memory_reduction': '~30-50%',\n",
    "            'quality_impact': 'None (slower training)',\n",
    "            'requirements': 'Any device',\n",
    "            'code_example': '''\n",
    "# Enable gradient checkpointing\n",
    "model.gradient_checkpointing_enable()\n",
    "\n",
    "# Or during model loading\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"model_name\",\n",
    "    use_cache=False  # Required for gradient checkpointing\n",
    ")'''\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Display techniques\n",
    "    for technique, info in techniques.items():\n",
    "        print(f\"🔧 {technique}\")\n",
    "        print(f\"   Description: {info['description']}\")\n",
    "        print(f\"   Memory Reduction: {info['memory_reduction']}\")\n",
    "        print(f\"   Quality Impact: {info['quality_impact']}\")\n",
    "        print(f\"   Requirements: {info['requirements']}\")\n",
    "        print(f\"   Code Example:{info['code_example']}\")\n",
    "        print()\n",
    "    \n",
    "    # Memory usage recommendations\n",
    "    print(\"💡 Memory Usage Recommendations:\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    recommendations = [\n",
    "        (\"< 8GB RAM\", \"Use 4-bit quantization with small models (< 1B parameters)\"),\n",
    "        (\"8-16GB RAM\", \"Use 8-bit quantization or CPU offloading for medium models\"),\n",
    "        (\"16-32GB RAM\", \"Can run larger models with quantization or full precision small models\"),\n",
    "        (\"> 32GB RAM\", \"Can run large models in full precision with proper GPU memory\")\n",
    "    ]\n",
    "    \n",
    "    for memory_range, recommendation in recommendations:\n",
    "        print(f\"  {memory_range}: {recommendation}\")\n",
    "    \n",
    "    print(\"\\n⚠️ Important Notes:\")\n",
    "    print(\"  • Quantization requires compatible GPU and bitsandbytes library\")\n",
    "    print(\"  • CPU offloading increases inference time but reduces memory usage\")\n",
    "    print(\"  • Always monitor memory usage during model loading and inference\")\n",
    "    print(\"  • Consider model size vs. available hardware when choosing optimization\")\n",
    "\n",
    "demonstrate_memory_optimization()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Hardware Considerations and Recommendations\n",
    "\n",
    "Let's analyze hardware requirements and provide recommendations for different use cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hardware analysis and recommendations\n",
    "def analyze_hardware_requirements():\n",
    "    \"\"\"Analyze current hardware and provide recommendations\"\"\"\n",
    "    \n",
    "    print(\"🖥️ Hardware Analysis and Recommendations\\n\")\n",
    "    \n",
    "    # Get current system specs\n",
    "    current_specs = {\n",
    "        'CPU Cores': psutil.cpu_count(),\n",
    "        'RAM (GB)': psutil.virtual_memory().total / (1024**3),\n",
    "        'GPU Available': torch.cuda.is_available(),\n",
    "        'GPU Memory (GB)': torch.cuda.get_device_properties(0).total_memory / (1024**3) if torch.cuda.is_available() else 0,\n",
    "        'GPU Name': torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'None'\n",
    "    }\n",
    "    \n",
    "    print(\"📊 Current System Specifications:\")\n",
    "    print(\"-\" * 40)\n",
    "    for spec, value in current_specs.items():\n",
    "        if isinstance(value, float):\n",
    "            print(f\"  {spec}: {value:.1f}\")\n",
    "        else:\n",
    "            print(f\"  {spec}: {value}\")\n",
    "    \n",
    "    # Model size recommendations\n",
    "    print(\"\\n🤖 Model Size Recommendations:\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    ram_gb = current_specs['RAM (GB)']\n",
    "    gpu_gb = current_specs['GPU Memory (GB)']\n",
    "    \n",
    "    model_recommendations = []\n",
    "    \n",
    "    if ram_gb < 8:\n",
    "        model_recommendations.append(\"⚠️ Limited RAM: Stick to very small models (< 500M parameters)\")\n",
    "        model_recommendations.append(\"   Recommended: distilgpt2, gpt2 (with quantization)\")\n",
    "    elif ram_gb < 16:\n",
    "        model_recommendations.append(\"✅ Moderate RAM: Can run small to medium models\")\n",
    "        model_recommendations.append(\"   Recommended: gpt2, gpt2-medium, small DialoGPT models\")\n",
    "    elif ram_gb < 32:\n",
    "        model_recommendations.append(\"🚀 Good RAM: Can run medium to large models with optimization\")\n",
    "        model_recommendations.append(\"   Recommended: gpt2-large, medium conversational models\")\n",
    "    else:\n",
    "        model_recommendations.append(\"💪 Excellent RAM: Can run large models comfortably\")\n",
    "        model_recommendations.append(\"   Recommended: Large language models with full precision\")\n",
    "    \n",
    "    if gpu_gb == 0:\n",
    "        model_recommendations.append(\"💻 CPU Only: Inference will be slower, use smaller models\")\n",
    "        model_recommendations.append(\"   Tip: Consider cloud GPU services for larger models\")\n",
    "    elif gpu_gb < 8:\n",
    "        model_recommendations.append(\"🎮 Limited GPU: Use quantization for better performance\")\n",
    "        model_recommendations.append(\"   Recommended: 8-bit quantization for medium models\")\n",
    "    elif gpu_gb < 16:\n",
    "        model_recommendations.append(\"🚀 Good GPU: Can run large models with quantization\")\n",
    "        model_recommendations.append(\"   Recommended: 4-bit quantization for very large models\")\n",
    "    else:\n",
    "        model_recommendations.append(\"💎 Excellent GPU: Can run very large models in full precision\")\n",
    "        model_recommendations.append(\"   Recommended: Full precision for best quality\")\n",
    "    \n",
    "    for rec in model_recommendations:\n",
    "        print(rec)\n",
    "    \n",
    "    # Performance optimization suggestions\n",
    "    print(\"\\n⚡ Performance Optimization Suggestions:\")\n",
    "    print(\"=\" * 45)\n",
    "    \n",
    "    optimizations = []\n",
    "    \n",
    "    if not torch.cuda.is_available():\n",
    "        optimizations.append(\"🔧 Install CUDA-compatible PyTorch for GPU acceleration\")\n",
    "        optimizations.append(\"🔧 Consider using torch.compile() for CPU optimization (PyTorch 2.0+)\")\n",
    "    \n",
    "    if gpu_gb > 0 and gpu_gb < 12:\n",
    "        optimizations.append(\"🔧 Use gradient checkpointing to reduce memory usage\")\n",
    "        optimizations.append(\"🔧 Enable mixed precision training (fp16) for faster inference\")\n",
    "    \n",
    "    if ram_gb > 16:\n",
    "        optimizations.append(\"🔧 Consider CPU offloading for very large models\")\n",
    "        optimizations.append(\"🔧 Use multiple smaller models instead of one large model\")\n",
    "    \n",
    "    optimizations.extend([\n",
    "        \"🔧 Batch multiple requests together for better throughput\",\n",
    "        \"🔧 Use caching for repeated inference requests\",\n",
    "        \"🔧 Monitor memory usage and adjust batch sizes accordingly\"\n",
    "    ])\n",
    "    \n",
    "    for opt in optimizations:\n",
    "        print(opt)\n",
    "    \n",
    "    # Cost-benefit analysis\n",
    "    print(\"\\n💰 Cost-Benefit Analysis:\")\n",
    "    print(\"=\" * 30)\n",
    "    \n",
    "    scenarios = [\n",
    "        {\n",
    "            'name': 'Local Development',\n",
    "            'pros': ['No API costs', 'Full privacy', 'Offline capability'],\n",
    "            'cons': ['Hardware investment', 'Slower inference', 'Limited model size'],\n",
    "            'best_for': 'Prototyping, learning, privacy-sensitive applications'\n",
    "        },\n",
    "        {\n",
    "            'name': 'Cloud API Services',\n",
    "            'pros': ['Fast inference', 'Large models', 'No hardware needed'],\n",
    "            'cons': ['Ongoing costs', 'Internet dependency', 'Privacy concerns'],\n",
    "            'best_for': 'Production applications, high-volume usage'\n",
    "        },\n",
    "        {\n",
    "            'name': 'Hybrid Approach',\n",
    "            'pros': ['Flexibility', 'Cost optimization', 'Fallback options'],\n",
    "            'cons': ['Complexity', 'Multiple integrations'],\n",
    "            'best_for': 'Enterprise applications, varying workloads'\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    for scenario in scenarios:\n",
    "        print(f\"\\n📋 {scenario['name']}:\")\n",
    "        print(f\"   Pros: {', '.join(scenario['pros'])}\")\n",
    "        print(f\"   Cons: {', '.join(scenario['cons'])}\")\n",
    "        print(f\"   Best for: {scenario['best_for']}\")\n",
    "    \n",
    "    return current_specs\n",
    "\n",
    "# Run hardware analysis\n",
    "hardware_specs = analyze_hardware_requirements()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Next Steps and Advanced Topics\n",
    "\n",
    "Congratulations! You've learned the fundamentals of using Hugging Face Transformers for local LLM deployment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next steps and resources\n",
    "def display_next_steps():\n",
    "    \"\"\"Display next steps and advanced topics\"\"\"\n",
    "    \n",
    "    print(\"🎓 Next Steps and Advanced Topics\\n\")\n",
    "    \n",
    "    next_steps = {\n",
    "        '🔬 Advanced Techniques': [\n",
    "            'Fine-tuning models on custom datasets',\n",
    "            'Parameter-efficient fine-tuning (LoRA, AdaLoRA)',\n",
    "            'Model distillation and compression',\n",
    "            'Custom tokenizer creation',\n",
    "            'Multi-modal models (text + images)'\n",
    "        ],\n",
    "        '🚀 Production Deployment': [\n",
    "            'Model serving with FastAPI or Flask',\n",
    "            'Containerization with Docker',\n",
    "            'Scaling with Kubernetes',\n",
    "            'Load balancing and caching strategies',\n",
    "            'Monitoring and logging'\n",
    "        ],\n",
    "        '⚡ Performance Optimization': [\n",
    "            'ONNX model conversion for faster inference',\n",
    "            'TensorRT optimization for NVIDIA GPUs',\n",
    "            'Dynamic batching and request queuing',\n",
    "            'Model parallelism for very large models',\n",
    "            'Custom CUDA kernels for specialized operations'\n",
    "        ],\n",
    "        '🔒 Security and Privacy': [\n",
    "            'Differential privacy in model training',\n",
    "            'Federated learning approaches',\n",
    "            'Input sanitization and validation',\n",
    "            'Model watermarking and detection',\n",
    "            'Secure multi-party computation'\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    for category, topics in next_steps.items():\n",
    "        print(f\"{category}:\")\n",
    "        for topic in topics:\n",
    "            print(f\"  • {topic}\")\n",
    "        print()\n",
    "    \n",
    "    print(\"📚 Recommended Resources:\")\n",
    "    print(\"=\" * 30)\n",
    "    \n",
    "    resources = [\n",
    "        \"🌐 Hugging Face Documentation: https://huggingface.co/docs\",\n",
    "        \"📖 Transformers Course: https://huggingface.co/course\",\n",
    "        \"🎯 Model Hub: https://huggingface.co/models\",\n",
    "        \"💬 Community Forum: https://discuss.huggingface.co\",\n",
    "        \"📊 Papers with Code: https://paperswithcode.com\",\n",
    "        \"🔬 Arxiv ML Papers: https://arxiv.org/list/cs.LG/recent\"\n",
    "    ]\n",
    "    \n",
    "    for resource in resources:\n",
    "        print(resource)\n",
    "    \n",
    "    print(\"\\n🛠️ Useful Libraries and Tools:\")\n",
    "    print(\"=\" * 35)\n",
    "    \n",
    "    tools = {\n",
    "        'accelerate': 'Distributed training and inference',\n",
    "        'datasets': 'Easy access to ML datasets',\n",
    "        'tokenizers': 'Fast tokenization library',\n",
    "        'gradio': 'Quick ML app interfaces',\n",
    "        'streamlit': 'Data app framework',\n",
    "        'wandb': 'Experiment tracking and visualization',\n",
    "        'tensorboard': 'TensorFlow\\'s visualization toolkit',\n",
    "        'optuna': 'Hyperparameter optimization'\n",
    "    }\n",
    "    \n",
    "    for tool, description in tools.items():\n",
    "        print(f\"  📦 {tool}: {description}\")\n",
    "    \n",
    "    print(\"\\n🎯 Practice Projects:\")\n",
    "    print(\"=\" * 20)\n",
    "    \n",
    "    projects = [\n",
    "        \"Build a chatbot with conversation memory\",\n",
    "        \"Create a code completion tool for your favorite language\",\n",
    "        \"Develop a text summarization service\",\n",
    "        \"Build a creative writing assistant\",\n",
    "        \"Create a question-answering system for documents\",\n",
    "        \"Develop a sentiment analysis API\",\n",
    "        \"Build a language translation tool\"\n",
    "    ]\n",
    "    \n",
    "    for i, project in enumerate(projects, 1):\n",
    "        print(f\"  {i}. {project}\")\n",
    "    \n",
    "    print(\"\\n✨ Thank you for completing the Transformers Fundamentals notebook!\")\n",
    "    print(\"Happy coding and experimenting with local LLMs! 🚀\")\n",
    "\n",
    "display_next_steps()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}