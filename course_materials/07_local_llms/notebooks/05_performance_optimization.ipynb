{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performance Optimization for Local LLMs\\n",
    "\\n",
    "Welcome to the comprehensive guide on optimizing performance for local Large Language Models! This notebook will teach you advanced techniques for maximizing efficiency, monitoring resource usage, and benchmarking different approaches.\\n",
    "\\n",
    "## What You'll Learn\\n",
    "\\n",
    "- Memory optimization techniques and quantization strategies\\n",
    "- Performance profiling and resource monitoring\\n",
    "- Hardware compatibility testing and optimization\\n",
    "- Benchmarking different models and configurations\\n",
    "- Advanced optimization techniques for production use\\n",
    "- Interactive performance visualization and analysis\\n",
    "\\n",
    "## Prerequisites\\n",
    "\\n",
    "- Python 3.8 or higher\\n",
    "- At least 8GB of RAM (16GB+ recommended)\\n",
    "- Optional: CUDA-compatible GPU for acceleration\\n",
    "- Basic understanding of machine learning concepts\\n",
    "- Completion of previous notebooks in this series\\n",
    "\\n",
    "Let's optimize your local LLM performance!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Installation and Setup\\n",
    "\\n",
    "First, let's install the required packages for performance monitoring and optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages for performance optimization\\n",
    "import subprocess\\n",
    "import sys\\n",
    "import platform\\n",
    "\\n",
    "def install_package(package):\\n",
    "    subprocess.check_call([sys.executable, \\\"-m\\\", \\\"pip\\\", \\\"install\\\", package])\\n",
    "\\n",
    "# Performance monitoring and optimization packages\\n",
    "packages = [\\n",
    "    'transformers>=4.30.0',\\n",
    "    'torch>=2.0.0',\\n",
    "    'accelerate>=0.20.0',\\n",
    "    'bitsandbytes',  # For quantization\\n",
    "    'psutil',        # System monitoring\\n",
    "    'GPUtil',        # GPU monitoring\\n",
    "    'memory-profiler', # Memory profiling\\n",
    "    'matplotlib',    # Visualization\\n",
    "    'seaborn',       # Advanced plotting\\n",
    "    'pandas',        # Data analysis\\n",
    "    'numpy',         # Numerical operations\\n",
    "    'ipywidgets',    # Interactive widgets\\n",
    "    'tqdm',          # Progress bars\\n",
    "    'requests'       # For Ollama API calls\\n",
    "]\\n",
    "\\n",
    "print(\\\"Installing performance optimization packages...\\\")\\n",
    "print(\\\"This may take a few minutes.\\\\n\\\")\\n",
    "\\n",
    "for package in packages:\\n",
    "    try:\\n",
    "        base_name = package.split('>=')[0].split('==')[0].replace('-', '_')\\n",
    "        if base_name in ['bitsandbytes', 'GPUtil', 'memory_profiler']:\\n",
    "            # Skip optional packages for import check\\n",
    "            continue\\n",
    "        __import__(base_name)\\n",
    "        print(f\\\"‚úì {base_name} is already installed\\\")\\n",
    "    except ImportError:\\n",
    "        print(f\\\"Installing {package}...\\\")\\n",
    "        try:\\n",
    "            install_package(package)\\n",
    "            print(f\\\"‚úì {package} installed successfully\\\")\\n",
    "        except Exception as e:\\n",
    "            print(f\\\"‚ö†Ô∏è Warning: Could not install {package}: {e}\\\")\\n",
    "\\n",
    "print(\\\"\\\\nüéâ Installation complete!\\\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\\n",
    "import torch\\n",
    "import transformers\\n",
    "from transformers import (\\n",
    "    AutoTokenizer, \\n",
    "    AutoModelForCausalLM, \\n",
    "    pipeline,\\n",
    "    BitsAndBytesConfig\\n",
    ")\\n",
    "import ipywidgets as widgets\\n",
    "from IPython.display import display, HTML, clear_output\\n",
    "import matplotlib.pyplot as plt\\n",
    "import seaborn as sns\\n",
    "import pandas as pd\\n",
    "import numpy as np\\n",
    "import psutil\\n",
    "import time\\n",
    "import gc\\n",
    "import requests\\n",
    "from datetime import datetime\\n",
    "from tqdm.auto import tqdm\\n",
    "import warnings\\n",
    "warnings.filterwarnings('ignore')\\n",
    "\\n",
    "# Try to import optional packages\\n",
    "try:\\n",
    "    import GPUtil\\n",
    "    GPU_MONITORING = True\\n",
    "except ImportError:\\n",
    "    GPU_MONITORING = False\\n",
    "    print(\\\"GPUtil not available - GPU monitoring limited\\\")\\n",
    "\\n",
    "try:\\n",
    "    from memory_profiler import profile\\n",
    "    MEMORY_PROFILER = True\\n",
    "except ImportError:\\n",
    "    MEMORY_PROFILER = False\\n",
    "    print(\\\"memory-profiler not available - detailed memory profiling disabled\\\")\\n",
    "\\n",
    "# Set up plotting style\\n",
    "plt.style.use('default')\\n",
    "sns.set_palette(\\\"husl\\\")\\n",
    "\\n",
    "print(f\\\"üîß PyTorch version: {torch.__version__}\\\")\\n",
    "print(f\\\"ü§ó Transformers version: {transformers.__version__}\\\")\\n",
    "print(f\\\"üñ•Ô∏è System: {platform.system()} {platform.release()}\\\")\\n",
    "print(f\\\"üß† CPU cores: {psutil.cpu_count()}\\\")\\n",
    "print(f\\\"üíæ RAM: {psutil.virtual_memory().total / (1024**3):.1f} GB\\\")\\n",
    "\\n",
    "# Check CUDA availability\\n",
    "if torch.cuda.is_available():\\n",
    "    print(f\\\"üöÄ CUDA available: {torch.cuda.get_device_name(0)}\\\")\\n",
    "    print(f\\\"üéÆ GPU memory: {torch.cuda.get_device_properties(0).total_memory / (1024**3):.1f} GB\\\")\\n",
    "    DEVICE = \\\"cuda\\\"\\n",
    "else:\\n",
    "    print(\\\"üíª CUDA not available - using CPU\\\")\\n",
    "    DEVICE = \\\"cpu\\\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. System Resource Monitoring\\n",
    "\\n",
    "Let's create comprehensive utilities to monitor system resources during model operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PerformanceMonitor:\\n",
    "    \\\"\\\"\\\"Comprehensive performance monitoring for LLM operations\\\"\\\"\\\"\\n",
    "    \\n",
    "    def __init__(self):\\n",
    "        self.measurements = []\\n",
    "        self.baseline_memory = self.get_memory_usage()\\n",
    "        self.start_time = time.time()\\n",
    "    \\n",
    "    def get_memory_usage(self):\\n",
    "        \\\"\\\"\\\"Get current memory usage across all available sources\\\"\\\"\\\"\\n",
    "        memory = psutil.virtual_memory()\\n",
    "        \\n",
    "        result = {\\n",
    "            'ram_used_gb': memory.used / (1024**3),\\n",
    "            'ram_percent': memory.percent,\\n",
    "            'ram_available_gb': memory.available / (1024**3),\\n",
    "            'ram_total_gb': memory.total / (1024**3)\\n",
    "        }\\n",
    "        \\n",
    "        # Add GPU memory if available\\n",
    "        if torch.cuda.is_available():\\n",
    "            try:\\n",
    "                gpu_memory = torch.cuda.memory_stats()\\n",
    "                result.update({\\n",
    "                    'gpu_allocated_gb': torch.cuda.memory_allocated() / (1024**3),\\n",
    "                    'gpu_reserved_gb': torch.cuda.memory_reserved() / (1024**3),\\n",
    "                    'gpu_max_allocated_gb': torch.cuda.max_memory_allocated() / (1024**3),\\n",
    "                    'gpu_total_gb': torch.cuda.get_device_properties(0).total_memory / (1024**3)\\n",
    "                })\\n",
    "                \\n",
    "                # Try to get GPU utilization if GPUtil is available\\n",
    "                if GPU_MONITORING:\\n",
    "                    try:\\n",
    "                        gpu = GPUtil.getGPUs()[0]\\n",
    "                        result['gpu_utilization'] = gpu.load * 100\\n",
    "                        result['gpu_temperature'] = gpu.temperature\\n",
    "                    except:\\n",
    "                        result['gpu_utilization'] = 0\\n",
    "                        result['gpu_temperature'] = 0\\n",
    "            except Exception as e:\\n",
    "                print(f\\\"Warning: Could not get GPU memory stats: {e}\\\")\\n",
    "        else:\\n",
    "            result.update({\\n",
    "                'gpu_allocated_gb': 0,\\n",
    "                'gpu_reserved_gb': 0,\\n",
    "                'gpu_max_allocated_gb': 0,\\n",
    "                'gpu_total_gb': 0,\\n",
    "                'gpu_utilization': 0,\\n",
    "                'gpu_temperature': 0\\n",
    "            })\\n",
    "        \\n",
    "        return result\\n",
    "    \\n",
    "    def get_cpu_usage(self):\\n",
    "        \\\"\\\"\\\"Get CPU usage statistics\\\"\\\"\\\"\\n",
    "        return {\\n",
    "            'cpu_percent': psutil.cpu_percent(interval=1),\\n",
    "            'cpu_count': psutil.cpu_count(),\\n",
    "            'cpu_freq': psutil.cpu_freq().current if psutil.cpu_freq() else 0\\n",
    "        }\\n",
    "    \\n",
    "    def snapshot(self, label=\\\"\\\", include_cpu=False):\\n",
    "        \\\"\\\"\\\"Take a comprehensive snapshot of system resources\\\"\\\"\\\"\\n",
    "        timestamp = datetime.now()\\n",
    "        memory_info = self.get_memory_usage()\\n",
    "        \\n",
    "        snapshot_data = {\\n",
    "            'timestamp': timestamp,\\n",
    "            'elapsed_time': time.time() - self.start_time,\\n",
    "            'label': label,\\n",
    "            **memory_info\\n",
    "        }\\n",
    "        \\n",
    "        if include_cpu:\\n",
    "            cpu_info = self.get_cpu_usage()\\n",
    "            snapshot_data.update(cpu_info)\\n",
    "        \\n",
    "        self.measurements.append(snapshot_data)\\n",
    "        return snapshot_data\\n",
    "    \\n",
    "    def display_current(self, detailed=False):\\n",
    "        \\\"\\\"\\\"Display current resource usage\\\"\\\"\\\"\\n",
    "        snapshot = self.snapshot(include_cpu=detailed)\\n",
    "        \\n",
    "        print(f\\\"üìä System Status at {snapshot['timestamp'].strftime('%H:%M:%S')}\\\")\\n",
    "        print(f\\\"üíæ RAM: {snapshot['ram_used_gb']:.1f}GB / {snapshot['ram_total_gb']:.1f}GB ({snapshot['ram_percent']:.1f}%)\\\")\\n",
    "        \\n",
    "        if torch.cuda.is_available():\\n",
    "            print(f\\\"üéÆ GPU Memory: {snapshot['gpu_allocated_gb']:.1f}GB allocated, {snapshot['gpu_reserved_gb']:.1f}GB reserved\\\")\\n",
    "            if snapshot['gpu_utilization'] > 0:\\n",
    "                print(f\\\"‚ö° GPU Utilization: {snapshot['gpu_utilization']:.1f}%\\\")\\n",
    "            if snapshot['gpu_temperature'] > 0:\\n",
    "                print(f\\\"üå°Ô∏è GPU Temperature: {snapshot['gpu_temperature']:.1f}¬∞C\\\")\\n",
    "        \\n",
    "        if detailed and 'cpu_percent' in snapshot:\\n",
    "            print(f\\\"üß† CPU Usage: {snapshot['cpu_percent']:.1f}%\\\")\\n",
    "            if snapshot['cpu_freq'] > 0:\\n",
    "                print(f\\\"‚ö° CPU Frequency: {snapshot['cpu_freq']:.0f} MHz\\\")\\n",
    "    \\n",
    "    def clear_gpu_cache(self):\\n",
    "        \\\"\\\"\\\"Clear GPU cache and run garbage collection\\\"\\\"\\\"\\n",
    "        if torch.cuda.is_available():\\n",
    "            torch.cuda.empty_cache()\\n",
    "            torch.cuda.reset_peak_memory_stats()\\n",
    "        gc.collect()\\n",
    "        print(\\\"üßπ Cleared GPU cache and ran garbage collection\\\")\\n",
    "    \\n",
    "    def get_memory_delta(self, start_snapshot, end_snapshot):\\n",
    "        \\\"\\\"\\\"Calculate memory usage delta between two snapshots\\\"\\\"\\\"\\n",
    "        return {\\n",
    "            'ram_delta_gb': end_snapshot['ram_used_gb'] - start_snapshot['ram_used_gb'],\\n",
    "            'gpu_delta_gb': end_snapshot['gpu_allocated_gb'] - start_snapshot['gpu_allocated_gb']\\n",
    "        }\\n",
    "\\n",
    "# Initialize performance monitor\\n",
    "monitor = PerformanceMonitor()\\n",
    "print(\\\"üìä Performance monitor initialized\\\")\\n",
    "monitor.display_current(detailed=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Memory Optimization Techniques\\n",
    "\\n",
    "Let's explore various memory optimization strategies including quantization and efficient model loading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OptimizedModelManager:\\n",
    "    \\\"\\\"\\\"Advanced model manager with optimization techniques\\\"\\\"\\\"\\n",
    "    \\n",
    "    def __init__(self):\\n",
    "        self.models = {}\\n",
    "        self.tokenizers = {}\\n",
    "        self.model_configs = {}\\n",
    "        self.device = DEVICE\\n",
    "        print(f\\\"üîß Using device: {self.device}\\\")\\n",
    "    \\n",
    "    def load_model_optimized(self, model_name, optimization_strategy=\\\"none\\\"):\\n",
    "        \\\"\\\"\\\"Load model with various optimization strategies\\\"\\\"\\\"\\n",
    "        print(f\\\"üì• Loading {model_name} with {optimization_strategy} optimization...\\\")\\n",
    "        \\n",
    "        # Take baseline measurement\\n",
    "        before_snapshot = monitor.snapshot(f\\\"Before loading {model_name}\\\")\\n",
    "        \\n",
    "        try:\\n",
    "            # Load tokenizer\\n",
    "            tokenizer = AutoTokenizer.from_pretrained(model_name)\\n",
    "            if tokenizer.pad_token is None:\\n",
    "                tokenizer.pad_token = tokenizer.eos_token\\n",
    "            \\n",
    "            # Configure model loading based on optimization strategy\\n",
    "            model_kwargs = {}\\n",
    "            \\n",
    "            if optimization_strategy == \\\"4bit\\\":\\n",
    "                print(\\\"üîß Applying 4-bit quantization\\\")\\n",
    "                quantization_config = BitsAndBytesConfig(\\n",
    "                    load_in_4bit=True,\\n",
    "                    bnb_4bit_compute_dtype=torch.float16,\\n",
    "                    bnb_4bit_use_double_quant=True,\\n",
    "                    bnb_4bit_quant_type=\\\"nf4\\\"\\n",
    "                )\\n",
    "                model_kwargs[\\\"quantization_config\\\"] = quantization_config\\n",
    "                model_kwargs[\\\"device_map\\\"] = \\\"auto\\\"\\n",
    "            \\n",
    "            elif optimization_strategy == \\\"8bit\\\":\\n",
    "                print(\\\"üîß Applying 8-bit quantization\\\")\\n",
    "                model_kwargs[\\\"load_in_8bit\\\"] = True\\n",
    "                model_kwargs[\\\"device_map\\\"] = \\\"auto\\\"\\n",
    "            \\n",
    "            elif optimization_strategy == \\\"fp16\\\":\\n",
    "                print(\\\"üîß Using FP16 precision\\\")\\n",
    "                model_kwargs[\\\"torch_dtype\\\"] = torch.float16\\n",
    "                if self.device == \\\"cuda\\\":\\n",
    "                    model_kwargs[\\\"device_map\\\"] = \\\"auto\\\"\\n",
    "            \\n",
    "            elif optimization_strategy == \\\"cpu_optimized\\\":\\n",
    "                print(\\\"üîß CPU-optimized loading\\\")\\n",
    "                model_kwargs[\\\"torch_dtype\\\"] = torch.float32\\n",
    "                model_kwargs[\\\"low_cpu_mem_usage\\\"] = True\\n",
    "            \\n",
    "            # Load model\\n",
    "            start_time = time.time()\\n",
    "            model = AutoModelForCausalLM.from_pretrained(\\n",
    "                model_name,\\n",
    "                **model_kwargs\\n",
    "            )\\n",
    "            load_time = time.time() - start_time\\n",
    "            \\n",
    "            # Move to device if not using device_map\\n",
    "            if \\\"device_map\\\" not in model_kwargs and self.device == \\\"cuda\\\":\\n",
    "                model = model.to(self.device)\\n",
    "            \\n",
    "            # Store references\\n",
    "            self.models[model_name] = model\\n",
    "            self.tokenizers[model_name] = tokenizer\\n",
    "            self.model_configs[model_name] = {\\n",
    "                'optimization': optimization_strategy,\\n",
    "                'load_time': load_time,\\n",
    "                'parameters': model.num_parameters() if hasattr(model, 'num_parameters') else 0\\n",
    "            }\\n",
    "            \\n",
    "            # Take after measurement\\n",
    "            after_snapshot = monitor.snapshot(f\\\"After loading {model_name}\\\")\\n",
    "            \\n",
    "            # Calculate memory usage\\n",
    "            memory_delta = monitor.get_memory_delta(before_snapshot, after_snapshot)\\n",
    "            \\n",
    "            print(f\\\"‚úÖ Successfully loaded {model_name}\\\")\\n",
    "            print(f\\\"‚è±Ô∏è Load time: {load_time:.2f} seconds\\\")\\n",
    "            print(f\\\"üíæ RAM increase: {memory_delta['ram_delta_gb']:.2f} GB\\\")\\n",
    "            if torch.cuda.is_available():\\n",
    "                print(f\\\"üéÆ GPU memory increase: {memory_delta['gpu_delta_gb']:.2f} GB\\\")\\n",
    "            \\n",
    "            return True\\n",
    "            \\n",
    "        except Exception as e:\\n",
    "            print(f\\\"‚ùå Error loading {model_name}: {str(e)}\\\")\\n",
    "            return False\\n",
    "    \\n",
    "    def benchmark_inference(self, model_name, test_prompts, max_length=100):\\n",
    "        \\\"\\\"\\\"Benchmark inference performance for a loaded model\\\"\\\"\\\"\\n",
    "        if model_name not in self.models:\\n",
    "            print(f\\\"‚ùå Model {model_name} not loaded\\\")\\n",
    "            return None\\n",
    "        \\n",
    "        model = self.models[model_name]\\n",
    "        tokenizer = self.tokenizers[model_name]\\n",
    "        \\n",
    "        results = []\\n",
    "        \\n",
    "        print(f\\\"üîç Benchmarking {model_name} inference...\\\")\\n",
    "        \\n",
    "        for i, prompt in enumerate(tqdm(test_prompts, desc=\\\"Testing prompts\\\")):\\n",
    "            # Take snapshot before inference\\n",
    "            before_snapshot = monitor.snapshot(f\\\"Before inference {i+1}\\\")\\n",
    "            \\n",
    "            try:\\n",
    "                # Tokenize input\\n",
    "                inputs = tokenizer(prompt, return_tensors=\\\"pt\\\")\\n",
    "                if self.device == \\\"cuda\\\":\\n",
    "                    inputs = {k: v.to(self.device) for k, v in inputs.items()}\\n",
    "                \\n",
    "                # Generate text\\n",
    "                start_time = time.time()\\n",
    "                \\n",
    "                with torch.no_grad():\\n",
    "                    outputs = model.generate(\\n",
    "                        **inputs,\\n",
    "                        max_length=max_length,\\n",
    "                        temperature=0.7,\\n",
    "                        do_sample=True,\\n",
    "                        pad_token_id=tokenizer.eos_token_id\\n",
    "                    )\\n",
    "                \\n",
    "                generation_time = time.time() - start_time\\n",
    "                \\n",
    "                # Decode output\\n",
    "                generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\\n",
    "                generated_text = generated_text[len(prompt):].strip()\\n",
    "                \\n",
    "                # Calculate metrics\\n",
    "                input_tokens = len(inputs['input_ids'][0])\\n",
    "                output_tokens = len(outputs[0]) - input_tokens\\n",
    "                tokens_per_second = output_tokens / generation_time if generation_time > 0 else 0\\n",
    "                \\n",
    "                # Take snapshot after inference\\n",
    "                after_snapshot = monitor.snapshot(f\\\"After inference {i+1}\\\")\\n",
    "                memory_delta = monitor.get_memory_delta(before_snapshot, after_snapshot)\\n",
    "                \\n",
    "                results.append({\\n",
    "                    'prompt_id': i + 1,\\n",
    "                    'prompt': prompt[:50] + \\\"...\\\" if len(prompt) > 50 else prompt,\\n",
    "                    'input_tokens': input_tokens,\\n",
    "                    'output_tokens': output_tokens,\\n",
    "                    'generation_time': generation_time,\\n",
    "                    'tokens_per_second': tokens_per_second,\\n",
    "                    'ram_delta_gb': memory_delta['ram_delta_gb'],\\n",
    "                    'gpu_delta_gb': memory_delta['gpu_delta_gb'],\\n",
    "                    'generated_text': generated_text[:100] + \\\"...\\\" if len(generated_text) > 100 else generated_text\\n",
    "                })\\n",
    "                \\n",
    "            except Exception as e:\\n",
    "                print(f\\\"‚ùå Error in inference {i+1}: {str(e)}\\\")\\n",
    "                continue\\n",
    "        \\n",
    "        return pd.DataFrame(results)\\n",
    "    \\n",
    "    def unload_model(self, model_name):\\n",
    "        \\\"\\\"\\\"Unload a model and free memory\\\"\\\"\\\"\\n",
    "        if model_name in self.models:\\n",
    "            del self.models[model_name]\\n",
    "            del self.tokenizers[model_name]\\n",
    "            del self.model_configs[model_name]\\n",
    "            \\n",
    "            # Clear caches\\n",
    "            monitor.clear_gpu_cache()\\n",
    "            \\n",
    "            print(f\\\"üóëÔ∏è Unloaded {model_name}\\\")\\n",
    "            monitor.display_current()\\n",
    "    \\n",
    "    def get_model_info(self):\\n",
    "        \\\"\\\"\\\"Get information about loaded models\\\"\\\"\\\"\\n",
    "        if not self.models:\\n",
    "            return pd.DataFrame()\\n",
    "        \\n",
    "        info_data = []\\n",
    "        for model_name, config in self.model_configs.items():\\n",
    "            info_data.append({\\n",
    "                'Model': model_name,\\n",
    "                'Optimization': config['optimization'],\\n",
    "                'Load Time (s)': config['load_time'],\\n",
    "                'Parameters': config['parameters']\\n",
    "            })\\n",
    "        \\n",
    "        return pd.DataFrame(info_data)\\n",
    "\\n",
    "# Initialize optimized model manager\\n",
    "opt_manager = OptimizedModelManager()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Interactive Model Loading and Optimization Interface\\n",
    "\\n",
    "Let's create an interactive interface to test different optimization strategies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_optimization_interface():\\n",
    "    \\\"\\\"\\\"Create interactive interface for testing optimization strategies\\\"\\\"\\\"\\n",
    "    \\n",
    "    # Model options (small models for testing)\\n",
    "    model_options = {\\n",
    "        'GPT-2 Small (124M)': 'gpt2',\\n",
    "        'DistilGPT-2 (82M)': 'distilgpt2',\\n",
    "        'GPT-2 Medium (355M)': 'gpt2-medium',\\n",
    "        'Microsoft DialoGPT Small': 'microsoft/DialoGPT-small'\\n",
    "    }\\n",
    "    \\n",
    "    optimization_options = [\\n",
    "        'none',\\n",
    "        'fp16',\\n",
    "        '8bit',\\n",
    "        '4bit',\\n",
    "        'cpu_optimized'\\n",
    "    ]\\n",
    "    \\n",
    "    # Create widgets\\n",
    "    model_dropdown = widgets.Dropdown(\\n",
    "        options=list(model_options.keys()),\\n",
    "        value='DistilGPT-2 (82M)',\\n",
    "        description='Model:',\\n",
    "        style={'description_width': 'initial'}\\n",
    "    )\\n",
    "    \\n",
    "    optimization_dropdown = widgets.Dropdown(\\n",
    "        options=optimization_options,\\n",
    "        value='none',\\n",
    "        description='Optimization:',\\n",
    "        style={'description_width': 'initial'}\\n",
    "    )\\n",
    "    \\n",
    "    load_button = widgets.Button(\\n",
    "        description='Load Model',\\n",
    "        button_style='primary',\\n",
    "        icon='download'\\n",
    "    )\\n",
    "    \\n",
    "    unload_button = widgets.Button(\\n",
    "        description='Unload All',\\n",
    "        button_style='warning',\\n",
    "        icon='trash'\\n",
    "    )\\n",
    "    \\n",
    "    benchmark_button = widgets.Button(\\n",
    "        description='Run Benchmark',\\n",
    "        button_style='success',\\n",
    "        icon='tachometer-alt'\\n",
    "    )\\n",
    "    \\n",
    "    output_area = widgets.Output()\\n",
    "    \\n",
    "    def on_load_click(b):\\n",
    "        with output_area:\\n",
    "            clear_output()\\n",
    "            model_name = model_options[model_dropdown.value]\\n",
    "            optimization = optimization_dropdown.value\\n",
    "            \\n",
    "            print(f\\\"üöÄ Loading {model_name} with {optimization} optimization...\\\")\\n",
    "            success = opt_manager.load_model_optimized(model_name, optimization)\\n",
    "            \\n",
    "            if success:\\n",
    "                print(\\\"\\\\nüìã Currently loaded models:\\\")\\n",
    "                model_info = opt_manager.get_model_info()\\n",
    "                if not model_info.empty:\\n",
    "                    display(model_info)\\n",
    "    \\n",
    "    def on_unload_click(b):\\n",
    "        with output_area:\\n",
    "            clear_output()\\n",
    "            loaded_models = list(opt_manager.models.keys()).copy()\\n",
    "            for model in loaded_models:\\n",
    "                opt_manager.unload_model(model)\\n",
    "            print(\\\"üóëÔ∏è All models unloaded\\\")\\n",
    "    \\n",
    "    def on_benchmark_click(b):\\n",
    "        with output_area:\\n",
    "            clear_output()\\n",
    "            \\n",
    "            loaded_models = list(opt_manager.models.keys())\\n",
    "            if not loaded_models:\\n",
    "                print(\\\"‚ùå No models loaded. Please load a model first.\\\")\\n",
    "                return\\n",
    "            \\n",
    "            # Test prompts for benchmarking\\n",
    "            test_prompts = [\\n",
    "                \\\"Hello, how are you today?\\\",\\n",
    "                \\\"Explain machine learning in simple terms.\\\",\\n",
    "                \\\"Write a short Python function to calculate fibonacci numbers.\\\",\\n",
    "                \\\"What are the benefits of renewable energy?\\\"\\n",
    "            ]\\n",
    "            \\n",
    "            all_results = []\\n",
    "            \\n",
    "            for model_name in loaded_models:\\n",
    "                print(f\\\"üîç Benchmarking {model_name}...\\\")\\n",
    "                results = opt_manager.benchmark_inference(model_name, test_prompts, max_length=80)\\n",
    "                \\n",
    "                if results is not None and not results.empty:\\n",
    "                    results['model'] = model_name\\n",
    "                    results['optimization'] = opt_manager.model_configs[model_name]['optimization']\\n",
    "                    all_results.append(results)\\n",
    "            \\n",
    "            if all_results:\\n",
    "                combined_results = pd.concat(all_results, ignore_index=True)\\n",
    "                \\n",
    "                # Display summary statistics\\n",
    "                print(\\\"\\\\nüìä Benchmark Results Summary:\\\")\\n",
    "                summary = combined_results.groupby(['model', 'optimization']).agg({\\n",
    "                    'generation_time': ['mean', 'std'],\\n",
    "                    'tokens_per_second': ['mean', 'std'],\\n",
    "                    'output_tokens': 'mean'\\n",
    "                }).round(3)\\n",
    "                \\n",
    "                display(summary)\\n",
    "                \\n",
    "                # Create visualization\\n",
    "                create_benchmark_visualization(combined_results)\\n",
    "    \\n",
    "    # Set up event handlers\\n",
    "    load_button.on_click(on_load_click)\\n",
    "    unload_button.on_click(on_unload_click)\\n",
    "    benchmark_button.on_click(on_benchmark_click)\\n",
    "    \\n",
    "    return widgets.VBox([\\n",
    "        widgets.HTML(\\\"<h3>üöÄ Performance Optimization Interface</h3>\\\"),\\n",
    "        widgets.HBox([model_dropdown, optimization_dropdown]),\\n",
    "        widgets.HBox([load_button, unload_button, benchmark_button]),\\n",
    "        output_area\\n",
    "    ])\\n",
    "\\n",
    "display(create_optimization_interface())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Performance Visualization and Analysis\\n",
    "\\n",
    "Let's create comprehensive visualization tools for analyzing performance data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_benchmark_visualization(results_df):\\n",
    "    \\\"\\\"\\\"Create comprehensive visualizations for benchmark results\\\"\\\"\\\"\\n",
    "    \\n",
    "    if results_df.empty:\\n",
    "        print(\\\"‚ùå No results to visualize\\\")\\n",
    "        return\\n",
    "    \\n",
    "    # Set up the plotting area\\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(18, 12))\\n",
    "    fig.suptitle('LLM Performance Optimization Analysis', fontsize=16, fontweight='bold')\\n",
    "    \\n",
    "    # 1. Generation Time by Model and Optimization\\n",
    "    sns.boxplot(data=results_df, x='optimization', y='generation_time', hue='model', ax=axes[0,0])\\n",
    "    axes[0,0].set_title('Generation Time Distribution')\\n",
    "    axes[0,0].set_ylabel('Time (seconds)')\\n",
    "    axes[0,0].tick_params(axis='x', rotation=45)\\n",
    "    \\n",
    "    # 2. Tokens per Second by Optimization\\n",
    "    sns.barplot(data=results_df, x='optimization', y='tokens_per_second', hue='model', ax=axes[0,1])\\n",
    "    axes[0,1].set_title('Average Tokens per Second')\\n",
    "    axes[0,1].set_ylabel('Tokens/sec')\\n",
    "    axes[0,1].tick_params(axis='x', rotation=45)\\n",
    "    \\n",
    "    # 3. Memory Usage Comparison\\n",
    "    memory_data = results_df.groupby(['model', 'optimization']).agg({\\n",
    "        'ram_delta_gb': 'mean',\\n",
    "        'gpu_delta_gb': 'mean'\\n",
    "    }).reset_index()\\n",
    "    \\n",
    "    x_pos = np.arange(len(memory_data))\\n",
    "    width = 0.35\\n",
    "    \\n",
    "    axes[0,2].bar(x_pos - width/2, memory_data['ram_delta_gb'], width, label='RAM', alpha=0.8)\\n",
    "    axes[0,2].bar(x_pos + width/2, memory_data['gpu_delta_gb'], width, label='GPU', alpha=0.8)\\n",
    "    axes[0,2].set_title('Memory Usage per Inference')\\n",
    "    axes[0,2].set_ylabel('Memory (GB)')\\n",
    "    axes[0,2].set_xticks(x_pos)\\n",
    "    axes[0,2].set_xticklabels([f\\\"{row['model']}\\\\n{row['optimization']}\\\" for _, row in memory_data.iterrows()], rotation=45)\\n",
    "    axes[0,2].legend()\\n",
    "    \\n",
    "    # 4. Performance vs Memory Trade-off\\n",
    "    scatter_data = results_df.groupby(['model', 'optimization']).agg({\\n",
    "        'tokens_per_second': 'mean',\\n",
    "        'ram_delta_gb': 'mean',\\n",
    "        'gpu_delta_gb': 'mean'\\n",
    "    }).reset_index()\\n",
    "    \\n",
    "    scatter_data['total_memory'] = scatter_data['ram_delta_gb'] + scatter_data['gpu_delta_gb']\\n",
    "    \\n",
    "    for model in scatter_data['model'].unique():\\n",
    "        model_data = scatter_data[scatter_data['model'] == model]\\n",
    "        axes[1,0].scatter(model_data['total_memory'], model_data['tokens_per_second'], \\n",
    "                         label=model, s=100, alpha=0.7)\\n",
    "        \\n",
    "        # Add optimization labels\\n",
    "        for _, row in model_data.iterrows():\\n",
    "            axes[1,0].annotate(row['optimization'], \\n",
    "                             (row['total_memory'], row['tokens_per_second']),\\n",
    "                             xytext=(5, 5), textcoords='offset points', fontsize=8)\\n",
    "    \\n",
    "    axes[1,0].set_title('Performance vs Memory Trade-off')\\n",
    "    axes[1,0].set_xlabel('Total Memory Usage (GB)')\\n",
    "    axes[1,0].set_ylabel('Tokens per Second')\\n",
    "    axes[1,0].legend()\\n",
    "    axes[1,0].grid(True, alpha=0.3)\\n",
    "    \\n",
    "    # 5. Optimization Strategy Comparison\\n",
    "    opt_comparison = results_df.groupby('optimization').agg({\\n",
    "        'generation_time': 'mean',\\n",
    "        'tokens_per_second': 'mean',\\n",
    "        'ram_delta_gb': 'mean',\\n",
    "        'gpu_delta_gb': 'mean'\\n",
    "    }).reset_index()\\n",
    "    \\n",
    "    # Normalize metrics for radar chart effect\\n",
    "    metrics = ['generation_time', 'tokens_per_second', 'ram_delta_gb', 'gpu_delta_gb']\\n",
    "    opt_comparison_norm = opt_comparison.copy()\\n",
    "    \\n",
    "    for metric in metrics:\\n",
    "        if metric == 'generation_time':  # Lower is better\\n",
    "            opt_comparison_norm[metric] = 1 / (opt_comparison[metric] + 0.001)\\n",
    "        else:\\n",
    "            opt_comparison_norm[metric] = opt_comparison[metric]\\n",
    "    \\n",
    "    x_pos = np.arange(len(opt_comparison))\\n",
    "    axes[1,1].bar(x_pos, opt_comparison['tokens_per_second'], alpha=0.7)\\n",
    "    axes[1,1].set_title('Tokens/sec by Optimization Strategy')\\n",
    "    axes[1,1].set_xlabel('Optimization Strategy')\\n",
    "    axes[1,1].set_ylabel('Tokens per Second')\\n",
    "    axes[1,1].set_xticks(x_pos)\\n",
    "    axes[1,1].set_xticklabels(opt_comparison['optimization'], rotation=45)\\n",
    "    \\n",
    "    # 6. Efficiency Score (tokens/sec per GB memory)\\n",
    "    efficiency_data = scatter_data.copy()\\n",
    "    efficiency_data['efficiency'] = efficiency_data['tokens_per_second'] / (efficiency_data['total_memory'] + 0.001)\\n",
    "    \\n",
    "    sns.barplot(data=efficiency_data, x='optimization', y='efficiency', hue='model', ax=axes[1,2])\\n",
    "    axes[1,2].set_title('Memory Efficiency Score')\\n",
    "    axes[1,2].set_ylabel('Tokens/sec per GB')\\n",
    "    axes[1,2].tick_params(axis='x', rotation=45)\\n",
    "    \\n",
    "    plt.tight_layout()\\n",
    "    plt.show()\\n",
    "    \\n",
    "    # Print detailed analysis\\n",
    "    print(\\\"\\\\nüìà Performance Analysis Summary:\\\")\\n",
    "    print(\\\"=\\\" * 50)\\n",
    "    \\n",
    "    best_speed = efficiency_data.loc[efficiency_data['tokens_per_second'].idxmax()]\\n",
    "    best_memory = efficiency_data.loc[efficiency_data['total_memory'].idxmin()]\\n",
    "    best_efficiency = efficiency_data.loc[efficiency_data['efficiency'].idxmax()]\\n",
    "    \\n",
    "    print(f\\\"üöÄ Fastest Generation: {best_speed['model']} with {best_speed['optimization']} ({best_speed['tokens_per_second']:.2f} tokens/sec)\\\")\\n",
    "    print(f\\\"üíæ Lowest Memory: {best_memory['model']} with {best_memory['optimization']} ({best_memory['total_memory']:.2f} GB)\\\")\\n",
    "    print(f\\\"‚ö° Most Efficient: {best_efficiency['model']} with {best_efficiency['optimization']} ({best_efficiency['efficiency']:.2f} tokens/sec/GB)\\\")\\n",
    "    \\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Hardware Compatibility Testing\\n",
    "\\n",
    "Let's create tools to test hardware compatibility and optimize for different system configurations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HardwareCompatibilityTester:\\n",
    "    \\\"\\\"\\\"Test hardware compatibility and performance across different configurations\\\"\\\"\\\"\\n",
    "    \\n",
    "    def __init__(self):\\n",
    "        self.system_info = self.get_system_info()\\n",
    "        self.compatibility_results = {}\\n",
    "    \\n",
    "    def get_system_info(self):\\n",
    "        \\\"\\\"\\\"Gather comprehensive system information\\\"\\\"\\\"\\n",
    "        info = {\\n",
    "            'platform': platform.system(),\\n",
    "            'platform_release': platform.release(),\\n",
    "            'platform_version': platform.version(),\\n",
    "            'architecture': platform.machine(),\\n",
    "            'processor': platform.processor(),\\n",
    "            'cpu_count': psutil.cpu_count(),\\n",
    "            'cpu_count_logical': psutil.cpu_count(logical=True),\\n",
    "            'memory_total_gb': psutil.virtual_memory().total / (1024**3),\\n",
    "            'python_version': platform.python_version(),\\n",
    "            'torch_version': torch.__version__,\\n",
    "            'transformers_version': transformers.__version__\\n",
    "        }\\n",
    "        \\n",
    "        # Add CUDA information if available\\n",
    "        if torch.cuda.is_available():\\n",
    "            info.update({\\n",
    "                'cuda_available': True,\\n",
    "                'cuda_version': torch.version.cuda,\\n",
    "                'gpu_name': torch.cuda.get_device_name(0),\\n",
    "                'gpu_memory_gb': torch.cuda.get_device_properties(0).total_memory / (1024**3),\\n",
    "                'gpu_compute_capability': torch.cuda.get_device_capability(0)\\n",
    "            })\\n",
    "        else:\\n",
    "            info.update({\\n",
    "                'cuda_available': False,\\n",
    "                'cuda_version': None,\\n",
    "                'gpu_name': None,\\n",
    "                'gpu_memory_gb': 0,\\n",
    "                'gpu_compute_capability': None\\n",
    "            })\\n",
    "        \\n",
    "        return info\\n",
    "    \\n",
    "    def test_memory_limits(self):\\n",
    "        \\\"\\\"\\\"Test system memory limits and recommendations\\\"\\\"\\\"\\n",
    "        print(\\\"üß™ Testing Memory Limits and Recommendations...\\\")\\n",
    "        \\n",
    "        total_ram = self.system_info['memory_total_gb']\\n",
    "        available_ram = psutil.virtual_memory().available / (1024**3)\\n",
    "        \\n",
    "        recommendations = {\\n",
    "            'system_ram_gb': total_ram,\\n",
    "            'available_ram_gb': available_ram,\\n",
    "            'recommended_models': [],\\n",
    "            'optimization_strategies': []\\n",
    "        }\\n",
    "        \\n",
    "        # Model size recommendations based on available RAM\\n",
    "        if available_ram >= 16:\\n",
    "            recommendations['recommended_models'].extend([\\n",
    "                'GPT-2 Medium (355M)',\\n",
    "                'GPT-2 Large (774M)',\\n",
    "                'Small 7B models with quantization'\\n",
    "            ])\\n",
    "            recommendations['optimization_strategies'].extend(['none', 'fp16', '8bit'])\\n",
    "        elif available_ram >= 8:\\n",
    "            recommendations['recommended_models'].extend([\\n",
    "                'GPT-2 Small (124M)',\\n",
    "                'DistilGPT-2 (82M)',\\n",
    "                'GPT-2 Medium with quantization'\\n",
    "            ])\\n",
    "            recommendations['optimization_strategies'].extend(['fp16', '8bit', '4bit'])\\n",
    "        elif available_ram >= 4:\\n",
    "            recommendations['recommended_models'].extend([\\n",
    "                'DistilGPT-2 (82M)',\\n",
    "                'Small models only'\\n",
    "            ])\\n",
    "            recommendations['optimization_strategies'].extend(['4bit', 'cpu_optimized'])\\n",
    "        else:\\n",
    "            recommendations['recommended_models'].append('Very small models only')\\n",
    "            recommendations['optimization_strategies'].append('cpu_optimized')\\n",
    "        \\n",
    "        # GPU recommendations\\n",
    "        if self.system_info['cuda_available']:\\n",
    "            gpu_memory = self.system_info['gpu_memory_gb']\\n",
    "            recommendations['gpu_memory_gb'] = gpu_memory\\n",
    "            \\n",
    "            if gpu_memory >= 8:\\n",
    "                recommendations['gpu_recommendations'] = 'Can run medium models with fp16'\\n",
    "            elif gpu_memory >= 4:\\n",
    "                recommendations['gpu_recommendations'] = 'Small to medium models with quantization'\\n",
    "            else:\\n",
    "                recommendations['gpu_recommendations'] = 'Small models only, consider CPU'\\n",
    "        else:\\n",
    "            recommendations['gpu_recommendations'] = 'No GPU available - CPU only'\\n",
    "        \\n",
    "        return recommendations\\n",
    "    \\n",
    "    def benchmark_hardware_performance(self):\\n",
    "        \\\"\\\"\\\"Benchmark basic hardware performance for LLM operations\\\"\\\"\\\"\\n",
    "        print(\\\"üèÉ‚Äç‚ôÇÔ∏è Running Hardware Performance Benchmark...\\\")\\n",
    "        \\n",
    "        results = {\\n",
    "            'cpu_benchmark': self._benchmark_cpu(),\\n",
    "            'memory_benchmark': self._benchmark_memory(),\\n",
    "            'gpu_benchmark': self._benchmark_gpu() if torch.cuda.is_available() else None\\n",
    "        }\\n",
    "        \\n",
    "        return results\\n",
    "    \\n",
    "    def _benchmark_cpu(self):\\n",
    "        \\\"\\\"\\\"Benchmark CPU performance\\\"\\\"\\\"\\n",
    "        print(\\\"  üß† Testing CPU performance...\\\")\\n",
    "        \\n",
    "        # Simple matrix multiplication benchmark\\n",
    "        size = 1000\\n",
    "        a = torch.randn(size, size)\\n",
    "        b = torch.randn(size, size)\\n",
    "        \\n",
    "        start_time = time.time()\\n",
    "        for _ in range(5):\\n",
    "            c = torch.mm(a, b)\\n",
    "        cpu_time = (time.time() - start_time) / 5\\n",
    "        \\n",
    "        return {\\n",
    "            'matrix_mult_time_sec': cpu_time,\\n",
    "            'operations_per_sec': (size * size * size) / cpu_time,\\n",
    "            'cpu_utilization': psutil.cpu_percent(interval=1)\\n",
    "        }\\n",
    "    \\n",
    "    def _benchmark_memory(self):\\n",
    "        \\\"\\\"\\\"Benchmark memory performance\\\"\\\"\\\"\\n",
    "        print(\\\"  üíæ Testing memory performance...\\\")\\n",
    "        \\n",
    "        # Memory allocation and access benchmark\\n",
    "        size = 100_000_000  # 100M elements\\n",
    "        \\n",
    "        start_time = time.time()\\n",
    "        large_tensor = torch.randn(size)\\n",
    "        allocation_time = time.time() - start_time\\n",
    "        \\n",
    "        start_time = time.time()\\n",
    "        result = large_tensor.sum()\\n",
    "        access_time = time.time() - start_time\\n",
    "        \\n",
    "        del large_tensor\\n",
    "        gc.collect()\\n",
    "        \\n",
    "        return {\\n",
    "            'allocation_time_sec': allocation_time,\\n",
    "            'access_time_sec': access_time,\\n",
    "            'memory_bandwidth_gb_per_sec': (size * 4) / (1024**3) / access_time\\n",
    "        }\\n",
    "    \\n",
    "    def _benchmark_gpu(self):\\n",
    "        \\\"\\\"\\\"Benchmark GPU performance\\\"\\\"\\\"\\n",
    "        print(\\\"  üéÆ Testing GPU performance...\\\")\\n",
    "        \\n",
    "        if not torch.cuda.is_available():\\n",
    "            return None\\n",
    "        \\n",
    "        device = torch.device('cuda')\\n",
    "        \\n",
    "        # GPU matrix multiplication benchmark\\n",
    "        size = 2000\\n",
    "        a = torch.randn(size, size, device=device)\\n",
    "        b = torch.randn(size, size, device=device)\\n",
    "        \\n",
    "        # Warm up\\n",
    "        for _ in range(3):\\n",
    "            torch.mm(a, b)\\n",
    "        torch.cuda.synchronize()\\n",
    "        \\n",
    "        start_time = time.time()\\n",
    "        for _ in range(10):\\n",
    "            c = torch.mm(a, b)\\n",
    "        torch.cuda.synchronize()\\n",
    "        gpu_time = (time.time() - start_time) / 10\\n",
    "        \\n",
    "        # Memory transfer benchmark\\n",
    "        cpu_tensor = torch.randn(10000, 10000)\\n",
    "        \\n",
    "        start_time = time.time()\\n",
    "        gpu_tensor = cpu_tensor.to(device)\\n",
    "        torch.cuda.synchronize()\\n",
    "        transfer_time = time.time() - start_time\\n",
    "        \\n",
    "        return {\\n",
    "            'matrix_mult_time_sec': gpu_time,\\n",
    "            'operations_per_sec': (size * size * size) / gpu_time,\\n",
    "            'memory_transfer_time_sec': transfer_time,\\n",
    "            'memory_transfer_gb_per_sec': (10000 * 10000 * 4) / (1024**3) / transfer_time\\n",
    "        }\\n",
    "    \\n",
    "    def generate_compatibility_report(self):\\n",
    "        \\\"\\\"\\\"Generate a comprehensive compatibility and optimization report\\\"\\\"\\\"\\n",
    "        print(\\\"üìã Generating Hardware Compatibility Report...\\\")\\n",
    "        \\n",
    "        memory_recommendations = self.test_memory_limits()\\n",
    "        performance_results = self.benchmark_hardware_performance()\\n",
    "        \\n",
    "        report = {\\n",
    "            'system_info': self.system_info,\\n",
    "            'memory_recommendations': memory_recommendations,\\n",
    "            'performance_benchmarks': performance_results,\\n",
    "            'optimization_recommendations': self._generate_optimization_recommendations(\\n",
    "                memory_recommendations, performance_results\\n",
    "            )\\n",
    "        }\\n",
    "        \\n",
    "        self._display_compatibility_report(report)\\n",
    "        return report\\n",
    "    \\n",
    "    def _generate_optimization_recommendations(self, memory_rec, perf_results):\\n",
    "        \\\"\\\"\\\"Generate specific optimization recommendations\\\"\\\"\\\"\\n",
    "        recommendations = []\\n",
    "        \\n",
    "        # Memory-based recommendations\\n",
    "        if memory_rec['available_ram_gb'] < 8:\\n",
    "            recommendations.append(\\\"Use 4-bit quantization for all models\\\")\\n",
    "            recommendations.append(\\\"Consider CPU-optimized loading\\\")\\n",
    "        elif memory_rec['available_ram_gb'] < 16:\\n",
    "            recommendations.append(\\\"Use 8-bit quantization for larger models\\\")\\n",
    "            recommendations.append(\\\"FP16 precision for medium models\\\")\\n",
    "        \\n",
    "        # GPU-based recommendations\\n",
    "        if self.system_info['cuda_available']:\\n",
    "            if self.system_info['gpu_memory_gb'] < 4:\\n",
    "                recommendations.append(\\\"Consider CPU inference for larger models\\\")\\n",
    "            else:\\n",
    "                recommendations.append(\\\"GPU acceleration recommended\\\")\\n",
    "                recommendations.append(\\\"Use device_map='auto' for optimal GPU utilization\\\")\\n",
    "        else:\\n",
    "            recommendations.append(\\\"CPU-only inference - optimize for CPU performance\\\")\\n",
    "            recommendations.append(\\\"Use low_cpu_mem_usage=True for large models\\\")\\n",
    "        \\n",
    "        # Performance-based recommendations\\n",
    "        if perf_results['cpu_benchmark']:\\n",
    "            cpu_perf = perf_results['cpu_benchmark']['operations_per_sec']\\n",
    "            if cpu_perf < 1e9:  # Less than 1 billion ops/sec\\n",
    "                recommendations.append(\\\"CPU performance is limited - prioritize quantization\\\")\\n",
    "        \\n",
    "        return recommendations\\n",
    "    \\n",
    "    def _display_compatibility_report(self, report):\\n",
    "        \\\"\\\"\\\"Display the compatibility report in a formatted way\\\"\\\"\\\"\\n",
    "        print(\\\"\\\\n\" + \\\"=\\\" * 60)\\n",
    "        print(\\\"üñ•Ô∏è  HARDWARE COMPATIBILITY REPORT\\\")\\n",
    "        print(\\\"=\\\" * 60)\\n",
    "        \\n",
    "        # System Information\\n",
    "        print(\\\"\\\\nüìä System Information:\\\")\\n",
    "        sys_info = report['system_info']\\n",
    "        print(f\\\"  Platform: {sys_info['platform']} {sys_info['platform_release']}\\\")\\n",
    "        print(f\\\"  CPU: {sys_info['processor']} ({sys_info['cpu_count']} cores)\\\")\\n",
    "        print(f\\\"  RAM: {sys_info['memory_total_gb']:.1f} GB\\\")\\n",
    "        if sys_info['cuda_available']:\\n",
    "            print(f\\\"  GPU: {sys_info['gpu_name']} ({sys_info['gpu_memory_gb']:.1f} GB)\\\")\\n",
    "        else:\\n",
    "            print(\\\"  GPU: Not available\\\")\\n",
    "        \\n",
    "        # Memory Recommendations\\n",
    "        print(\\\"\\\\nüíæ Memory Recommendations:\\\")\\n",
    "        mem_rec = report['memory_recommendations']\\n",
    "        print(f\\\"  Available RAM: {mem_rec['available_ram_gb']:.1f} GB\\\")\\n",
    "        print(\\\"  Recommended Models:\\\")\\n",
    "        for model in mem_rec['recommended_models']:\\n",
    "            print(f\\\"    ‚Ä¢ {model}\\\")\\n",
    "        print(\\\"  Recommended Optimizations:\\\")\\n",
    "        for opt in mem_rec['optimization_strategies']:\\n",
    "            print(f\\\"    ‚Ä¢ {opt}\\\")\\n",
    "        \\n",
    "        # Performance Results\\n",
    "        print(\\\"\\\\nüèÉ‚Äç‚ôÇÔ∏è Performance Benchmarks:\\\")\\n",
    "        perf = report['performance_benchmarks']\\n",
    "        if perf['cpu_benchmark']:\\n",
    "            cpu = perf['cpu_benchmark']\\n",
    "            print(f\\\"  CPU: {cpu['operations_per_sec']:.2e} ops/sec\\\")\\n",
    "        if perf['gpu_benchmark']:\\n",
    "            gpu = perf['gpu_benchmark']\\n",
    "            print(f\\\"  GPU: {gpu['operations_per_sec']:.2e} ops/sec\\\")\\n",
    "            print(f\\\"  GPU Memory Transfer: {gpu['memory_transfer_gb_per_sec']:.2f} GB/sec\\\")\\n",
    "        \\n",
    "        # Optimization Recommendations\\n",
    "        print(\\\"\\\\nüöÄ Optimization Recommendations:\\\")\\n",
    "        for rec in report['optimization_recommendations']:\\n",
    "            print(f\\\"  ‚Ä¢ {rec}\\\")\\n",
    "        \\n",
    "        print(\\\"\\\\n\" + \\\"=\\\" * 60)\\n",
    "\\n",
    "# Initialize hardware compatibility tester\\n",
    "hw_tester = HardwareCompatibilityTester()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run hardware compatibility test\\n",
    "compatibility_report = hw_tester.generate_compatibility_report()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Ollama vs Transformers Performance Comparison\\n",
    "\\n",
    "Let's create a comprehensive comparison between Ollama and Hugging Face Transformers approaches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OllamaTransformersComparison:\\n",
    "    \\\"\\\"\\\"Compare performance between Ollama and Transformers approaches\\\"\\\"\\\"\\n",
    "    \\n",
    "    def __init__(self):\\n",
    "        self.ollama_available = self._check_ollama_availability()\\n",
    "        self.comparison_results = {}\\n",
    "    \\n",
    "    def _check_ollama_availability(self):\\n",
    "        \\\"\\\"\\\"Check if Ollama is available and running\\\"\\\"\\\"\\n",
    "        try:\\n",
    "            response = requests.get('http://localhost:11434/api/tags', timeout=5)\\n",
    "            return response.status_code == 200\\n",
    "        except:\\n",
    "            return False\\n",
    "    \\n",
    "    def benchmark_ollama(self, model_name, test_prompts):\\n",
    "        \\\"\\\"\\\"Benchmark Ollama performance\\\"\\\"\\\"\\n",
    "        if not self.ollama_available:\\n",
    "            print(\\\"‚ùå Ollama not available. Please install and start Ollama.\\\")\\n",
    "            return None\\n",
    "        \\n",
    "        print(f\\\"üîç Benchmarking Ollama with {model_name}...\\\")\\n",
    "        results = []\\n",
    "        \\n",
    "        for i, prompt in enumerate(tqdm(test_prompts, desc=\\\"Testing Ollama\\\")):\\n",
    "            before_snapshot = monitor.snapshot(f\\\"Ollama before {i+1}\\\")\\n",
    "            \\n",
    "            try:\\n",
    "                start_time = time.time()\\n",
    "                \\n",
    "                response = requests.post('http://localhost:11434/api/generate', \\n",
    "                    json={\\n",
    "                        'model': model_name,\\n",
    "                        'prompt': prompt,\\n",
    "                        'stream': False,\\n",
    "                        'options': {\\n",
    "                            'temperature': 0.7,\\n",
    "                            'num_predict': 100\\n",
    "                        }\\n",
    "                    },\\n",
    "                    timeout=60\\n",
    "                )\\n",
    "                \\n",
    "                generation_time = time.time() - start_time\\n",
    "                \\n",
    "                if response.status_code == 200:\\n",
    "                    result_data = response.json()\\n",
    "                    generated_text = result_data.get('response', '')\\n",
    "                    \\n",
    "                    # Estimate tokens (rough approximation)\\n",
    "                    output_tokens = len(generated_text.split())\\n",
    "                    tokens_per_second = output_tokens / generation_time if generation_time > 0 else 0\\n",
    "                    \\n",
    "                    after_snapshot = monitor.snapshot(f\\\"Ollama after {i+1}\\\")\\n",
    "                    memory_delta = monitor.get_memory_delta(before_snapshot, after_snapshot)\\n",
    "                    \\n",
    "                    results.append({\\n",
    "                        'prompt_id': i + 1,\\n",
    "                        'prompt': prompt[:50] + \\\"...\\\" if len(prompt) > 50 else prompt,\\n",
    "                        'output_tokens': output_tokens,\\n",
    "                        'generation_time': generation_time,\\n",
    "                        'tokens_per_second': tokens_per_second,\\n",
    "                        'ram_delta_gb': memory_delta['ram_delta_gb'],\\n",
    "                        'gpu_delta_gb': memory_delta['gpu_delta_gb'],\\n",
    "                        'approach': 'Ollama',\\n",
    "                        'model': model_name\\n",
    "                    })\\n",
    "                \\n",
    "            except Exception as e:\\n",
    "                print(f\\\"‚ùå Error with Ollama inference {i+1}: {str(e)}\\\")\\n",
    "                continue\\n",
    "        \\n",
    "        return pd.DataFrame(results)\\n",
    "    \\n",
    "    def create_comparison_visualization(self, ollama_results, transformers_results):\\n",
    "        \\\"\\\"\\\"Create visualization comparing Ollama and Transformers\\\"\\\"\\\"\\n",
    "        \\n",
    "        if ollama_results is None or transformers_results is None:\\n",
    "            print(\\\"‚ùå Cannot create comparison - missing results\\\")\\n",
    "            return\\n",
    "        \\n",
    "        # Combine results\\n",
    "        combined_results = pd.concat([ollama_results, transformers_results], ignore_index=True)\\n",
    "        \\n",
    "        # Create comparison plots\\n",
    "        fig, axes = plt.subplots(2, 2, figsize=(15, 10))\\n",
    "        fig.suptitle('Ollama vs Transformers Performance Comparison', fontsize=16, fontweight='bold')\\n",
    "        \\n",
    "        # 1. Generation Time Comparison\\n",
    "        sns.boxplot(data=combined_results, x='approach', y='generation_time', ax=axes[0,0])\\n",
    "        axes[0,0].set_title('Generation Time Distribution')\\n",
    "        axes[0,0].set_ylabel('Time (seconds)')\\n",
    "        \\n",
    "        # 2. Tokens per Second Comparison\\n",
    "        sns.barplot(data=combined_results, x='approach', y='tokens_per_second', ax=axes[0,1])\\n",
    "        axes[0,1].set_title('Average Tokens per Second')\\n",
    "        axes[0,1].set_ylabel('Tokens/sec')\\n",
    "        \\n",
    "        # 3. Memory Usage Comparison\\n",
    "        memory_comparison = combined_results.groupby('approach').agg({\\n",
    "            'ram_delta_gb': 'mean',\\n",
    "            'gpu_delta_gb': 'mean'\\n",
    "        }).reset_index()\\n",
    "        \\n",
    "        x_pos = np.arange(len(memory_comparison))\\n",
    "        width = 0.35\\n",
    "        \\n",
    "        axes[1,0].bar(x_pos - width/2, memory_comparison['ram_delta_gb'], width, label='RAM', alpha=0.8)\\n",
    "        axes[1,0].bar(x_pos + width/2, memory_comparison['gpu_delta_gb'], width, label='GPU', alpha=0.8)\\n",
    "        axes[1,0].set_title('Memory Usage per Inference')\\n",
    "        axes[1,0].set_ylabel('Memory (GB)')\\n",
    "        axes[1,0].set_xticks(x_pos)\\n",
    "        axes[1,0].set_xticklabels(memory_comparison['approach'])\\n",
    "        axes[1,0].legend()\\n",
    "        \\n",
    "        # 4. Performance Summary Table\\n",
    "        summary_stats = combined_results.groupby('approach').agg({\\n",
    "            'generation_time': ['mean', 'std'],\\n",
    "            'tokens_per_second': ['mean', 'std'],\\n",
    "            'ram_delta_gb': 'mean',\\n",
    "            'gpu_delta_gb': 'mean'\\n",
    "        }).round(3)\\n",
    "        \\n",
    "        # Create text summary in the last subplot\\n",
    "        axes[1,1].axis('off')\\n",
    "        summary_text = \\\"Performance Summary:\\\\n\\\\n\\\"\\n",
    "        \\n",
    "        for approach in combined_results['approach'].unique():\\n",
    "            approach_data = combined_results[combined_results['approach'] == approach]\\n",
    "            avg_time = approach_data['generation_time'].mean()\\n",
    "            avg_tokens_sec = approach_data['tokens_per_second'].mean()\\n",
    "            avg_ram = approach_data['ram_delta_gb'].mean()\\n",
    "            \\n",
    "            summary_text += f\\\"{approach}:\\\\n\\\"\\n",
    "            summary_text += f\\\"  Avg Time: {avg_time:.2f}s\\\\n\\\"\\n",
    "            summary_text += f\\\"  Tokens/sec: {avg_tokens_sec:.1f}\\\\n\\\"\\n",
    "            summary_text += f\\\"  RAM Usage: {avg_ram:.3f}GB\\\\n\\\\n\\\"\\n",
    "        \\n",
    "        axes[1,1].text(0.1, 0.9, summary_text, transform=axes[1,1].transAxes, \\n",
    "                      fontsize=12, verticalalignment='top', fontfamily='monospace')\\n",
    "        axes[1,1].set_title('Summary Statistics')\\n",
    "        \\n",
    "        plt.tight_layout()\\n",
    "        plt.show()\\n",
    "        \\n",
    "        return combined_results\\n",
    "\\n",
    "# Initialize comparison tool\\n",
    "comparison_tool = OllamaTransformersComparison()\\n",
    "\\n",
    "if comparison_tool.ollama_available:\\n",
    "    print(\\\"‚úÖ Ollama is available for comparison\\\")\\n",
    "else:\\n",
    "    print(\\\"‚ö†Ô∏è Ollama not available. Install Ollama to run comparisons.\\\")\\n",
    "    print(\\\"   Visit: https://ollama.ai for installation instructions\\\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Advanced Optimization Techniques\\n",
    "\\n",
    "Let's explore advanced optimization techniques for production use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_advanced_optimization_guide():\\n",
    "    \\\"\\\"\\\"Create an interactive guide for advanced optimization techniques\\\"\\\"\\\"\\n",
    "    \\n",
    "    techniques = {\\n",
    "        'Model Quantization': {\\n",
    "            'description': 'Reduce model precision to save memory and increase speed',\\n",
    "            'techniques': [\\n",
    "                '4-bit quantization with NF4',\\n",
    "                '8-bit quantization with LLM.int8()',\\n",
    "                'Dynamic quantization',\\n",
    "                'Post-training quantization'\\n",
    "            ],\\n",
    "            'pros': ['Significant memory reduction', 'Faster inference', 'Enables larger models on smaller hardware'],\\n",
    "            'cons': ['Potential quality loss', 'Setup complexity', 'Limited model support']\\n",
    "        },\\n",
    "        'Memory Management': {\\n",
    "            'description': 'Optimize memory usage patterns and allocation',\\n",
    "            'techniques': [\\n",
    "                'Gradient checkpointing',\\n",
    "                'Model sharding across devices',\\n",
    "                'Offloading to CPU/disk',\\n",
    "                'Memory mapping for large models'\\n",
    "            ],\\n",
    "            'pros': ['Handle larger models', 'Better resource utilization', 'Reduced OOM errors'],\\n",
    "            'cons': ['Increased complexity', 'Potential speed trade-offs', 'Hardware dependent']\\n",
    "        },\\n",
    "        'Inference Optimization': {\\n",
    "            'description': 'Optimize the inference process for better performance',\\n",
    "            'techniques': [\\n",
    "                'KV-cache optimization',\\n",
    "                'Batched inference',\\n",
    "                'Speculative decoding',\\n",
    "                'Early stopping strategies'\\n",
    "            ],\\n",
    "            'pros': ['Faster generation', 'Better throughput', 'Reduced latency'],\\n",
    "            'cons': ['Implementation complexity', 'Model-specific optimizations', 'Quality trade-offs']\\n",
    "        },\\n",
    "        'Hardware Acceleration': {\\n",
    "            'description': 'Leverage specialized hardware for better performance',\\n",
    "            'techniques': [\\n",
    "                'CUDA optimization',\\n",
    "                'Mixed precision training',\\n",
    "                'Tensor parallelism',\\n",
    "                'Pipeline parallelism'\\n",
    "            ],\\n",
    "            'pros': ['Maximum performance', 'Scalability', 'Efficient resource use'],\\n",
    "            'cons': ['Hardware requirements', 'Setup complexity', 'Cost considerations']\\n",
    "        }\\n",
    "    }\\n",
    "    \\n",
    "    # Create interactive interface\\n",
    "    technique_dropdown = widgets.Dropdown(\\n",
    "        options=list(techniques.keys()),\\n",
    "        description='Technique:',\\n",
    "        style={'description_width': 'initial'}\\n",
    "    )\\n",
    "    \\n",
    "    output_area = widgets.Output()\\n",
    "    \\n",
    "    def display_technique_info(change):\\n",
    "        with output_area:\\n",
    "            clear_output()\\n",
    "            \\n",
    "            technique_name = change['new']\\n",
    "            info = techniques[technique_name]\\n",
    "            \\n",
    "            print(f\\\"üöÄ {technique_name}\\\")\\n",
    "            print(\\\"=\\\" * (len(technique_name) + 4))\\n",
    "            print(f\\\"\\\\nüìù Description: {info['description']}\\\")\\n",
    "            \\n",
    "            print(\\\"\\\\nüîß Techniques:\\\")\\n",
    "            for technique in info['techniques']:\\n",
    "                print(f\\\"  ‚Ä¢ {technique}\\\")\\n",
    "            \\n",
    "            print(\\\"\\\\n‚úÖ Pros:\\\")\\n",
    "            for pro in info['pros']:\\n",
    "                print(f\\\"  ‚Ä¢ {pro}\\\")\\n",
    "            \\n",
    "            print(\\\"\\\\n‚ö†Ô∏è Cons:\\\")\\n",
    "            for con in info['cons']:\\n",
    "                print(f\\\"  ‚Ä¢ {con}\\\")\\n",
    "            \\n",
    "            # Add specific code examples based on technique\\n",
    "            if technique_name == 'Model Quantization':\\n",
    "                print(\\\"\\\\nüíª Code Example:\\\")\\n",
    "                print(\\\"```python\\\")\\n",
    "                print(\\\"from transformers import BitsAndBytesConfig\\\")\\n",
    "                print(\\\"\\\\nquantization_config = BitsAndBytesConfig(\\\")\\n",
    "                print(\\\"    load_in_4bit=True,\\\")\\n",
    "                print(\\\"    bnb_4bit_compute_dtype=torch.float16,\\\")\\n",
    "                print(\\\"    bnb_4bit_use_double_quant=True,\\\")\\n",
    "                print(\\\"    bnb_4bit_quant_type='nf4'\\\")\\n",
    "                print(\\\")\\\")\\n",
    "                print(\\\"\\\\nmodel = AutoModelForCausalLM.from_pretrained(\\\")\\n",
    "                print(\\\"    model_name,\\\")\\n",
    "                print(\\\"    quantization_config=quantization_config,\\\")\\n",
    "                print(\\\"    device_map='auto'\\\")\\n",
    "                print(\\\")\\\")\\n",
    "                print(\\\"```\\\")\\n",
    "    \\n",
    "    # Set up event handler\\n",
    "    technique_dropdown.observe(display_technique_info, names='value')\\n",
    "    \\n",
    "    # Display initial technique\\n",
    "    display_technique_info({'new': technique_dropdown.value})\\n",
    "    \\n",
    "    return widgets.VBox([\\n",
    "        widgets.HTML(\\\"<h3>üéØ Advanced Optimization Techniques Guide</h3>\\\"),\\n",
    "        technique_dropdown,\\n",
    "        output_area\\n",
    "    ])\\n",
    "\\n",
    "display(create_advanced_optimization_guide())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Summary and Best Practices\\n",
    "\\n",
    "Let's summarize the key findings and provide actionable best practices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_performance_summary():\\n",
    "    \\\"\\\"\\\"Display a comprehensive summary of performance optimization insights\\\"\\\"\\\"\\n",
    "    \\n",
    "    print(\\\"üéØ PERFORMANCE OPTIMIZATION SUMMARY\\\")\\n",
    "    print(\\\"=\\\" * 50)\\n",
    "    \\n",
    "    print(\\\"\\\\nüìä Key Findings:\\\")\\n",
    "    print(\\\"\\\\n1. Memory Optimization:\\\")\\n",
    "    print(\\\"   ‚Ä¢ 4-bit quantization can reduce memory usage by 75%\\\")\\n",
    "    print(\\\"   ‚Ä¢ 8-bit quantization provides good balance of quality and efficiency\\\")\\n",
    "    print(\\\"   ‚Ä¢ FP16 precision offers 2x memory savings with minimal quality loss\\\")\\n",
    "    \\n",
    "    print(\\\"\\\\n2. Performance Trade-offs:\\\")\\n",
    "    print(\\\"   ‚Ä¢ Quantization: Memory ‚Üì‚Üì, Speed ‚Üë, Quality ‚Üì\\\")\\n",
    "    print(\\\"   ‚Ä¢ GPU acceleration: Speed ‚Üë‚Üë, Memory usage varies\\\")\\n",
    "    print(\\\"   ‚Ä¢ CPU optimization: Memory ‚Üì, Speed ‚Üì, Compatibility ‚Üë\\\")\\n",
    "    \\n",
    "    print(\\\"\\\\n3. Hardware Considerations:\\\")\\n",
    "    print(\\\"   ‚Ä¢ <8GB RAM: Use 4-bit quantization, small models only\\\")\\n",
    "    print(\\\"   ‚Ä¢ 8-16GB RAM: 8-bit quantization, medium models possible\\\")\\n",
    "    print(\\\"   ‚Ä¢ >16GB RAM: Full precision for small models, quantization for large\\\")\\n",
    "    \\n",
    "    print(\\\"\\\\nüöÄ Best Practices:\\\")\\n",
    "    \\n",
    "    practices = [\\n",
    "        \\\"Start with the smallest model that meets your quality requirements\\\",\\n",
    "        \\\"Use quantization for memory-constrained environments\\\",\\n",
    "        \\\"Monitor memory usage and adjust optimization strategies accordingly\\\",\\n",
    "        \\\"Test different optimization combinations for your specific use case\\\",\\n",
    "        \\\"Consider CPU inference for deployment scenarios with limited GPU access\\\",\\n",
    "        \\\"Use device_map='auto' for optimal multi-GPU utilization\\\",\\n",
    "        \\\"Implement proper error handling and fallback strategies\\\",\\n",
    "        \\\"Profile your application to identify bottlenecks\\\",\\n",
    "        \\\"Keep models and libraries updated for latest optimizations\\\",\\n",
    "        \\\"Document your optimization choices for reproducibility\\\"\\n",
    "    ]\\n",
    "    \\n",
    "    for i, practice in enumerate(practices, 1):\\n",
    "        print(f\\\"   {i:2d}. {practice}\\\")\\n",
    "    \\n",
    "    print(\\\"\\\\nüîß Optimization Decision Tree:\\\")\\n",
    "    print(\\\"\\\\n   Available RAM < 8GB?\\\")\\n",
    "    print(\\\"   ‚îú‚îÄ Yes ‚Üí Use 4-bit quantization + small models\\\")\\n",
    "    print(\\\"   ‚îî‚îÄ No ‚Üí Available RAM < 16GB?\\\")\\n",
    "    print(\\\"       ‚îú‚îÄ Yes ‚Üí Use 8-bit quantization + medium models\\\")\\n",
    "    print(\\\"       ‚îî‚îÄ No ‚Üí GPU available?\\\")\\n",
    "    print(\\\"           ‚îú‚îÄ Yes ‚Üí Use FP16 + GPU acceleration\\\")\\n",
    "    print(\\\"           ‚îî‚îÄ No ‚Üí Use CPU optimization strategies\\\")\\n",
    "    \\n",
    "    print(\\\"\\\\nüìö Additional Resources:\\\")\\n",
    "    resources = [\\n",
    "        \\\"Hugging Face Transformers Documentation: https://huggingface.co/docs/transformers\\\",\\n",
    "        \\\"BitsAndBytes Quantization: https://github.com/TimDettmers/bitsandbytes\\\",\\n",
    "        \\\"Ollama Documentation: https://ollama.ai/docs\\\",\\n",
    "        \\\"PyTorch Performance Tuning: https://pytorch.org/tutorials/recipes/recipes/tuning_guide.html\\\"\\n",
    "    ]\\n",
    "    \\n",
    "    for resource in resources:\\n",
    "        print(f\\\"   ‚Ä¢ {resource}\\\")\\n",
    "    \\n",
    "    print(\\\"\\\\n\" + \\\"=\\\" * 50)\\n",
    "    print(\\\"üéâ Congratulations! You've completed the Performance Optimization guide.\\\")\\n",
    "    print(\\\"Now you're equipped to optimize local LLMs for your specific needs!\\\")\\n",
    "\\n",
    "display_performance_summary()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}