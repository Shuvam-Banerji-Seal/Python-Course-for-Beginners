{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ollama Introduction: Getting Started with Local LLMs\n",
    "\n",
    "Welcome to the world of local Large Language Models! In this notebook, you'll learn how to use Ollama to run powerful AI models directly on your computer.\n",
    "\n",
    "## What You'll Learn\n",
    "\n",
    "- How to install and set up Ollama\n",
    "- Basic model management (downloading, listing, removing models)\n",
    "- Making your first API calls\n",
    "- Interactive examples with different models\n",
    "- Parameter adjustment and experimentation\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- Python 3.8 or higher\n",
    "- At least 8GB of RAM (16GB recommended)\n",
    "- Internet connection for initial model downloads\n",
    "\n",
    "Let's get started!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Installation and Setup\n",
    "\n",
    "First, let's check if Ollama is installed and install the required Python packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "def install_package(package):\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package])\n",
    "\n",
    "# Install required packages\n",
    "packages = ['requests', 'ipywidgets', 'matplotlib', 'pandas']\n",
    "\n",
    "for package in packages:\n",
    "    try:\n",
    "        __import__(package)\n",
    "        print(f\"‚úì {package} is already installed\")\n",
    "    except ImportError:\n",
    "        print(f\"Installing {package}...\")\n",
    "        install_package(package)\n",
    "        print(f\"‚úì {package} installed successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import requests\n",
    "import json\n",
    "import time\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, HTML, clear_output\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Ollama Connection and Health Check\n",
    "\n",
    "Let's create a simple class to interact with Ollama and check if it's running."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OllamaClient:\n",
    "    def __init__(self, base_url=\"http://localhost:11434\"):\n",
    "        self.base_url = base_url\n",
    "        self.session = requests.Session()\n",
    "    \n",
    "    def is_running(self):\n",
    "        \"\"\"Check if Ollama server is running\"\"\"\n",
    "        try:\n",
    "            response = self.session.get(f\"{self.base_url}/api/tags\", timeout=5)\n",
    "            return response.status_code == 200\n",
    "        except requests.exceptions.RequestException:\n",
    "            return False\n",
    "    \n",
    "    def list_models(self):\n",
    "        \"\"\"List all available models\"\"\"\n",
    "        try:\n",
    "            response = self.session.get(f\"{self.base_url}/api/tags\")\n",
    "            if response.status_code == 200:\n",
    "                return response.json().get('models', [])\n",
    "            return []\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"Error listing models: {e}\")\n",
    "            return []\n",
    "    \n",
    "    def pull_model(self, model_name, callback=None):\n",
    "        \"\"\"Download a model from Ollama registry\"\"\"\n",
    "        try:\n",
    "            response = self.session.post(\n",
    "                f\"{self.base_url}/api/pull\",\n",
    "                json={\"name\": model_name},\n",
    "                stream=True\n",
    "            )\n",
    "            \n",
    "            for line in response.iter_lines():\n",
    "                if line:\n",
    "                    data = json.loads(line.decode('utf-8'))\n",
    "                    if callback:\n",
    "                        callback(data)\n",
    "                    if data.get('status') == 'success':\n",
    "                        return True\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            print(f\"Error pulling model: {e}\")\n",
    "            return False\n",
    "    \n",
    "    def generate(self, model, prompt, system=None, stream=False):\n",
    "        \"\"\"Generate text using specified model\"\"\"\n",
    "        payload = {\n",
    "            \"model\": model,\n",
    "            \"prompt\": prompt,\n",
    "            \"stream\": stream\n",
    "        }\n",
    "        \n",
    "        if system:\n",
    "            payload[\"system\"] = system\n",
    "        \n",
    "        try:\n",
    "            response = self.session.post(\n",
    "                f\"{self.base_url}/api/generate\",\n",
    "                json=payload\n",
    "            )\n",
    "            \n",
    "            if response.status_code == 200:\n",
    "                return response.json()\n",
    "            else:\n",
    "                return {\"error\": f\"HTTP {response.status_code}: {response.text}\"}\n",
    "        except Exception as e:\n",
    "            return {\"error\": str(e)}\n",
    "\n",
    "# Initialize Ollama client\n",
    "ollama = OllamaClient()\n",
    "\n",
    "# Check if Ollama is running\n",
    "if ollama.is_running():\n",
    "    print(\"‚úì Ollama is running and accessible!\")\n",
    "    models = ollama.list_models()\n",
    "    print(f\"Found {len(models)} models installed\")\n",
    "else:\n",
    "    print(\"‚ùå Ollama is not running or not accessible\")\n",
    "    print(\"Please make sure Ollama is installed and running.\")\n",
    "    print(\"Visit: https://ollama.ai for installation instructions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Model Management\n",
    "\n",
    "Let's explore the models available and learn how to download new ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_models():\n",
    "    \"\"\"Display available models in a nice format\"\"\"\n",
    "    models = ollama.list_models()\n",
    "    \n",
    "    if not models:\n",
    "        print(\"No models found. Let's download a small model to get started!\")\n",
    "        return []\n",
    "    \n",
    "    print(\"Available Models:\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    model_data = []\n",
    "    for model in models:\n",
    "        name = model.get('name', 'Unknown')\n",
    "        size = model.get('size', 0)\n",
    "        size_gb = size / (1024**3) if size > 0 else 0\n",
    "        modified = model.get('modified_at', '')\n",
    "        \n",
    "        model_data.append({\n",
    "            'Name': name,\n",
    "            'Size (GB)': f\"{size_gb:.2f}\",\n",
    "            'Modified': modified[:10] if modified else 'Unknown'\n",
    "        })\n",
    "        \n",
    "        print(f\"üì¶ {name}\")\n",
    "        print(f\"   Size: {size_gb:.2f} GB\")\n",
    "        print(f\"   Modified: {modified[:10] if modified else 'Unknown'}\")\n",
    "        print()\n",
    "    \n",
    "    return model_data\n",
    "\n",
    "model_list = display_models()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interactive Model Download\n",
    "\n",
    "If you don't have any models, let's download a small one to get started. We'll use `llama2:7b-chat` as it's a good balance of capability and size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interactive model downloader\n",
    "def download_model_with_progress(model_name):\n",
    "    \"\"\"Download model with progress display\"\"\"\n",
    "    progress_widget = widgets.IntProgress(\n",
    "        value=0,\n",
    "        min=0,\n",
    "        max=100,\n",
    "        description='Downloading:',\n",
    "        bar_style='info',\n",
    "        style={'bar_color': 'blue'},\n",
    "        orientation='horizontal'\n",
    "    )\n",
    "    \n",
    "    status_label = widgets.Label(value=\"Starting download...\")\n",
    "    display(progress_widget, status_label)\n",
    "    \n",
    "    def update_progress(data):\n",
    "        status = data.get('status', '')\n",
    "        if 'completed' in data and 'total' in data:\n",
    "            completed = data['completed']\n",
    "            total = data['total']\n",
    "            if total > 0:\n",
    "                progress = int((completed / total) * 100)\n",
    "                progress_widget.value = progress\n",
    "                status_label.value = f\"{status}: {progress}% ({completed}/{total} bytes)\"\n",
    "        else:\n",
    "            status_label.value = status\n",
    "    \n",
    "    success = ollama.pull_model(model_name, callback=update_progress)\n",
    "    \n",
    "    if success:\n",
    "        progress_widget.value = 100\n",
    "        progress_widget.bar_style = 'success'\n",
    "        status_label.value = f\"‚úì Successfully downloaded {model_name}!\"\n",
    "    else:\n",
    "        progress_widget.bar_style = 'danger'\n",
    "        status_label.value = f\"‚ùå Failed to download {model_name}\"\n",
    "    \n",
    "    return success\n",
    "\n",
    "# Create download interface\n",
    "model_dropdown = widgets.Dropdown(\n",
    "    options=['llama2:7b-chat', 'codellama:7b', 'mistral:7b', 'phi:2.7b'],\n",
    "    value='llama2:7b-chat',\n",
    "    description='Model:',\n",
    ")\n",
    "\n",
    "download_button = widgets.Button(\n",
    "    description='Download Model',\n",
    "    button_style='primary',\n",
    "    icon='download'\n",
    ")\n",
    "\n",
    "def on_download_click(b):\n",
    "    model_name = model_dropdown.value\n",
    "    print(f\"Downloading {model_name}...\")\n",
    "    download_model_with_progress(model_name)\n",
    "\n",
    "download_button.on_click(on_download_click)\n",
    "\n",
    "print(\"Select a model to download:\")\n",
    "display(widgets.HBox([model_dropdown, download_button]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Your First Ollama Conversation\n",
    "\n",
    "Now let's have our first conversation with a local LLM!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_chat(model_name, prompt):\n",
    "    \"\"\"Simple chat function with timing\"\"\"\n",
    "    start_time = time.time()\n",
    "    \n",
    "    print(f\"ü§ñ Using model: {model_name}\")\n",
    "    print(f\"üë§ You: {prompt}\")\n",
    "    print(\"ü§ñ Assistant: \", end=\"\")\n",
    "    \n",
    "    response = ollama.generate(model_name, prompt)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    \n",
    "    if 'error' in response:\n",
    "        print(f\"‚ùå Error: {response['error']}\")\n",
    "        return None\n",
    "    \n",
    "    assistant_response = response.get('response', 'No response received')\n",
    "    print(assistant_response)\n",
    "    \n",
    "    # Display timing information\n",
    "    duration = end_time - start_time\n",
    "    print(f\"\\n‚è±Ô∏è Response time: {duration:.2f} seconds\")\n",
    "    \n",
    "    return {\n",
    "        'prompt': prompt,\n",
    "        'response': assistant_response,\n",
    "        'duration': duration,\n",
    "        'model': model_name\n",
    "    }\n",
    "\n",
    "# Test with a simple prompt\n",
    "models = ollama.list_models()\n",
    "if models:\n",
    "    test_model = models[0]['name']\n",
    "    result = simple_chat(test_model, \"Hello! Can you introduce yourself?\")\n",
    "else:\n",
    "    print(\"Please download a model first using the interface above.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Interactive Chat Interface\n",
    "\n",
    "Let's create an interactive chat interface where you can experiment with different prompts and parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interactive chat interface\n",
    "class InteractiveChat:\n",
    "    def __init__(self):\n",
    "        self.conversation_history = []\n",
    "        self.setup_widgets()\n",
    "    \n",
    "    def setup_widgets(self):\n",
    "        # Get available models\n",
    "        models = ollama.list_models()\n",
    "        model_names = [model['name'] for model in models] if models else ['No models available']\n",
    "        \n",
    "        # Create widgets\n",
    "        self.model_selector = widgets.Dropdown(\n",
    "            options=model_names,\n",
    "            description='Model:',\n",
    "            style={'description_width': 'initial'}\n",
    "        )\n",
    "        \n",
    "        self.system_prompt = widgets.Textarea(\n",
    "            value=\"You are a helpful assistant.\",\n",
    "            placeholder=\"Enter system prompt (optional)\",\n",
    "            description='System:',\n",
    "            layout=widgets.Layout(width='100%', height='60px'),\n",
    "            style={'description_width': 'initial'}\n",
    "        )\n",
    "        \n",
    "        self.user_input = widgets.Textarea(\n",
    "            placeholder=\"Type your message here...\",\n",
    "            description='Message:',\n",
    "            layout=widgets.Layout(width='100%', height='80px'),\n",
    "            style={'description_width': 'initial'}\n",
    "        )\n",
    "        \n",
    "        self.send_button = widgets.Button(\n",
    "            description='Send',\n",
    "            button_style='primary',\n",
    "            icon='paper-plane'\n",
    "        )\n",
    "        \n",
    "        self.clear_button = widgets.Button(\n",
    "            description='Clear History',\n",
    "            button_style='warning',\n",
    "            icon='trash'\n",
    "        )\n",
    "        \n",
    "        self.output_area = widgets.Output()\n",
    "        \n",
    "        # Set up event handlers\n",
    "        self.send_button.on_click(self.send_message)\n",
    "        self.clear_button.on_click(self.clear_history)\n",
    "        \n",
    "    def send_message(self, b):\n",
    "        if not self.user_input.value.strip():\n",
    "            return\n",
    "        \n",
    "        with self.output_area:\n",
    "            model = self.model_selector.value\n",
    "            system = self.system_prompt.value if self.system_prompt.value.strip() else None\n",
    "            prompt = self.user_input.value\n",
    "            \n",
    "            print(f\"\\n{'='*60}\")\n",
    "            print(f\"üë§ You: {prompt}\")\n",
    "            print(f\"ü§ñ {model}: \", end=\"\")\n",
    "            \n",
    "            start_time = time.time()\n",
    "            response = ollama.generate(model, prompt, system=system)\n",
    "            end_time = time.time()\n",
    "            \n",
    "            if 'error' in response:\n",
    "                print(f\"‚ùå Error: {response['error']}\")\n",
    "            else:\n",
    "                assistant_response = response.get('response', 'No response')\n",
    "                print(assistant_response)\n",
    "                \n",
    "                # Store in history\n",
    "                self.conversation_history.append({\n",
    "                    'timestamp': datetime.now(),\n",
    "                    'model': model,\n",
    "                    'system': system,\n",
    "                    'prompt': prompt,\n",
    "                    'response': assistant_response,\n",
    "                    'duration': end_time - start_time\n",
    "                })\n",
    "            \n",
    "            print(f\"\\n‚è±Ô∏è Response time: {end_time - start_time:.2f}s\")\n",
    "        \n",
    "        # Clear input\n",
    "        self.user_input.value = \"\"\n",
    "    \n",
    "    def clear_history(self, b):\n",
    "        self.conversation_history = []\n",
    "        self.output_area.clear_output()\n",
    "        with self.output_area:\n",
    "            print(\"Conversation history cleared.\")\n",
    "    \n",
    "    def display(self):\n",
    "        return widgets.VBox([\n",
    "            widgets.HTML(\"<h3>ü§ñ Interactive Ollama Chat</h3>\"),\n",
    "            self.model_selector,\n",
    "            self.system_prompt,\n",
    "            self.user_input,\n",
    "            widgets.HBox([self.send_button, self.clear_button]),\n",
    "            self.output_area\n",
    "        ])\n",
    "\n",
    "# Create and display the chat interface\n",
    "chat = InteractiveChat()\n",
    "display(chat.display())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Parameter Experimentation\n",
    "\n",
    "Let's explore how different parameters affect the model's responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameter experimentation interface\n",
    "def create_parameter_experiment():\n",
    "    \"\"\"Create an interface to experiment with different parameters\"\"\"\n",
    "    \n",
    "    # Get available models\n",
    "    models = ollama.list_models()\n",
    "    model_names = [model['name'] for model in models] if models else ['No models available']\n",
    "    \n",
    "    # Create widgets for parameters\n",
    "    model_widget = widgets.Dropdown(\n",
    "        options=model_names,\n",
    "        description='Model:'\n",
    "    )\n",
    "    \n",
    "    prompt_widget = widgets.Textarea(\n",
    "        value=\"Write a short story about a robot learning to paint.\",\n",
    "        description='Prompt:',\n",
    "        layout=widgets.Layout(width='100%', height='80px')\n",
    "    )\n",
    "    \n",
    "    system_prompts = {\n",
    "        'Default': \"You are a helpful assistant.\",\n",
    "        'Creative Writer': \"You are a creative writer who loves crafting imaginative stories with vivid descriptions.\",\n",
    "        'Technical Expert': \"You are a technical expert who provides precise, detailed explanations.\",\n",
    "        'Casual Friend': \"You are a casual, friendly person who speaks in a relaxed, conversational tone.\"\n",
    "    }\n",
    "    \n",
    "    system_widget = widgets.Dropdown(\n",
    "        options=list(system_prompts.keys()),\n",
    "        description='Style:'\n",
    "    )\n",
    "    \n",
    "    run_button = widgets.Button(\n",
    "        description='Generate Responses',\n",
    "        button_style='success',\n",
    "        icon='play'\n",
    "    )\n",
    "    \n",
    "    output_area = widgets.Output()\n",
    "    \n",
    "    def run_experiment(b):\n",
    "        with output_area:\n",
    "            clear_output()\n",
    "            \n",
    "            model = model_widget.value\n",
    "            prompt = prompt_widget.value\n",
    "            system_key = system_widget.value\n",
    "            system = system_prompts[system_key]\n",
    "            \n",
    "            print(f\"üß™ Experiment: {system_key} Style\")\n",
    "            print(f\"üìù Prompt: {prompt}\")\n",
    "            print(f\"ü§ñ Model: {model}\")\n",
    "            print(\"\\n\" + \"=\"*60 + \"\\n\")\n",
    "            \n",
    "            start_time = time.time()\n",
    "            response = ollama.generate(model, prompt, system=system)\n",
    "            end_time = time.time()\n",
    "            \n",
    "            if 'error' in response:\n",
    "                print(f\"‚ùå Error: {response['error']}\")\n",
    "            else:\n",
    "                print(response.get('response', 'No response'))\n",
    "                print(f\"\\n‚è±Ô∏è Generated in {end_time - start_time:.2f} seconds\")\n",
    "    \n",
    "    run_button.on_click(run_experiment)\n",
    "    \n",
    "    return widgets.VBox([\n",
    "        widgets.HTML(\"<h3>üß™ Parameter Experimentation</h3>\"),\n",
    "        widgets.HTML(\"<p>Try different system prompts to see how they affect the model's responses:</p>\"),\n",
    "        model_widget,\n",
    "        prompt_widget,\n",
    "        system_widget,\n",
    "        run_button,\n",
    "        output_area\n",
    "    ])\n",
    "\n",
    "display(create_parameter_experiment())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Performance Analysis\n",
    "\n",
    "Let's analyze the performance of different models and track response times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance analysis\n",
    "def analyze_performance():\n",
    "    \"\"\"Analyze performance across different models and prompts\"\"\"\n",
    "    \n",
    "    models = ollama.list_models()\n",
    "    if not models:\n",
    "        print(\"No models available for performance analysis.\")\n",
    "        return\n",
    "    \n",
    "    test_prompts = [\n",
    "        \"Hello, how are you?\",\n",
    "        \"Explain quantum computing in simple terms.\",\n",
    "        \"Write a haiku about programming.\"\n",
    "    ]\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    print(\"üîç Running performance analysis...\\n\")\n",
    "    \n",
    "    for model in models[:2]:  # Test first 2 models to save time\n",
    "        model_name = model['name']\n",
    "        print(f\"Testing {model_name}...\")\n",
    "        \n",
    "        for i, prompt in enumerate(test_prompts, 1):\n",
    "            print(f\"  Prompt {i}/3: \", end=\"\")\n",
    "            \n",
    "            start_time = time.time()\n",
    "            response = ollama.generate(model_name, prompt)\n",
    "            end_time = time.time()\n",
    "            \n",
    "            if 'error' not in response:\n",
    "                duration = end_time - start_time\n",
    "                response_text = response.get('response', '')\n",
    "                word_count = len(response_text.split())\n",
    "                \n",
    "                results.append({\n",
    "                    'Model': model_name,\n",
    "                    'Prompt': f\"Prompt {i}\",\n",
    "                    'Duration (s)': duration,\n",
    "                    'Words': word_count,\n",
    "                    'Words/sec': word_count / duration if duration > 0 else 0\n",
    "                })\n",
    "                \n",
    "                print(f\"{duration:.2f}s ({word_count} words)\")\n",
    "            else:\n",
    "                print(\"Error\")\n",
    "        \n",
    "        print()\n",
    "    \n",
    "    # Create DataFrame and visualize results\n",
    "    if results:\n",
    "        df = pd.DataFrame(results)\n",
    "        \n",
    "        # Display summary table\n",
    "        print(\"üìä Performance Summary:\")\n",
    "        print(df.to_string(index=False))\n",
    "        \n",
    "        # Create visualizations\n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "        \n",
    "        # Response time comparison\n",
    "        df.groupby('Model')['Duration (s)'].mean().plot(kind='bar', ax=ax1, color='skyblue')\n",
    "        ax1.set_title('Average Response Time by Model')\n",
    "        ax1.set_ylabel('Seconds')\n",
    "        ax1.tick_params(axis='x', rotation=45)\n",
    "        \n",
    "        # Words per second comparison\n",
    "        df.groupby('Model')['Words/sec'].mean().plot(kind='bar', ax=ax2, color='lightgreen')\n",
    "        ax2.set_title('Average Words per Second by Model')\n",
    "        ax2.set_ylabel('Words/sec')\n",
    "        ax2.tick_params(axis='x', rotation=45)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        return df\n",
    "    else:\n",
    "        print(\"No successful responses to analyze.\")\n",
    "        return None\n",
    "\n",
    "# Run performance analysis\n",
    "performance_data = analyze_performance()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Conversation History Analysis\n",
    "\n",
    "Let's analyze the conversation history from our interactive chat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze conversation history\n",
    "def analyze_conversation_history(chat_instance):\n",
    "    \"\"\"Analyze the conversation history from the interactive chat\"\"\"\n",
    "    \n",
    "    if not chat_instance.conversation_history:\n",
    "        print(\"No conversation history to analyze. Try using the interactive chat above first!\")\n",
    "        return\n",
    "    \n",
    "    history = chat_instance.conversation_history\n",
    "    \n",
    "    print(f\"üìà Conversation Analysis ({len(history)} interactions)\\n\")\n",
    "    \n",
    "    # Basic statistics\n",
    "    total_duration = sum(conv['duration'] for conv in history)\n",
    "    avg_duration = total_duration / len(history)\n",
    "    \n",
    "    print(f\"‚è±Ô∏è Total conversation time: {total_duration:.2f} seconds\")\n",
    "    print(f\"‚è±Ô∏è Average response time: {avg_duration:.2f} seconds\")\n",
    "    \n",
    "    # Model usage\n",
    "    models_used = {}\n",
    "    for conv in history:\n",
    "        model = conv['model']\n",
    "        models_used[model] = models_used.get(model, 0) + 1\n",
    "    \n",
    "    print(f\"\\nü§ñ Models used:\")\n",
    "    for model, count in models_used.items():\n",
    "        print(f\"  {model}: {count} times\")\n",
    "    \n",
    "    # Response length analysis\n",
    "    response_lengths = [len(conv['response'].split()) for conv in history]\n",
    "    avg_length = sum(response_lengths) / len(response_lengths)\n",
    "    \n",
    "    print(f\"\\nüìù Response analysis:\")\n",
    "    print(f\"  Average response length: {avg_length:.1f} words\")\n",
    "    print(f\"  Shortest response: {min(response_lengths)} words\")\n",
    "    print(f\"  Longest response: {max(response_lengths)} words\")\n",
    "    \n",
    "    # Create timeline visualization\n",
    "    if len(history) > 1:\n",
    "        timestamps = [conv['timestamp'] for conv in history]\n",
    "        durations = [conv['duration'] for conv in history]\n",
    "        \n",
    "        plt.figure(figsize=(10, 4))\n",
    "        plt.plot(range(1, len(durations) + 1), durations, 'o-', color='blue', alpha=0.7)\n",
    "        plt.title('Response Time Over Conversation')\n",
    "        plt.xlabel('Interaction Number')\n",
    "        plt.ylabel('Response Time (seconds)')\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.show()\n",
    "\n",
    "# Analyze the chat history (if available)\n",
    "try:\n",
    "    analyze_conversation_history(chat)\n",
    "except NameError:\n",
    "    print(\"Chat instance not available. Use the interactive chat above to generate some conversation history first!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Next Steps and Resources\n",
    "\n",
    "Congratulations! You've learned the basics of using Ollama for local LLM deployment. Here's what you can explore next:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
"metadata": {},
   "outputs": [],
   "source": [
    "# Next steps and additional resources\n",
    "print(\"üéâ Congratulations on completing the Ollama introduction!\")\n",
    "print(\"\\nüìö Next notebooks to explore:\")\n",
    "print(\"  ‚Ä¢ 02_transformers_basics.ipynb - Alternative approaches\")\n",
    "print(\"  ‚Ä¢ 03_model_formats_explained.ipynb - Model formats\")\n",
    "print(\"  ‚Ä¢ 04_prompt_engineering.ipynb - Advanced prompting\")\n",
    "print(\"  ‚Ä¢ 05_performance_optimization.ipynb - Optimization\")\n",
    "print(\"\\nüîó External resources:\")\n",
    "print(\"  ‚Ä¢ Ollama Documentation: https://ollama.ai/docs\")\n",
    "print(\"  ‚Ä¢ Model Library: https://ollama.ai/library\")\n",
    "print(\"  ‚Ä¢ Community: https://github.com/ollama/ollama\")\n",
    "print(\"\\nüöÄ Happy experimenting with local LLMs!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}