# Hugging Face Transformers

This section explores alternative approaches to local LLM deployment using Hugging Face Transformers.

## Contents

- `transformers_setup.md` - Installation and configuration guide
- `local_inference.py` - Basic model loading and inference examples
- `memory_optimization.py` - Techniques for efficient memory usage
- `model_comparison.py` - Performance comparisons between approaches

## Quick Start

1. Set up the Transformers environment
2. Try basic inference examples
3. Learn memory optimization techniques
4. Compare performance with Ollama