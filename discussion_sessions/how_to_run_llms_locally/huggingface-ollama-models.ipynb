{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Downloading and Running Hugging Face Models with Ollama\n",
        "\n",
        "This notebook demonstrates how to download models from Hugging Face and run them with Ollama, including popular models like Gemma, Llama, and others.\n",
        "\n",
        "## Table of Contents\n",
        "1. [Prerequisites and Setup](#prerequisites)\n",
        "2. [Direct Ollama Model Downloads](#direct-downloads)\n",
        "3. [Hugging Face Model Integration](#hf-integration)\n",
        "4. [Model Conversion Process](#conversion)\n",
        "5. [Running and Testing Models](#testing)\n",
        "6. [Model Management](#management)\n",
        "7. [Performance Comparison](#comparison)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Prerequisites and Setup {#prerequisites}\n",
        "\n",
        "First, let's install the required packages and set up our environment:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install required packages\n",
        "!pip install ollama huggingface_hub transformers torch\n",
        "!pip install requests tqdm rich"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import ollama\n",
        "import requests\n",
        "import json\n",
        "import subprocess\n",
        "import time\n",
        "from pathlib import Path\n",
        "from typing import List, Dict, Optional\n",
        "from rich.console import Console\n",
        "from rich.table import Table\n",
        "from rich.panel import Panel\n",
        "from rich.progress import Progress, SpinnerColumn, TextColumn, BarColumn, TaskProgressColumn\n",
        "from rich.markdown import Markdown\n",
        "from huggingface_hub import HfApi, list_models\n",
        "\n",
        "console = Console()\n",
        "client = ollama.Client()\n",
        "\n",
        "console.print(\"‚úÖ All packages imported successfully!\", style=\"green\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Verify Ollama Installation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def check_ollama_status():\n",
        "    \"\"\"Check if Ollama is running and accessible\"\"\"\n",
        "    try:\n",
        "        response = requests.get('http://localhost:11434/api/tags', timeout=5)\n",
        "        if response.status_code == 200:\n",
        "            models = response.json()\n",
        "            console.print(\"‚úÖ Ollama is running!\", style=\"green\")\n",
        "            console.print(f\"üì¶ Currently installed models: {len(models.get('models', []))}\", style=\"blue\")\n",
        "            return True\n",
        "        else:\n",
        "            console.print(f\"‚ùå Ollama responded with status: {response.status_code}\", style=\"red\")\n",
        "            return False\n",
        "    except requests.exceptions.ConnectionError:\n",
        "        console.print(\"‚ùå Cannot connect to Ollama. Make sure it's running:\", style=\"red\")\n",
        "        console.print(\"   Run: ollama serve\", style=\"yellow\")\n",
        "        return False\n",
        "    except Exception as e:\n",
        "        console.print(f\"‚ùå Error checking Ollama: {e}\", style=\"red\")\n",
        "        return False\n",
        "\n",
        "ollama_running = check_ollama_status()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Direct Ollama Model Downloads {#direct-downloads}\n",
        "\n",
        "Ollama provides direct access to many popular models. Let's explore what's available and download some models:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class OllamaModelManager:\n",
        "    \"\"\"Manager for Ollama model operations\"\"\"\n",
        "    \n",
        "    def __init__(self):\n",
        "        self.client = ollama.Client()\n",
        "        self.console = Console()\n",
        "    \n",
        "    def list_available_models(self) -> List[str]:\n",
        "        \"\"\"List currently installed models\"\"\"\n",
        "        try:\n",
        "            models = self.client.list()\n",
        "            return [model['name'] for model in models['models']]\n",
        "        except Exception as e:\n",
        "            self.console.print(f\"‚ùå Error listing models: {e}\", style=\"red\")\n",
        "            return []\n",
        "    \n",
        "    def pull_model(self, model_name: str) -> bool:\n",
        "        \"\"\"Download a model from Ollama library\"\"\"\n",
        "        try:\n",
        "            self.console.print(f\"üì• Downloading {model_name}...\", style=\"cyan\")\n",
        "            \n",
        "            # Use subprocess to show real-time progress\n",
        "            process = subprocess.Popen(\n",
        "                ['ollama', 'pull', model_name],\n",
        "                stdout=subprocess.PIPE,\n",
        "                stderr=subprocess.STDOUT,\n",
        "                universal_newlines=True,\n",
        "                bufsize=1\n",
        "            )\n",
        "            \n",
        "            # Stream output in real-time\n",
        "            for line in process.stdout:\n",
        "                print(line.strip())\n",
        "            \n",
        "            process.wait()\n",
        "            \n",
        "            if process.returncode == 0:\n",
        "                self.console.print(f\"‚úÖ Successfully downloaded {model_name}!\", style=\"green\")\n",
        "                return True\n",
        "            else:\n",
        "                self.console.print(f\"‚ùå Failed to download {model_name}\", style=\"red\")\n",
        "                return False\n",
        "                \n",
        "        except Exception as e:\n",
        "            self.console.print(f\"‚ùå Error downloading {model_name}: {e}\", style=\"red\")\n",
        "            return False\n",
        "    \n",
        "    def get_model_info(self, model_name: str) -> Optional[Dict]:\n",
        "        \"\"\"Get detailed information about a model\"\"\"\n",
        "        try:\n",
        "            info = self.client.show(model_name)\n",
        "            return info\n",
        "        except Exception as e:\n",
        "            self.console.print(f\"‚ùå Error getting info for {model_name}: {e}\", style=\"red\")\n",
        "            return None\n",
        "    \n",
        "    def remove_model(self, model_name: str) -> bool:\n",
        "        \"\"\"Remove a model\"\"\"\n",
        "        try:\n",
        "            subprocess.run(['ollama', 'rm', model_name], check=True)\n",
        "            self.console.print(f\"üóëÔ∏è Removed {model_name}\", style=\"yellow\")\n",
        "            return True\n",
        "        except subprocess.CalledProcessError:\n",
        "            self.console.print(f\"‚ùå Failed to remove {model_name}\", style=\"red\")\n",
        "            return False\n",
        "\n",
        "# Initialize model manager\n",
        "model_manager = OllamaModelManager()\n",
        "\n",
        "# Show currently installed models\n",
        "current_models = model_manager.list_available_models()\n",
        "console.print(f\"üì¶ Currently installed models: {len(current_models)}\", style=\"blue\")\n",
        "for model in current_models:\n",
        "    console.print(f\"  ‚Ä¢ {model}\", style=\"dim\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Popular Models Available in Ollama\n",
        "\n",
        "Here are some popular models you can download directly:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Popular models available in Ollama\n",
        "popular_models = {\n",
        "    \"Gemma Models\": [\n",
        "        \"gemma2:2b\",\n",
        "        \"gemma2:9b\", \n",
        "        \"gemma2:27b\",\n",
        "        \"gemma:2b\",\n",
        "        \"gemma:7b\"\n",
        "    ],\n",
        "    \"Llama Models\": [\n",
        "        \"llama3.2:1b\",\n",
        "        \"llama3.2:3b\",\n",
        "        \"llama3.1:8b\",\n",
        "        \"llama3.1:70b\",\n",
        "        \"llama2:7b\",\n",
        "        \"llama2:13b\"\n",
        "    ],\n",
        "    \"Code Models\": [\n",
        "        \"codellama:7b\",\n",
        "        \"codellama:13b\",\n",
        "        \"codegemma:2b\",\n",
        "        \"codegemma:7b\"\n",
        "    ],\n",
        "    \"Other Popular Models\": [\n",
        "        \"mistral:7b\",\n",
        "        \"mixtral:8x7b\",\n",
        "        \"phi3:3.8b\",\n",
        "        \"qwen2:7b\",\n",
        "        \"deepseek-coder:6.7b\"\n",
        "    ]\n",
        "}\n",
        "\n",
        "# Display available models in a nice table\n",
        "table = Table(title=\"ü§ñ Popular Models Available in Ollama\")\n",
        "table.add_column(\"Category\", style=\"cyan\", no_wrap=True)\n",
        "table.add_column(\"Models\", style=\"green\")\n",
        "table.add_column(\"Size Info\", style=\"yellow\")\n",
        "\n",
        "size_info = {\n",
        "    \"1b-3b\": \"~1-2GB (Fast, good for basic tasks)\",\n",
        "    \"7b-9b\": \"~4-5GB (Balanced performance)\",\n",
        "    \"13b-27b\": \"~7-15GB (High quality responses)\",\n",
        "    \"70b+\": \"~40GB+ (Best quality, requires powerful hardware)\"\n",
        "}\n",
        "\n",
        "for category, models in popular_models.items():\n",
        "    models_text = \"\\n\".join([f\"‚Ä¢ {model}\" for model in models])\n",
        "    \n",
        "    # Determine size category\n",
        "    if any(\"1b\" in model or \"2b\" in model or \"3b\" in model for model in models):\n",
        "        size_cat = \"1b-3b\"\n",
        "    elif any(\"7b\" in model or \"9b\" in model for model in models):\n",
        "        size_cat = \"7b-9b\"\n",
        "    elif any(\"13b\" in model or \"27b\" in model for model in models):\n",
        "        size_cat = \"13b-27b\"\n",
        "    else:\n",
        "        size_cat = \"7b-9b\"  # default\n",
        "    \n",
        "    table.add_row(category, models_text, size_info[size_cat])\n",
        "\n",
        "console.print(table)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Download Gemma Models\n",
        "\n",
        "Let's download some Gemma models as examples:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Models to download (starting with smaller ones)\n",
        "models_to_download = [\n",
        "    \"gemma2:2b\",  # Small, fast model\n",
        "    # \"gemma2:9b\",  # Uncomment if you have enough RAM/VRAM\n",
        "]\n",
        "\n",
        "if ollama_running:\n",
        "    console.print(\"üöÄ Starting model downloads...\", style=\"bold cyan\")\n",
        "    \n",
        "    for model in models_to_download:\n",
        "        console.print(f\"\\nüì• Downloading {model}...\", style=\"cyan\")\n",
        "        \n",
        "        # Check if model already exists\n",
        "        if model in current_models:\n",
        "            console.print(\"‚úÖ {model} already installed!\".format(model=model), style=\"green\")\n",
        "            continue\n",
        "        \n",
        "        # Download the model\n",
        "        success = model_manager.pull_model(model)\n",
        "        \n",
        "        if success:\n",
        "            console.print(f\"‚úÖ {model} downloaded successfully!\", style=\"green\")\n",
        "        else:\n",
        "            console.print(f\"‚ùå Failed to download {model}\", style=\"red\")\n",
        "        \n",
        "        time.sleep(1)  # Brief pause between downloads\n",
        "    \n",
        "    # Update model list\n",
        "    current_models = model_manager.list_available_models()\n",
        "    console.print(f\"\\nüì¶ Total models now: {len(current_models)}\", style=\"blue\")\n",
        "else:\n",
        "    console.print(\"‚ö†Ô∏è Ollama not running. Please start Ollama first.\", style=\"yellow\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Hugging Face Model Integration {#hf-integration}\n",
        "\n",
        "Now let's explore how to work with Hugging Face models and potentially convert them for Ollama:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class HuggingFaceModelExplorer:\n",
        "    \"\"\"Explorer for Hugging Face models\"\"\"\n",
        "    \n",
        "    def __init__(self):\n",
        "        self.api = HfApi()\n",
        "        self.console = Console()\n",
        "    \n",
        "    def search_models(self, query: str, limit: int = 10) -> List[Dict]:\n",
        "        \"\"\"Search for models on Hugging Face\"\"\"\n",
        "        try:\n",
        "            models = list_models(\n",
        "                search=query,\n",
        "                limit=limit,\n",
        "                sort=\"downloads\",\n",
        "                direction=-1\n",
        "            )\n",
        "            \n",
        "            model_info = []\n",
        "            for model in models:\n",
        "                info = {\n",
        "                    'id': model.id,\n",
        "                    'downloads': getattr(model, 'downloads', 0),\n",
        "                    'likes': getattr(model, 'likes', 0),\n",
        "                    'tags': getattr(model, 'tags', []),\n",
        "                    'pipeline_tag': getattr(model, 'pipeline_tag', 'unknown')\n",
        "                }\n",
        "                model_info.append(info)\n",
        "            \n",
        "            return model_info\n",
        "            \n",
        "        except Exception as e:\n",
        "            self.console.print(f\"‚ùå Error searching models: {e}\", style=\"red\")\n",
        "            return []\n",
        "    \n",
        "    def display_models(self, models: List[Dict], title: str = \"Models\"):\n",
        "        \"\"\"Display models in a nice table\"\"\"\n",
        "        if not models:\n",
        "            self.console.print(\"No models found.\", style=\"yellow\")\n",
        "            return\n",
        "        \n",
        "        table = Table(title=f\"ü§ó {title}\")\n",
        "        table.add_column(\"Model ID\", style=\"cyan\")\n",
        "        table.add_column(\"Downloads\", style=\"green\")\n",
        "        table.add_column(\"Likes\", style=\"yellow\")\n",
        "        table.add_column(\"Type\", style=\"magenta\")\n",
        "        \n",
        "        for model in models:\n",
        "            downloads = f\"{model['downloads']:,}\" if model['downloads'] else \"N/A\"\n",
        "            likes = str(model['likes']) if model['likes'] else \"0\"\n",
        "            pipeline = model['pipeline_tag'] or \"text-generation\"\n",
        "            \n",
        "            table.add_row(\n",
        "                model['id'],\n",
        "                downloads,\n",
        "                likes,\n",
        "                pipeline\n",
        "            )\n",
        "        \n",
        "        self.console.print(table)\n",
        "\n",
        "# Initialize HF explorer\n",
        "hf_explorer = HuggingFaceModelExplorer()\n",
        "\n",
        "# Search for Gemma models on Hugging Face\n",
        "console.print(\"üîç Searching for Gemma models on Hugging Face...\", style=\"cyan\")\n",
        "gemma_models = hf_explorer.search_models(\"gemma\", limit=8)\n",
        "hf_explorer.display_models(gemma_models, \"Gemma Models on Hugging Face\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Search for other popular models\n",
        "console.print(\"\\nüîç Searching for Llama models...\", style=\"cyan\")\n",
        "llama_models = hf_explorer.search_models(\"llama\", limit=6)\n",
        "hf_explorer.display_models(llama_models, \"Llama Models on Hugging Face\")\n",
        "\n",
        "console.print(\"\\nüîç Searching for Mistral models...\", style=\"cyan\")\n",
        "mistral_models = hf_explorer.search_models(\"mistral\", limit=6)\n",
        "hf_explorer.display_models(mistral_models, \"Mistral Models on Hugging Face\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Model Conversion Process {#conversion}\n",
        "\n",
        "While Ollama provides many models directly, sometimes you might want to convert a Hugging Face model. Here's how to do it:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class ModelConverter:\n",
        "    \"\"\"Helper for converting Hugging Face models to Ollama format\"\"\"\n",
        "    \n",
        "    def __init__(self):\n",
        "        self.console = Console()\n",
        "    \n",
        "    def create_modelfile(self, model_path: str, model_name: str, \n",
        "                        system_prompt: str = None, \n",
        "                        temperature: float = 0.7) -> str:\n",
        "        \"\"\"Create a Modelfile for Ollama\"\"\"\n",
        "        \n",
        "        modelfile_content = f\"FROM {model_path}\\n\"\n",
        "        \n",
        "        if system_prompt:\n",
        "            modelfile_content += f'SYSTEM \"{system_prompt}\"\\n'\n",
        "        \n",
        "        modelfile_content += f\"PARAMETER temperature {temperature}\\n\"\n",
        "        modelfile_content += \"PARAMETER num_ctx 4096\\n\"\n",
        "        \n",
        "        # Save to file\n",
        "        modelfile_path = f\"Modelfile.{model_name}\"\n",
        "        with open(modelfile_path, 'w') as f:\n",
        "            f.write(modelfile_content)\n",
        "        \n",
        "        self.console.print(f\"üìÑ Created Modelfile: {modelfile_path}\", style=\"green\")\n",
        "        self.console.print(f\"Content:\\n{modelfile_content}\", style=\"dim\")\n",
        "        \n",
        "        return modelfile_path\n",
        "    \n",
        "    def convert_hf_model(self, hf_model_id: str, ollama_model_name: str, \n",
        "                        system_prompt: str = None) -> bool:\n",
        "        \"\"\"Convert a Hugging Face model to Ollama format\"\"\"\n",
        "        \n",
        "        try:\n",
        "            self.console.print(f\"üîÑ Converting {hf_model_id} to Ollama format...\", style=\"cyan\")\n",
        "            \n",
        "            # Create Modelfile\n",
        "            modelfile_path = self.create_modelfile(\n",
        "                hf_model_id, \n",
        "                ollama_model_name,\n",
        "                system_prompt\n",
        "            )\n",
        "            \n",
        "            # Create the model in Ollama\n",
        "            self.console.print(f\"üèóÔ∏è Creating Ollama model: {ollama_model_name}\", style=\"cyan\")\n",
        "            \n",
        "            process = subprocess.Popen(\n",
        "                ['ollama', 'create', ollama_model_name, '-f', modelfile_path],\n",
        "                stdout=subprocess.PIPE,\n",
        "                stderr=subprocess.STDOUT,\n",
        "                universal_newlines=True\n",
        "            )\n",
        "            \n",
        "            # Stream output\n",
        "            for line in process.stdout:\n",
        "                print(line.strip())\n",
        "            \n",
        "            process.wait()\n",
        "            \n",
        "            if process.returncode == 0:\n",
        "                self.console.print(f\"‚úÖ Successfully created {ollama_model_name}!\", style=\"green\")\n",
        "                return True\n",
        "            else:\n",
        "                self.console.print(f\"‚ùå Failed to create {ollama_model_name}\", style=\"red\")\n",
        "                return False\n",
        "                \n",
        "        except Exception as e:\n",
        "            self.console.print(f\"‚ùå Conversion error: {e}\", style=\"red\")\n",
        "            return False\n",
        "    \n",
        "    def download_and_convert(self, hf_model_id: str, ollama_model_name: str):\n",
        "        \"\"\"Download from HF and convert to Ollama (example workflow)\"\"\"\n",
        "        \n",
        "        self.console.print(f\"üìã Conversion workflow for {hf_model_id}:\", style=\"bold cyan\")\n",
        "        \n",
        "        steps = [\n",
        "            \"1. Download model from Hugging Face\",\n",
        "            \"2. Convert to GGUF format (if needed)\",\n",
        "            \"3. Create Ollama Modelfile\",\n",
        "            \"4. Import into Ollama\",\n",
        "            \"5. Test the model\"\n",
        "        ]\n",
        "        \n",
        "        for step in steps:\n",
        "            self.console.print(f\"  {step}\", style=\"dim\")\n",
        "        \n",
        "        self.console.print(\"\\n‚ö†Ô∏è Note: This is a simplified example.\", style=\"yellow\")\n",
        "        self.console.print(\"For complex conversions, you might need additional tools like:\", style=\"yellow\")\n",
        "        self.console.print(\"  ‚Ä¢ llama.cpp for GGUF conversion\", style=\"dim\")\n",
        "        self.console.print(\"  ‚Ä¢ Specific model conversion scripts\", style=\"dim\")\n",
        "\n",
        "# Initialize converter\n",
        "converter = ModelConverter()\n",
        "\n",
        "# Example: Show conversion workflow\n",
        "converter.download_and_convert(\"google/gemma-2b\", \"my-gemma-2b\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Creating Custom Modelfiles\n",
        "\n",
        "Let's create some example Modelfiles for different use cases:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Example Modelfiles for different purposes\n",
        "modelfile_examples = {\n",
        "    \"coding_assistant\": {\n",
        "        \"system_prompt\": \"You are an expert programming assistant. Provide clear, well-commented code examples and explain complex concepts simply.\",\n",
        "        \"temperature\": 0.3,\n",
        "        \"description\": \"Focused on coding tasks with lower temperature for consistency\"\n",
        "    },\n",
        "    \"creative_writer\": {\n",
        "        \"system_prompt\": \"You are a creative writing assistant. Write engaging stories with vivid descriptions and compelling characters.\",\n",
        "        \"temperature\": 0.9,\n",
        "        \"description\": \"Creative writing with higher temperature for variety\"\n",
        "    },\n",
        "    \"research_assistant\": {\n",
        "        \"system_prompt\": \"You are a research assistant. Provide accurate, well-sourced information and cite your reasoning clearly.\",\n",
        "        \"temperature\": 0.5,\n",
        "        \"description\": \"Balanced temperature for factual accuracy\"\n",
        "    }\n",
        "}\n",
        "\n",
        "console.print(\"üìù Example Modelfile Configurations:\", style=\"bold cyan\")\n",
        "\n",
        "for name, config in modelfile_examples.items():\n",
        "    console.print(f\"\\nüéØ {name.replace('_', ' ').title()}:\", style=\"green\")\n",
        "    console.print(f\"   Description: {config['description']}\", style=\"dim\")\n",
        "    console.print(f\"   Temperature: {config['temperature']}\", style=\"blue\")\n",
        "    console.print(f\"   System Prompt: {config['system_prompt'][:80]}...\", style=\"yellow\")\n",
        "    \n",
        "    # Create example Modelfile content\n",
        "    modelfile_content = f\"\"\"FROM gemma2:2b\n",
        "SYSTEM \"{config['system_prompt']}\"\n",
        "PARAMETER temperature {config['temperature']}\n",
        "PARAMETER num_ctx 4096\n",
        "PARAMETER top_p 0.9\"\"\"\n",
        "    \n",
        "    console.print(Panel(\n",
        "        modelfile_content,\n",
        "        title=f\"Modelfile.{name}\",\n",
        "        border_style=\"dim\"\n",
        "    ))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Running and Testing Models {#testing}\n",
        "\n",
        "Now let's test the models we've downloaded:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class ModelTester:\n",
        "    \"\"\"Test and compare different models\"\"\"\n",
        "    \n",
        "    def __init__(self):\n",
        "        self.client = ollama.Client()\n",
        "        self.console = Console()\n",
        "    \n",
        "    def test_model(self, model_name: str, prompt: str, max_tokens: int = 150) -> Dict:\n",
        "        \"\"\"Test a single model with a prompt\"\"\"\n",
        "        try:\n",
        "            start_time = time.time()\n",
        "            \n",
        "            response = self.client.chat(\n",
        "                model=model_name,\n",
        "                messages=[{'role': 'user', 'content': prompt}],\n",
        "                options={'num_predict': max_tokens}\n",
        "            )\n",
        "            \n",
        "            end_time = time.time()\n",
        "            \n",
        "            return {\n",
        "                'model': model_name,\n",
        "                'prompt': prompt,\n",
        "                'response': response['message']['content'],\n",
        "                'time_taken': end_time - start_time,\n",
        "                'success': True\n",
        "            }\n",
        "            \n",
        "        except Exception as e:\n",
        "            return {\n",
        "                'model': model_name,\n",
        "                'prompt': prompt,\n",
        "                'error': str(e),\n",
        "                'success': False\n",
        "            }\n",
        "    \n",
        "    def compare_models(self, models: List[str], prompt: str) -> List[Dict]:\n",
        "        \"\"\"Compare multiple models with the same prompt\"\"\"\n",
        "        results = []\n",
        "        \n",
        "        self.console.print(f\"üî¨ Testing prompt: '{prompt}'\", style=\"cyan\")\n",
        "        self.console.print(f\"üìä Comparing {len(models)} models...\\n\", style=\"blue\")\n",
        "        \n",
        "        for model in models:\n",
        "            self.console.print(f\"Testing {model}...\", style=\"dim\")\n",
        "            result = self.test_model(model, prompt)\n",
        "            results.append(result)\n",
        "            \n",
        "            if result['success']:\n",
        "                self.console.print(f\"‚úÖ {model}: {result['time_taken']:.2f}s\", style=\"green\")\n",
        "            else:\n",
        "                self.console.print(f\"‚ùå {model}: {result['error']}\", style=\"red\")\n",
        "        \n",
        "        return results\n",
        "    \n",
        "    def display_comparison(self, results: List[Dict]):\n",
        "        \"\"\"Display comparison results in a nice format\"\"\"\n",
        "        successful_results = [r for r in results if r['success']]\n",
        "        \n",
        "        if not successful_results:\n",
        "            self.console.print(\"‚ùå No successful results to display\", style=\"red\")\n",
        "            return\n",
        "        \n",
        "        self.console.print(\"\\nüìä Model Comparison Results:\", style=\"bold cyan\")\n",
        "        \n",
        "        for i, result in enumerate(successful_results, 1):\n",
        "            self.console.print(f\"\\n{i}. ü§ñ {result['model']}:\", style=\"bold green\")\n",
        "            self.console.print(f\"   ‚è±Ô∏è Time: {result['time_taken']:.2f} seconds\", style=\"blue\")\n",
        "            self.console.print(f\"   üìù Response length: {len(result['response'])} characters\", style=\"yellow\")\n",
        "            \n",
        "            # Show response in a panel\n",
        "            self.console.print(Panel(\n",
        "                result['response'][:300] + (\"...\" if len(result['response']) > 300 else \"\"),\n",
        "                title=f\"Response from {result['model']}\",\n",
        "                border_style=\"green\"\n",
        "            ))\n",
        "\n",
        "# Initialize tester\n",
        "tester = ModelTester()\n",
        "\n",
        "# Get available models for testing\n",
        "available_models = model_manager.list_available_models()\n",
        "console.print(f\"üì¶ Available models for testing: {len(available_models)}\", style=\"blue\")\n",
        "\n",
        "if available_models:\n",
        "    for model in available_models:\n",
        "        console.print(f\"  ‚Ä¢ {model}\", style=\"dim\")\n",
        "else:\n",
        "    console.print(\"‚ö†Ô∏è No models available. Please download some models first.\", style=\"yellow\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test models with different types of prompts\n",
        "test_prompts = [\n",
        "    \"Explain quantum computing in simple terms.\",\n",
        "    \"Write a Python function to calculate fibonacci numbers.\",\n",
        "    \"What are the benefits of renewable energy?\"\n",
        "]\n",
        "\n",
        "if available_models and ollama_running:\n",
        "    # Test with first available model\n",
        "    test_model = available_models[0]\n",
        "    \n",
        "    console.print(f\"\\nüß™ Testing {test_model} with different prompts:\", style=\"bold cyan\")\n",
        "    \n",
        "    for i, prompt in enumerate(test_prompts, 1):\n",
        "        console.print(f\"\\n{i}. Testing: '{prompt}'\", style=\"cyan\")\n",
        "        \n",
        "        result = tester.test_model(test_model, prompt, max_tokens=100)\n",
        "        \n",
        "        if result['success']:\n",
        "            console.print(f\"‚úÖ Response time: {result['time_taken']:.2f}s\", style=\"green\")\n",
        "            console.print(Panel(\n",
        "                result['response'],\n",
        "                title=f\"Response from {test_model}\",\n",
        "                border_style=\"green\"\n",
        "            ))\n",
        "        else:\n",
        "            console.print(f\"‚ùå Error: {result['error']}\", style=\"red\")\n",
        "        \n",
        "        time.sleep(1)  # Brief pause between tests\n",
        "    \n",
        "    # If multiple models available, compare them\n",
        "    if len(available_models) > 1:\n",
        "        console.print(\"\\nüî¨ Comparing multiple models...\", style=\"bold magenta\")\n",
        "        comparison_results = tester.compare_models(\n",
        "            available_models[:2],  # Compare first two models\n",
        "            \"What is artificial intelligence?\"\n",
        "        )\n",
        "        tester.display_comparison(comparison_results)\n",
        "        \n",
        "else:\n",
        "    console.print(\"‚ö†Ô∏è Cannot run tests - no models available or Ollama not running\", style=\"yellow\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Model Management {#management}\n",
        "\n",
        "Let's create tools for managing our models effectively:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class AdvancedModelManager:\n",
        "    \"\"\"Advanced model management with detailed information\"\"\"\n",
        "    \n",
        "    def __init__(self):\n",
        "        self.client = ollama.Client()\n",
        "        self.console = Console()\n",
        "    \n",
        "    def get_detailed_model_info(self) -> List[Dict]:\n",
        "        \"\"\"Get detailed information about all models\"\"\"\n",
        "        try:\n",
        "            models = self.client.list()\n",
        "            detailed_info = []\n",
        "            \n",
        "            for model in models['models']:\n",
        "                name = model['name']\n",
        "                size_gb = model.get('size', 0) / (1024**3)\n",
        "                modified = model.get('modified_at', 'Unknown')\n",
        "                \n",
        "                # Try to get additional info\n",
        "                try:\n",
        "                    info = self.client.show(name)\n",
        "                    parameters = info.get('details', {}).get('parameter_size', 'Unknown')\n",
        "                    family = info.get('details', {}).get('family', 'Unknown')\n",
        "                except:\n",
        "                    parameters = 'Unknown'\n",
        "                    family = 'Unknown'\n",
        "                \n",
        "                detailed_info.append({\n",
        "                    'name': name,\n",
        "                    'size_gb': size_gb,\n",
        "                    'modified': modified,\n",
        "                    'parameters': parameters,\n",
        "                    'family': family\n",
        "                })\n",
        "            \n",
        "            return detailed_info\n",
        "            \n",
        "        except Exception as e:\n",
        "            self.console.print(f\"‚ùå Error getting model info: {e}\", style=\"red\")\n",
        "            return []\n",
        "    \n",
        "    def display_model_table(self, models: List[Dict]):\n",
        "        \"\"\"Display models in a detailed table\"\"\"\n",
        "        if not models:\n",
        "            self.console.print(\"No models found.\", style=\"yellow\")\n",
        "            return\n",
        "        \n",
        "        table = Table(title=\"ü§ñ Installed Models - Detailed View\")\n",
        "        table.add_column(\"Model Name\", style=\"cyan\", no_wrap=True)\n",
        "        table.add_column(\"Size (GB)\", style=\"green\")\n",
        "        table.add_column(\"Parameters\", style=\"yellow\")\n",
        "        table.add_column(\"Family\", style=\"magenta\")\n",
        "        table.add_column(\"Modified\", style=\"blue\")\n",
        "        \n",
        "        total_size = 0\n",
        "        for model in models:\n",
        "            size_str = f\"{model['size_gb']:.1f}\"\n",
        "            total_size += model['size_gb']\n",
        "            \n",
        "            # Format modified date\n",
        "            modified = model['modified']\n",
        "            if modified != 'Unknown':\n",
        "                try:\n",
        "                    from datetime import datetime\n",
        "                    dt = datetime.fromisoformat(modified.replace('Z', '+00:00'))\n",
        "                    modified = dt.strftime('%Y-%m-%d')\n",
        "                except:\n",
        "                    pass\n",
        "            \n",
        "            table.add_row(\n",
        "                model['name'],\n",
        "                size_str,\n",
        "                str(model['parameters']),\n",
        "                model['family'],\n",
        "                modified\n",
        "            )\n",
        "        \n",
        "        self.console.print(table)\n",
        "        self.console.print(f\"\\nüíæ Total storage used: {total_size:.1f} GB\", style=\"bold blue\")\n",
        "    \n",
        "    def model_usage_stats(self, models: List[Dict]):\n",
        "        \"\"\"Show usage statistics\"\"\"\n",
        "        if not models:\n",
        "            return\n",
        "        \n",
        "        # Group by family\n",
        "        families = {}\n",
        "        for model in models:\n",
        "            family = model['family']\n",
        "            if family not in families:\n",
        "                families[family] = []\n",
        "            families[family].append(model)\n",
        "        \n",
        "        console.print(\"\\nüìä Model Statistics:\", style=\"bold cyan\")\n",
        "        \n",
        "        for family, family_models in families.items():\n",
        "            count = len(family_models)\n",
        "            total_size = sum(m['size_gb'] for m in family_models)\n",
        "            console.print(f\"  üè∑Ô∏è {family}: {count} models, {total_size:.1f} GB\", style=\"green\")\n",
        "    \n",
        "    def cleanup_suggestions(self, models: List[Dict]):\n",
        "        \"\"\"Suggest models that could be removed to save space\"\"\"\n",
        "        if len(models) <= 2:\n",
        "            console.print(\"\\nüßπ No cleanup suggestions - you have few models.\", style=\"green\")\n",
        "            return\n",
        "        \n",
        "        # Sort by size (largest first)\n",
        "        large_models = sorted(models, key=lambda x: x['size_gb'], reverse=True)\n",
        "        \n",
        "        console.print(\"\\nüßπ Cleanup Suggestions:\", style=\"bold yellow\")\n",
        "        console.print(\"Consider removing large models you don't use frequently:\", style=\"dim\")\n",
        "        \n",
        "        for model in large_models[:3]:  # Show top 3 largest\n",
        "            if model['size_gb'] > 5:  # Only suggest large models\n",
        "                console.print(f\"  ‚Ä¢ {model['name']} ({model['size_gb']:.1f} GB)\", style=\"yellow\")\n",
        "                console.print(f\"    Command: ollama rm {model['name']}\", style=\"dim\")\n",
        "\n",
        "# Initialize advanced manager\n",
        "advanced_manager = AdvancedModelManager()\n",
        "\n",
        "if ollama_running:\n",
        "    # Get detailed model information\n",
        "    detailed_models = advanced_manager.get_detailed_model_info()\n",
        "    \n",
        "    if detailed_models:\n",
        "        advanced_manager.display_model_table(detailed_models)\n",
        "        advanced_manager.model_usage_stats(detailed_models)\n",
        "        advanced_manager.cleanup_suggestions(detailed_models)\n",
        "    else:\n",
        "        console.print(\"No models found or error retrieving model information.\", style=\"yellow\")\n",
        "else:\n",
        "    console.print(\"‚ö†Ô∏è Ollama not running - cannot retrieve model information\", style=\"yellow\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Performance Comparison {#comparison}\n",
        "\n",
        "Let's create a comprehensive performance comparison tool:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class PerformanceBenchmark:\n",
        "    \"\"\"Benchmark different models for performance comparison\"\"\"\n",
        "    \n",
        "    def __init__(self):\n",
        "        self.client = ollama.Client()\n",
        "        self.console = Console()\n",
        "    \n",
        "    def benchmark_model(self, model_name: str, test_prompts: List[str], \n",
        "                       num_runs: int = 3) -> Dict:\n",
        "        \"\"\"Benchmark a single model with multiple prompts\"\"\"\n",
        "        results = {\n",
        "            'model': model_name,\n",
        "            'tests': [],\n",
        "            'avg_time': 0,\n",
        "            'total_tokens': 0,\n",
        "            'success_rate': 0\n",
        "        }\n",
        "        \n",
        "        successful_tests = 0\n",
        "        total_time = 0\n",
        "        total_tokens = 0\n",
        "        \n",
        "        for prompt in test_prompts:\n",
        "            prompt_results = []\n",
        "            \n",
        "            for run in range(num_runs):\n",
        "                try:\n",
        "                    start_time = time.time()\n",
        "                    \n",
        "                    response = self.client.chat(\n",
        "                        model=model_name,\n",
        "                        messages=[{'role': 'user', 'content': prompt}],\n",
        "                        options={'num_predict': 100}\n",
        "                    )\n",
        "                    \n",
        "                    end_time = time.time()\n",
        "                    response_time = end_time - start_time\n",
        "                    response_text = response['message']['content']\n",
        "                    token_count = len(response_text.split())\n",
        "                    \n",
        "                    prompt_results.append({\n",
        "                        'time': response_time,\n",
        "                        'tokens': token_count,\n",
        "                        'success': True\n",
        "                    })\n",
        "                    \n",
        "                    successful_tests += 1\n",
        "                    total_time += response_time\n",
        "                    total_tokens += token_count\n",
        "                    \n",
        "                except Exception as e:\n",
        "                    prompt_results.append({\n",
        "                        'error': str(e),\n",
        "                        'success': False\n",
        "                    })\n",
        "            \n",
        "            # Calculate averages for this prompt\n",
        "            successful_runs = [r for r in prompt_results if r['success']]\n",
        "            if successful_runs:\n",
        "                avg_time = sum(r['time'] for r in successful_runs) / len(successful_runs)\n",
        "                avg_tokens = sum(r['tokens'] for r in successful_runs) / len(successful_runs)\n",
        "            else:\n",
        "                avg_time = 0\n",
        "                avg_tokens = 0\n",
        "            \n",
        "            results['tests'].append({\n",
        "                'prompt': prompt,\n",
        "                'runs': prompt_results,\n",
        "                'avg_time': avg_time,\n",
        "                'avg_tokens': avg_tokens\n",
        "            })\n",
        "        \n",
        "        # Calculate overall statistics\n",
        "        total_tests = len(test_prompts) * num_runs\n",
        "        results['success_rate'] = (successful_tests / total_tests) * 100 if total_tests > 0 else 0\n",
        "        results['avg_time'] = total_time / successful_tests if successful_tests > 0 else 0\n",
        "        results['avg_tokens_per_second'] = (total_tokens / total_time) if total_time > 0 else 0\n",
        "        \n",
        "        return results\n",
        "    \n",
        "    def display_benchmark_results(self, results: List[Dict]):\n",
        "        \"\"\"Display benchmark results in a comprehensive format\"\"\"\n",
        "        if not results:\n",
        "            self.console.print(\"No benchmark results to display.\", style=\"yellow\")\n",
        "            return\n",
        "        \n",
        "        # Summary table\n",
        "        table = Table(title=\"üèÜ Model Performance Benchmark\")\n",
        "        table.add_column(\"Model\", style=\"cyan\")\n",
        "        table.add_column(\"Avg Time (s)\", style=\"green\")\n",
        "        table.add_column(\"Tokens/sec\", style=\"yellow\")\n",
        "        table.add_column(\"Success Rate\", style=\"blue\")\n",
        "        table.add_column(\"Rating\", style=\"magenta\")\n",
        "        \n",
        "        for result in results:\n",
        "            # Calculate performance rating\n",
        "            speed_score = min(100, (result['avg_tokens_per_second'] / 10) * 100)\n",
        "            reliability_score = result['success_rate']\n",
        "            overall_rating = (speed_score + reliability_score) / 2\n",
        "            \n",
        "            if overall_rating >= 80:\n",
        "                rating = \"‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê\"\n",
        "            elif overall_rating >= 60:\n",
        "                rating = \"‚≠ê‚≠ê‚≠ê‚≠ê\"\n",
        "            elif overall_rating >= 40:\n",
        "                rating = \"‚≠ê‚≠ê‚≠ê\"\n",
        "            elif overall_rating >= 20:\n",
        "                rating = \"‚≠ê‚≠ê\"\n",
        "            else:\n",
        "                rating = \"‚≠ê\"\n",
        "            \n",
        "            table.add_row(\n",
        "                result['model'],\n",
        "                f\"{result['avg_time']:.2f}\",\n",
        "                f\"{result['avg_tokens_per_second']:.1f}\",\n",
        "                f\"{result['success_rate']:.1f}%\",\n",
        "                rating\n",
        "            )\n",
        "        \n",
        "        self.console.print(table)\n",
        "        \n",
        "        # Detailed breakdown\n",
        "        self.console.print(\"\\nüìä Detailed Performance Analysis:\", style=\"bold cyan\")\n",
        "        \n",
        "        for result in results:\n",
        "            self.console.print(f\"\\nü§ñ {result['model']}:\", style=\"bold green\")\n",
        "            \n",
        "            for test in result['tests']:\n",
        "                successful_runs = len([r for r in test['runs'] if r['success']])\n",
        "                total_runs = len(test['runs'])\n",
        "                \n",
        "                self.console.print(f\"  üìù '{test['prompt'][:50]}...'\", style=\"dim\")\n",
        "                self.console.print(f\"     Success: {successful_runs}/{total_runs}, \"\n",
        "                                 f\"Avg Time: {test['avg_time']:.2f}s, \"\n",
        "                                 f\"Avg Tokens: {test['avg_tokens']:.0f}\", style=\"blue\")\n",
        "\n",
        "# Benchmark test prompts\n",
        "benchmark_prompts = [\n",
        "    \"What is machine learning?\",\n",
        "    \"Write a Python function to sort a list.\",\n",
        "    \"Explain the theory of relativity briefly.\"\n",
        "]\n",
        "\n",
        "if available_models and ollama_running and len(available_models) > 0:\n",
        "    benchmark = PerformanceBenchmark()\n",
        "    \n",
        "    console.print(\"\\nüèÅ Starting Performance Benchmark...\", style=\"bold cyan\")\n",
        "    console.print(f\"Testing {len(available_models)} model(s) with {len(benchmark_prompts)} prompts\", style=\"blue\")\n",
        "    \n",
        "    benchmark_results = []\n",
        "    \n",
        "    for model in available_models[:2]:  # Limit to first 2 models to save time\n",
        "        console.print(f\"\\nüî¨ Benchmarking {model}...\", style=\"cyan\")\n",
        "        \n",
        "        with Progress(\n",
        "            SpinnerColumn(),\n",
        "            TextColumn(\"[progress.description]{task.description}\"),\n",
        "            console=console,\n",
        "        ) as progress:\n",
        "            task = progress.add_task(f\"Testing {model}...\", total=None)\n",
        "            \n",
        "            result = benchmark.benchmark_model(model, benchmark_prompts, num_runs=2)\n",
        "            benchmark_results.append(result)\n",
        "            \n",
        "            progress.update(task, completed=True)\n",
        "    \n",
        "    # Display results\n",
        "    benchmark.display_benchmark_results(benchmark_results)\n",
        "    \n",
        "else:\n",
        "    console.print(\"‚ö†Ô∏è Cannot run benchmark - no models available or Ollama not running\", style=\"yellow\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Conclusion and Next Steps\n",
        "\n",
        "This notebook has covered comprehensive model management with Ollama and Hugging Face integration:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Summary of what we've accomplished\n",
        "summary = \"\"\"\n",
        "# üéâ Hugging Face + Ollama Integration Complete!\n",
        "\n",
        "## What We've Covered:\n",
        "\n",
        "### üì• **Model Downloads**\n",
        "- Direct Ollama model downloads (Gemma, Llama, etc.)\n",
        "- Hugging Face model exploration\n",
        "- Model availability checking\n",
        "\n",
        "### üîÑ **Model Conversion**\n",
        "- Creating custom Modelfiles\n",
        "- Converting HF models to Ollama format\n",
        "- Custom system prompts and parameters\n",
        "\n",
        "### üß™ **Testing & Benchmarking**\n",
        "- Model performance testing\n",
        "- Comparative analysis\n",
        "- Speed and accuracy metrics\n",
        "\n",
        "### üõ†Ô∏è **Management Tools**\n",
        "- Advanced model information\n",
        "- Storage usage tracking\n",
        "- Cleanup suggestions\n",
        "\n",
        "## üöÄ Next Steps:\n",
        "\n",
        "1. **Download More Models**\n",
        "   ```bash\n",
        "   ollama pull gemma2:9b\n",
        "   ollama pull llama3.1:8b\n",
        "   ollama pull codellama:7b\n",
        "   ```\n",
        "\n",
        "2. **Create Custom Models**\n",
        "   - Experiment with different system prompts\n",
        "   - Fine-tune parameters for your use case\n",
        "   - Create specialized assistants\n",
        "\n",
        "3. **Build Applications**\n",
        "   - Integrate models into your projects\n",
        "   - Create web interfaces\n",
        "   - Build domain-specific chatbots\n",
        "\n",
        "4. **Optimize Performance**\n",
        "   - Test different quantization levels\n",
        "   - Monitor resource usage\n",
        "   - Implement caching strategies\n",
        "\n",
        "## üìö Useful Commands:\n",
        "\n",
        "```bash\n",
        "# List available models\n",
        "ollama list\n",
        "\n",
        "# Download a model\n",
        "ollama pull model_name\n",
        "\n",
        "# Remove a model\n",
        "ollama rm model_name\n",
        "\n",
        "# Create custom model\n",
        "ollama create my_model -f Modelfile\n",
        "\n",
        "# Show model info\n",
        "ollama show model_name\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "*Happy modeling with Ollama and Hugging Face! ü§óü¶ô*\n",
        "\"\"\"\n",
        "\n",
        "console.print(Panel(\n",
        "    Markdown(summary),\n",
        "    title=\"üìã Tutorial Summary\",\n",
        "    border_style=\"gold\"\n",
        "))\n",
        "\n",
        "# Final status check\n",
        "if ollama_running:\n",
        "    final_models = model_manager.list_available_models()\n",
        "    console.print(f\"\\n‚úÖ Tutorial complete! You now have {len(final_models)} model(s) ready to use.\", style=\"bold green\")\n",
        "    \n",
        "    if final_models:\n",
        "        console.print(\"üéØ Try chatting with your models:\", style=\"cyan\")\n",
        "        for model in final_models[:3]:  # Show first 3\n",
        "            console.print(f\"  ollama run {model}\", style=\"dim\")\n",
        "else:\n",
        "    console.print(\"\\n‚ö†Ô∏è Start Ollama to begin using your models: ollama serve\", style=\"yellow\")\n",
        "\n",
        "console.print(\"\\nüöÄ Ready to build amazing AI applications!\", style=\"bold magenta\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
