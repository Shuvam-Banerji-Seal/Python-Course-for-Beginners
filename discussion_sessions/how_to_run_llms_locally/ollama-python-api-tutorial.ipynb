{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Complete Guide to Ollama Python API\n",
    "\n",
    "This notebook provides a comprehensive, step-by-step guide to using the Ollama API with Python. We'll cover everything from installation to advanced usage patterns.\n",
    "\n",
    "## Table of Contents\n",
    "1. [Prerequisites and Installation](#prerequisites)\n",
    "2. [Basic Setup and Configuration](#setup)\n",
    "3. [Simple Chat Interactions](#basic-chat)\n",
    "4. [Working with Different Models](#models)\n",
    "5. [Streaming Responses](#streaming)\n",
    "6. [Advanced Features](#advanced)\n",
    "7. [Error Handling and Best Practices](#best-practices)\n",
    "8. [Real-world Examples](#examples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Prerequisites and Installation {#prerequisites}\n",
    "\n",
    "Before we start, make sure you have:\n",
    "- Python 3.7 or higher\n",
    "- Ollama installed and running on your system\n",
    "- At least one model downloaded in Ollama\n",
    "\n",
    "### Installing Required Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install the official Ollama Python library\n",
    "!pip install ollama\n",
    "\n",
    "# Optional: Install additional helpful libraries\n",
    "!pip install requests json-stream"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verify Ollama is Running\n",
    "\n",
    "Let's check if Ollama is running and accessible:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "try:\n",
    "    response = requests.get('http://localhost:11434/api/tags')\n",
    "    if response.status_code == 200:\n",
    "        models = response.json()\n",
    "        print(\"‚úÖ Ollama is running!\")\n",
    "        print(f\"Available models: {len(models.get('models', []))}\")\n",
    "        for model in models.get('models', []):\n",
    "            print(f\"  - {model['name']}\")\n",
    "    else:\n",
    "        print(f\"‚ùå Ollama responded with status code: {response.status_code}\")\n",
    "except requests.exceptions.ConnectionError:\n",
    "    print(\"‚ùå Cannot connect to Ollama. Make sure it's running on localhost:11434\")\n",
    "    print(\"Run 'ollama serve' in your terminal to start Ollama\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you see the message `‚úÖ Ollama is running!`, you're all set to start using the Ollama Python API."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Basic Setup and Configuration {#setup}\n",
    "\n",
    "Now let's import the Ollama library and set up our basic configuration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ollama\n",
    "from typing import Dict, List, Any\n",
    "import time\n",
    "\n",
    "# Create an Ollama client\n",
    "client = ollama.Client()\n",
    "\n",
    "# You can also specify a custom host if Ollama is running elsewhere\n",
    "# client = ollama.Client(host='http://localhost:11434')\n",
    "\n",
    "print(\"‚úÖ Ollama client initialized successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### List Available Models\n",
    "\n",
    "Let's see what models we have available:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_models():\n",
    "    \"\"\"List all available models in Ollama\"\"\"\n",
    "    try:\n",
    "        models = client.list()\n",
    "        print(f\"üìã Available Models ({len(models['models'])}):\\n\")\n",
    "        \n",
    "        for model in models['models']:\n",
    "            name = model['name']\n",
    "            size = model.get('size', 0) / (1024**3)  # Convert to GB\n",
    "            modified = model.get('modified_at', 'Unknown')\n",
    "            print(f\"  ü§ñ {name}\")\n",
    "            print(f\"     Size: {size:.1f} GB\")\n",
    "            print(f\"     Modified: {modified}\\n\")\n",
    "            \n",
    "        return [model['name'] for model in models['models']]\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error listing models: {e}\")\n",
    "        return []\n",
    "\n",
    "available_models = list_models()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Simple Chat Interactions {#basic-chat}\n",
    "\n",
    "Let's start with basic chat functionality. We'll create a simple function to chat with a model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_chat(model_name: str, message: str) -> str:\n",
    "    \"\"\"Send a simple message to a model and get a response\"\"\"\n",
    "    try:\n",
    "        response = client.chat(\n",
    "            model=model_name,\n",
    "            messages=[\n",
    "                {\n",
    "                    'role': 'user',\n",
    "                    'content': message,\n",
    "                },\n",
    "            ],\n",
    "        )\n",
    "        return response['message']['content']\n",
    "    except Exception as e:\n",
    "        return f\"Error: {e}\"\n",
    "\n",
    "# Example usage\n",
    "if available_models:\n",
    "    model = available_models[0]  # Use the first available model\n",
    "    print(f\"ü§ñ Using model: {model}\\n\")\n",
    "    \n",
    "    question = \"What is Python and why is it popular?\"\n",
    "    print(f\"üë§ User: {question}\\n\")\n",
    "    \n",
    "    answer = simple_chat(model, question)\n",
    "    print(f\"ü§ñ Assistant: {answer}\")\n",
    "else:\n",
    "    print(\"‚ùå No models available. Please download a model first using 'ollama pull <model_name>'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interactive Chat Function\n",
    "\n",
    "Let's create a more interactive chat function that maintains conversation history:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OllamaChat:\n",
    "    def __init__(self, model_name: str):\n",
    "        self.model_name = model_name\n",
    "        self.conversation_history = []\n",
    "        self.client = ollama.Client()\n",
    "    \n",
    "    def send_message(self, message: str) -> str:\n",
    "        \"\"\"Send a message and maintain conversation history\"\"\"\n",
    "        # Add user message to history\n",
    "        self.conversation_history.append({\n",
    "            'role': 'user',\n",
    "            'content': message\n",
    "        })\n",
    "        \n",
    "        try:\n",
    "            # Get response from model\n",
    "            response = self.client.chat(\n",
    "                model=self.model_name,\n",
    "                messages=self.conversation_history\n",
    "            )\n",
    "            \n",
    "            assistant_message = response['message']['content']\n",
    "            \n",
    "            # Add assistant response to history\n",
    "            self.conversation_history.append({\n",
    "                'role': 'assistant',\n",
    "                'content': assistant_message\n",
    "            })\n",
    "            \n",
    "            return assistant_message\n",
    "            \n",
    "        except Exception as e:\n",
    "            return f\"Error: {e}\"\n",
    "    \n",
    "    def clear_history(self):\n",
    "        \"\"\"Clear the conversation history\"\"\"\n",
    "        self.conversation_history = []\n",
    "        print(\"üóëÔ∏è Conversation history cleared\")\n",
    "    \n",
    "    def get_history(self):\n",
    "        \"\"\"Get the current conversation history\"\"\"\n",
    "        return self.conversation_history\n",
    "    \n",
    "    def print_history(self):\n",
    "        \"\"\"Print the conversation history in a readable format\"\"\"\n",
    "        print(\"üìú Conversation History:\\n\")\n",
    "        for i, message in enumerate(self.conversation_history, 1):\n",
    "            role_emoji = \"üë§\" if message['role'] == 'user' else \"ü§ñ\"\n",
    "            role_name = \"User\" if message['role'] == 'user' else \"Assistant\"\n",
    "            print(f\"{i}. {role_emoji} {role_name}: {message['content']}\\n\")\n",
    "\n",
    "# Example usage\n",
    "if available_models:\n",
    "    chat = OllamaChat(available_models[0])\n",
    "    print(f\"üí¨ Started chat with {available_models[0]}\\n\")\n",
    "    \n",
    "    # Have a conversation\n",
    "    response1 = chat.send_message(\"Hello! What's your name?\")\n",
    "    print(f\"ü§ñ: {response1}\\n\")\n",
    "    \n",
    "    response2 = chat.send_message(\"Can you remember what I just asked you?\")\n",
    "    print(f\"ü§ñ: {response2}\\n\")\n",
    "    \n",
    "    # Show conversation history\n",
    "    chat.print_history()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Working with Different Models {#models}\n",
    "\n",
    "Let's explore how to work with different models and compare their responses:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_models(models: List[str], question: str):\n",
    "    \"\"\"Compare responses from different models\"\"\"\n",
    "    print(f\"‚ùì Question: {question}\\n\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    for model in models:\n",
    "        print(f\"\\nü§ñ Model: {model}\")\n",
    "        print(\"-\" * 40)\n",
    "        \n",
    "        start_time = time.time()\n",
    "        response = simple_chat(model, question)\n",
    "        end_time = time.time()\n",
    "        \n",
    "        print(f\"Response: {response}\")\n",
    "        print(f\"‚è±Ô∏è Time taken: {end_time - start_time:.2f} seconds\")\n",
    "        print(\"=\" * 80)\n",
    "\n",
    "# Example: Compare different models (if you have multiple)\n",
    "if len(available_models) > 1:\n",
    "    compare_models(\n",
    "        available_models[:2],  # Compare first two models\n",
    "        \"Explain quantum computing in simple terms\"\n",
    "    )\n",
    "else:\n",
    "    print(\"‚ÑπÔ∏è Only one model available. Download more models to compare responses.\")\n",
    "    print(\"Example: ollama pull llama2 && ollama pull codellama\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Information and Management\n",
    "\n",
    "Let's create functions to get detailed information about models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_info(model_name: str):\n",
    "    \"\"\"Get detailed information about a specific model\"\"\"\n",
    "    try:\n",
    "        info = client.show(model_name)\n",
    "        \n",
    "        print(f\"üìä Model Information: {model_name}\\n\")\n",
    "        print(f\"License: {info.get('license', 'Unknown')}\")\n",
    "        print(f\"Template: {info.get('template', 'Unknown')}\")\n",
    "        print(f\"Parameters: {info.get('parameters', 'Unknown')}\")\n",
    "        \n",
    "        if 'modelfile' in info:\n",
    "            print(f\"\\nüìÑ Modelfile:\\n{info['modelfile']}\")\n",
    "            \n",
    "        return info\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error getting model info: {e}\")\n",
    "        return None\n",
    "\n",
    "# Get info for the first available model\n",
    "if available_models:\n",
    "    model_info = get_model_info(available_models[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Streaming Responses {#streaming}\n",
    "\n",
    "For longer responses, streaming can provide a better user experience. Let's implement streaming:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stream_chat(model_name: str, message: str):\n",
    "    \"\"\"Stream a chat response token by token\"\"\"\n",
    "    print(f\"ü§ñ {model_name} (streaming): \", end=\"\", flush=True)\n",
    "    \n",
    "    try:\n",
    "        stream = client.chat(\n",
    "            model=model_name,\n",
    "            messages=[{'role': 'user', 'content': message}],\n",
    "            stream=True,\n",
    "        )\n",
    "        \n",
    "        full_response = \"\"\n",
    "        for chunk in stream:\n",
    "            content = chunk['message']['content']\n",
    "            print(content, end=\"\", flush=True)\n",
    "            full_response += content\n",
    "        \n",
    "        print(\"\\n\")  # New line after streaming is complete\n",
    "        return full_response\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\n‚ùå Streaming error: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "# Example streaming\n",
    "if available_models:\n",
    "    print(\"üì° Streaming Example:\\n\")\n",
    "    stream_chat(\n",
    "        available_models[0], \n",
    "        \"Write a short story about a robot learning to paint\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Advanced Streaming with Progress Tracking\n",
    "\n",
    "Let's create a more sophisticated streaming function with progress tracking:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from datetime import datetime\n",
    "\n",
    "class StreamingChat:\n",
    "    def __init__(self, model_name: str):\n",
    "        self.model_name = model_name\n",
    "        self.client = ollama.Client()\n",
    "    \n",
    "    def stream_with_stats(self, message: str, show_stats: bool = True):\n",
    "        \"\"\"Stream response with statistics\"\"\"\n",
    "        start_time = time.time()\n",
    "        token_count = 0\n",
    "        full_response = \"\"\n",
    "        \n",
    "        if show_stats:\n",
    "            print(f\"üöÄ Starting stream at {datetime.now().strftime('%H:%M:%S')}\")\n",
    "            print(f\"üìù Prompt: {message}\\n\")\n",
    "            print(f\"ü§ñ {self.model_name}: \", end=\"\", flush=True)\n",
    "        \n",
    "        try:\n",
    "            stream = self.client.chat(\n",
    "                model=self.model_name,\n",
    "                messages=[{'role': 'user', 'content': message}],\n",
    "                stream=True,\n",
    "            )\n",
    "            \n",
    "            for chunk in stream:\n",
    "                content = chunk['message']['content']\n",
    "                print(content, end=\"\", flush=True)\n",
    "                full_response += content\n",
    "                token_count += len(content.split())\n",
    "            \n",
    "            end_time = time.time()\n",
    "            duration = end_time - start_time\n",
    "            \n",
    "            if show_stats:\n",
    "                print(f\"\\n\\nüìä Statistics:\")\n",
    "                print(f\"   ‚è±Ô∏è Duration: {duration:.2f} seconds\")\n",
    "                print(f\"   üî§ Approximate tokens: {token_count}\")\n",
    "                print(f\"   ‚ö° Tokens/second: {token_count/duration:.1f}\")\n",
    "                print(f\"   üìè Response length: {len(full_response)} characters\")\n",
    "            \n",
    "            return {\n",
    "                'response': full_response,\n",
    "                'duration': duration,\n",
    "                'token_count': token_count,\n",
    "                'tokens_per_second': token_count/duration if duration > 0 else 0\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"\\n‚ùå Error: {e}\")\n",
    "            return None\n",
    "\n",
    "# Example usage\n",
    "if available_models:\n",
    "    streaming_chat = StreamingChat(available_models[0])\n",
    "    result = streaming_chat.stream_with_stats(\n",
    "        \"Explain the concept of machine learning in detail\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Advanced Features {#advanced}\n",
    "\n",
    "Let's explore some advanced features of the Ollama API:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom System Prompts\n",
    "\n",
    "System prompts help define the behavior and personality of the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat_with_system_prompt(model_name: str, system_prompt: str, user_message: str):\n",
    "    \"\"\"Chat with a custom system prompt\"\"\"\n",
    "    try:\n",
    "        response = client.chat(\n",
    "            model=model_name,\n",
    "            messages=[\n",
    "                {\n",
    "                    'role': 'system',\n",
    "                    'content': system_prompt\n",
    "                },\n",
    "                {\n",
    "                    'role': 'user',\n",
    "                    'content': user_message\n",
    "                }\n",
    "            ]\n",
    "        )\n",
    "        return response['message']['content']\n",
    "    except Exception as e:\n",
    "        return f\"Error: {e}\"\n",
    "\n",
    "# Example: Create a coding assistant\n",
    "if available_models:\n",
    "    system_prompt = \"\"\"\n",
    "    You are a helpful Python programming assistant. You should:\n",
    "    - Provide clear, well-commented code examples\n",
    "    - Explain complex concepts in simple terms\n",
    "    - Always include error handling where appropriate\n",
    "    - Suggest best practices and optimizations\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"üîß System Prompt Example - Python Assistant:\\n\")\n",
    "    response = chat_with_system_prompt(\n",
    "        available_models[0],\n",
    "        system_prompt,\n",
    "        \"How do I read a CSV file and handle missing values?\"\n",
    "    )\n",
    "    print(f\"ü§ñ: {response}\\n\")\n",
    "    \n",
    "    # Example: Create a creative writing assistant\n",
    "    creative_prompt = \"\"\"\n",
    "    You are a creative writing assistant. You should:\n",
    "    - Write in an engaging, descriptive style\n",
    "    - Use vivid imagery and metaphors\n",
    "    - Create compelling characters and scenarios\n",
    "    - Always end with a cliffhanger or thought-provoking question\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"‚ú® System Prompt Example - Creative Writer:\\n\")\n",
    "    response = chat_with_system_prompt(\n",
    "        available_models[0],\n",
    "        creative_prompt,\n",
    "        \"Write a short paragraph about a mysterious door\"\n",
    "    )\n",
    "    print(f\"ü§ñ: {response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Temperature and Generation Parameters\n",
    "\n",
    "Control the creativity and randomness of responses:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat_with_options(model_name: str, message: str, temperature: float = 0.7, \n",
    "                     top_p: float = 0.9, top_k: int = 40):\n",
    "    \"\"\"Chat with custom generation parameters\"\"\"\n",
    "    try:\n",
    "        response = client.chat(\n",
    "            model=model_name,\n",
    "            messages=[{'role': 'user', 'content': message}],\n",
    "            options={\n",
    "                'temperature': temperature,\n",
    "                'top_p': top_p,\n",
    "                'top_k': top_k,\n",
    "            }\n",
    "        )\n",
    "        return response['message']['content']\n",
    "    except Exception as e:\n",
    "        return f\"Error: {e}\"\n",
    "\n",
    "# Compare different temperature settings\n",
    "if available_models:\n",
    "    question = \"Write a creative opening line for a science fiction story\"\n",
    "    temperatures = [0.1, 0.7, 1.2]\n",
    "    \n",
    "    print(f\"üå°Ô∏è Temperature Comparison for: '{question}'\\n\")\n",
    "    \n",
    "    for temp in temperatures:\n",
    "        print(f\"Temperature {temp} ({\n",
    "            'Conservative' if temp < 0.5 else \n",
    "            'Balanced' if temp < 1.0 else 'Creative'\n",
    "        }):\")\n",
    "        response = chat_with_options(available_models[0], question, temperature=temp)\n",
    "        print(f\"ü§ñ: {response}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Working with Embeddings\n",
    "\n",
    "Generate embeddings for text similarity and semantic search:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from typing import List\n",
    "\n",
    "def get_embeddings(model_name: str, texts: List[str]):\n",
    "    \"\"\"Get embeddings for a list of texts\"\"\"\n",
    "    embeddings = []\n",
    "    \n",
    "    for text in texts:\n",
    "        try:\n",
    "            response = client.embeddings(\n",
    "                model=model_name,\n",
    "                prompt=text\n",
    "            )\n",
    "            embeddings.append(response['embedding'])\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error getting embedding for '{text}': {e}\")\n",
    "            embeddings.append(None)\n",
    "    \n",
    "    return embeddings\n",
    "\n",
    "def cosine_similarity(vec1, vec2):\n",
    "    \"\"\"Calculate cosine similarity between two vectors\"\"\"\n",
    "    if vec1 is None or vec2 is None:\n",
    "        return 0.0\n",
    "    \n",
    "    vec1 = np.array(vec1)\n",
    "    vec2 = np.array(vec2)\n",
    "    \n",
    "    dot_product = np.dot(vec1, vec2)\n",
    "    norm1 = np.linalg.norm(vec1)\n",
    "    norm2 = np.linalg.norm(vec2)\n",
    "    \n",
    "    if norm1 == 0 or norm2 == 0:\n",
    "        return 0.0\n",
    "    \n",
    "    return dot_product / (norm1 * norm2)\n",
    "\n",
    "# Example: Text similarity\n",
    "if available_models:\n",
    "    # Note: Not all models support embeddings. This will work with models like nomic-embed-text\n",
    "    texts = [\n",
    "        \"The cat sat on the mat\",\n",
    "        \"A feline rested on the rug\",\n",
    "        \"Dogs are loyal animals\",\n",
    "        \"Python is a programming language\"\n",
    "    ]\n",
    "    \n",
    "    print(\"üîç Embedding Example (Note: Requires embedding-capable model):\\n\")\n",
    "    \n",
    "    # Try with the first available model (may not support embeddings)\n",
    "    try:\n",
    "        embeddings = get_embeddings(available_models[0], texts)\n",
    "        \n",
    "        if all(emb is not None for emb in embeddings):\n",
    "            print(\"üìä Text Similarity Matrix:\\n\")\n",
    "            \n",
    "            for i, text1 in enumerate(texts):\n",
    "                for j, text2 in enumerate(texts):\n",
    "                    if i <= j:  # Only show upper triangle\n",
    "                        similarity = cosine_similarity(embeddings[i], embeddings[j])\n",
    "                        print(f\"{i+1}-{j+1}: {similarity:.3f} | '{text1}' <-> '{text2}'\")\n",
    "        else:\n",
    "            print(\"‚ö†Ô∏è This model doesn't support embeddings. Try 'ollama pull nomic-embed-text'\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Embeddings not supported by this model: {e}\")\n",
    "        print(\"Try downloading an embedding model: ollama pull nomic-embed-text\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Error Handling and Best Practices {#best-practices}\n",
    "\n",
    "Let's implement robust error handling and best practices:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from functools import wraps\n",
    "import time\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "def retry_on_failure(max_retries: int = 3, delay: float = 1.0):\n",
    "    \"\"\"Decorator to retry function calls on failure\"\"\"\n",
    "    def decorator(func):\n",
    "        @wraps(func)\n",
    "        def wrapper(*args, **kwargs):\n",
    "            last_exception = None\n",
    "            \n",
    "            for attempt in range(max_retries):\n",
    "                try:\n",
    "                    return func(*args, **kwargs)\n",
    "                except Exception as e:\n",
    "                    last_exception = e\n",
    "                    logger.warning(f\"Attempt {attempt + 1} failed: {e}\")\n",
    "                    \n",
    "                    if attempt < max_retries - 1:\n",
    "                        time.sleep(delay * (2 ** attempt))  # Exponential backoff\n",
    "            \n",
    "            raise last_exception\n",
    "        return wrapper\n",
    "    return decorator\n",
    "\n",
    "class RobustOllamaClient:\n",
    "    def __init__(self, host: str = 'http://localhost:11434'):\n",
    "        self.client = ollama.Client(host=host)\n",
    "        self.host = host\n",
    "    \n",
    "    def is_server_available(self) -> bool:\n",
    "        \"\"\"Check if Ollama server is available\"\"\"\n",
    "        try:\n",
    "            self.client.list()\n",
    "            return True\n",
    "        except Exception:\n",
    "            return False\n",
    "    \n",
    "    def is_model_available(self, model_name: str) -> bool:\n",
    "        \"\"\"Check if a specific model is available\"\"\"\n",
    "        try:\n",
    "            models = self.client.list()\n",
    "            return any(model['name'] == model_name for model in models['models'])\n",
    "        except Exception:\n",
    "            return False\n",
    "    \n",
    "    @retry_on_failure(max_retries=3)\n",
    "    def safe_chat(self, model_name: str, messages: List[Dict], **kwargs) -> Dict:\n",
    "        \"\"\"Safe chat with error handling and retries\"\"\"\n",
    "        # Validate inputs\n",
    "        if not self.is_server_available():\n",
    "            raise ConnectionError(f\"Ollama server not available at {self.host}\")\n",
    "        \n",
    "        if not self.is_model_available(model_name):\n",
    "            raise ValueError(f\"Model '{model_name}' not available\")\n",
    "        \n",
    "        if not messages:\n",
    "            raise ValueError(\"Messages cannot be empty\")\n",
    "        \n",
    "        # Make the request\n",
    "        response = self.client.chat(\n",
    "            model=model_name,\n",
    "            messages=messages,\n",
    "            **kwargs\n",
    "        )\n",
    "        \n",
    "        return response\n",
    "    \n",
    "    def chat_with_timeout(self, model_name: str, message: str, timeout: int = 30) -> str:\n",
    "        \"\"\"Chat with timeout handling\"\"\"\n",
    "        import signal\n",
    "        \n",
    "        def timeout_handler(signum, frame):\n",
    "            raise TimeoutError(f\"Request timed out after {timeout} seconds\")\n",
    "        \n",
    "        # Set up timeout\n",
    "        signal.signal(signal.SIGALRM, timeout_handler)\n",
    "        signal.alarm(timeout)\n",
    "        \n",
    "        try:\n",
    "            response = self.safe_chat(\n",
    "                model_name,\n",
    "                [{'role': 'user', 'content': message}]\n",
    "            )\n",
    "            return response['message']['content']\n",
    "        finally:\n",
    "            signal.alarm(0)  # Cancel the alarm\n",
    "\n",
    "# Example usage\n",
    "robust_client = RobustOllamaClient()\n",
    "\n",
    "print(\"üõ°Ô∏è Robust Client Examples:\\n\")\n",
    "\n",
    "# Test server availability\n",
    "if robust_client.is_server_available():\n",
    "    print(\"‚úÖ Server is available\")\n",
    "    \n",
    "    if available_models:\n",
    "        model = available_models[0]\n",
    "        \n",
    "        # Test model availability\n",
    "        if robust_client.is_model_available(model):\n",
    "            print(f\"‚úÖ Model '{model}' is available\")\n",
    "            \n",
    "            # Safe chat example\n",
    "            try:\n",
    "                response = robust_client.safe_chat(\n",
    "                    model,\n",
    "                    [{'role': 'user', 'content': 'Hello, how are you?'}]\n",
    "                )\n",
    "                print(f\"ü§ñ: {response['message']['content']}\")\n",
    "            except Exception as e:\n",
    "                print(f\"‚ùå Safe chat failed: {e}\")\n",
    "        else:\n",
    "            print(f\"‚ùå Model '{model}' is not available\")\n",
    "else:\n",
    "    print(\"‚ùå Server is not available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input Validation and Sanitization\n",
    "\n",
    "Always validate and sanitize user inputs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from typing import Optional\n",
    "\n",
    "class InputValidator:\n",
    "    @staticmethod\n",
    "    def validate_message(message: str, max_length: int = 10000) -> str:\n",
    "        \"\"\"Validate and sanitize user message\"\"\"\n",
    "        if not isinstance(message, str):\n",
    "            raise TypeError(\"Message must be a string\")\n",
    "        \n",
    "        if not message.strip():\n",
    "            raise ValueError(\"Message cannot be empty\")\n",
    "        \n",
    "        if len(message) > max_length:\n",
    "            raise ValueError(f\"Message too long. Maximum {max_length} characters allowed\")\n",
    "        \n",
    "        # Remove potentially harmful characters\n",
    "        sanitized = re.sub(r'[\\x00-\\x08\\x0B\\x0C\\x0E-\\x1F\\x7F]', '', message)\n",
    "        \n",
    "        return sanitized.strip()\n",
    "    \n",
    "    @staticmethod\n",
    "    def validate_model_name(model_name: str) -> str:\n",
    "        \"\"\"Validate model name\"\"\"\n",
    "        if not isinstance(model_name, str):\n",
    "            raise TypeError(\"Model name must be a string\")\n",
    "        \n",
    "        if not model_name.strip():\n",
    "            raise ValueError(\"Model name cannot be empty\")\n",
    "        \n",
    "        # Basic model name validation (alphanumeric, hyphens, underscores, colons)\n",
    "        if not re.match(r'^[a-zA-Z0-9_:-]+$', model_name):\n",
    "            raise ValueError(\"Invalid model name format\")\n",
    "        \n",
    "        return model_name.strip()\n",
    "    \n",
    "    @staticmethod\n",
    "    def validate_temperature(temperature: float) -> float:\n",
    "        \"\"\"Validate temperature parameter\"\"\"\n",
    "        if not isinstance(temperature, (int, float)):\n",
    "            raise TypeError(\"Temperature must be a number\")\n",
    "        \n",
    "        if temperature < 0 or temperature > 2:\n",
    "            raise ValueError(\"Temperature must be between 0 and 2\")\n",
    "        \n",
    "        return float(temperature)\n",
    "\n",
    "def safe_chat_with_validation(model_name: str, message: str, \n",
    "                             temperature: float = 0.7) -> Optional[str]:\n",
    "    \"\"\"Chat function with comprehensive input validation\"\"\"\n",
    "    try:\n",
    "        # Validate inputs\n",
    "        validated_model = InputValidator.validate_model_name(model_name)\n",
    "        validated_message = InputValidator.validate_message(message)\n",
    "        validated_temp = InputValidator.validate_temperature(temperature)\n",
    "        \n",
    "        # Use robust client\n",
    "        response = robust_client.safe_chat(\n",
    "            validated_model,\n",
    "            [{'role': 'user', 'content': validated_message}],\n",
    "            options={'temperature': validated_temp}\n",
    "        )\n",
    "        \n",
    "        return response['message']['content']\n",
    "        \n",
    "    except (TypeError, ValueError) as e:\n",
    "        logger.error(f\"Validation error: {e}\")\n",
    "        return f\"Validation error: {e}\"\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Chat error: {e}\")\n",
    "        return f\"Chat error: {e}\"\n",
    "\n",
    "# Example usage\n",
    "if available_models:\n",
    "    print(\"üîí Validation Examples:\\n\")\n",
    "    \n",
    "    # Valid input\n",
    "    response = safe_chat_with_validation(\n",
    "        available_models[0],\n",
    "        \"What is the capital of France?\",\n",
    "        0.5\n",
    "    )\n",
    "    print(f\"‚úÖ Valid input: {response[:100]}...\\n\")\n",
    "    \n",
    "    # Invalid temperature\n",
    "    response = safe_chat_with_validation(\n",
    "        available_models[0],\n",
    "        \"Hello\",\n",
    "        5.0  # Invalid temperature\n",
    "    )\n",
    "    print(f\"‚ùå Invalid temperature: {response}\\n\")\n",
    "    \n",
    "    # Empty message\n",
    "    response = safe_chat_with_validation(\n",
    "        available_models[0],\n",
    "        \"\",  # Empty message\n",
    "        0.7\n",
    "    )\n",
    "    print(f\"‚ùå Empty message: {response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Real-world Examples {#examples}\n",
    "\n",
    "Let's create some practical, real-world examples:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 1: Document Summarizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DocumentSummarizer:\n",
    "    def __init__(self, model_name: str):\n",
    "        self.model_name = model_name\n",
    "        self.client = ollama.Client()\n",
    "    \n",
    "    def summarize(self, text: str, summary_type: str = \"brief\") -> str:\n",
    "        \"\"\"Summarize a document\"\"\"\n",
    "        prompts = {\n",
    "            \"brief\": \"Provide a brief 2-3 sentence summary of the following text:\",\n",
    "            \"detailed\": \"Provide a detailed summary with key points of the following text:\",\n",
    "            \"bullet\": \"Summarize the following text as bullet points:\"\n",
    "        }\n",
    "        \n",
    "        prompt = prompts.get(summary_type, prompts[\"brief\"])\n",
    "        full_prompt = f\"{prompt}\\n\\n{text}\"\n",
    "        \n",
    "        try:\n",
    "            response = self.client.chat(\n",
    "                model=self.model_name,\n",
    "                messages=[{'role': 'user', 'content': full_prompt}]\n",
    "            )\n",
    "            return response['message']['content']\n",
    "        except Exception as e:\n",
    "            return f\"Error: {e}\"\n",
    "    \n",
    "    def extract_key_points(self, text: str, num_points: int = 5) -> str:\n",
    "        \"\"\"Extract key points from text\"\"\"\n",
    "        prompt = f\"Extract the {num_points} most important key points from the following text:\\n\\n{text}\"\n",
    "        \n",
    "        try:\n",
    "            response = self.client.chat(\n",
    "                model=self.model_name,\n",
    "                messages=[{'role': 'user', 'content': prompt}]\n",
    "            )\n",
    "            return response['message']['content']\n",
    "        except Exception as e:\n",
    "            return f\"Error: {e}\"\n",
    "\n",
    "# Example usage\n",
    "if available_models:\n",
    "    summarizer = DocumentSummarizer(available_models[0])\n",
    "    \n",
    "    sample_text = \"\"\"\n",
    "    Artificial Intelligence (AI) has become one of the most transformative technologies of the 21st century. \n",
    "    It encompasses various subfields including machine learning, natural language processing, computer vision, \n",
    "    and robotics. Machine learning, in particular, has seen remarkable progress with the development of deep \n",
    "    learning algorithms that can process vast amounts of data and identify complex patterns. These advances \n",
    "    have led to breakthrough applications in healthcare, where AI can assist in medical diagnosis and drug \n",
    "    discovery, in transportation with autonomous vehicles, and in finance for fraud detection and algorithmic \n",
    "    trading. However, the rapid advancement of AI also raises important ethical considerations, including \n",
    "    concerns about job displacement, privacy, bias in algorithms, and the need for responsible AI development. \n",
    "    As we move forward, it's crucial to balance innovation with ethical considerations to ensure that AI \n",
    "    benefits society as a whole.\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"üìÑ Document Summarizer Example:\\n\")\n",
    "    print(\"Original text length:\", len(sample_text), \"characters\\n\")\n",
    "    \n",
    "    # Brief summary\n",
    "    brief = summarizer.summarize(sample_text, \"brief\")\n",
    "    print(f\"üìù Brief Summary:\\n{brief}\\n\")\n",
    "    \n",
    "    # Key points\n",
    "    key_points = summarizer.extract_key_points(sample_text, 3)\n",
    "    print(f\"üîë Key Points:\\n{key_points}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 2: Code Assistant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CodeAssistant:\n",
    "    def __init__(self, model_name: str):\n",
    "        self.model_name = model_name\n",
    "        self.client = ollama.Client()\n",
    "    \n",
    "    def explain_code(self, code: str, language: str = \"python\") -> str:\n",
    "        \"\"\"Explain what a piece of code does\"\"\"\n",
    "        prompt = f\"Explain what this {language} code does, line by line:\\n\\n```{language}\\n{code}\\n```\"\n",
    "        \n",
    "        try:\n",
    "            response = self.client.chat(\n",
    "                model=self.model_name,\n",
    "                messages=[{'role': 'user', 'content': prompt}]\n",
    "            )\n",
    "            return response['message']['content']\n",
    "        except Exception as e:\n",
    "            return f\"Error: {e}\"\n",
    "    \n",
    "    def debug_code(self, code: str, error_message: str, language: str = \"python\") -> str:\n",
    "        \"\"\"Help debug code with error message\"\"\"\n",
    "        prompt = f\"\"\"\n",
    "        I'm getting this error with my {language} code:\n",
    "        \n",
    "        Error: {error_message}\n",
    "        \n",
    "        Code:\n",
    "        ```{language}\n",
    "        {code}\n",
    "        ```\n",
    "        \n",
    "        Please help me identify the issue and suggest a fix.\n",
    "        \"\"\"\n",
    "        \n",
    "        try:\n",
    "            response = self.client.chat(\n",
    "                model=self.model_name,\n",
    "                messages=[{'role': 'user', 'content': prompt}]\n",
    "            )\n",
    "            return response['message']['content']\n",
    "        except Exception as e:\n",
    "            return f\"Error: {e}\"\n",
    "    \n",
    "    def optimize_code(self, code: str, language: str = \"python\") -> str:\n",
    "        \"\"\"Suggest optimizations for code\"\"\"\n",
    "        prompt = f\"\"\"\n",
    "        Please review this {language} code and suggest optimizations for better performance, \n",
    "        readability, or best practices:\n",
    "        \n",
    "        ```{language}\n",
    "        {code}\n",
    "        ```\n",
    "        \"\"\"\n",
    "        \n",
    "        try:\n",
    "            response = self.client.chat(\n",
    "                model=self.model_name,\n",
    "                messages=[{'role': 'user', 'content': prompt}]\n",
    "            )\n",
    "            return response['message']['content']\n",
    "        except Exception as e:\n",
    "            return f\"Error: {e}\"\n",
    "    \n",
    "    def generate_tests(self, code: str, language: str = \"python\") -> str:\n",
    "        \"\"\"Generate unit tests for code\"\"\"\n",
    "        prompt = f\"\"\"\n",
    "        Generate comprehensive unit tests for this {language} code:\n",
    "        \n",
    "        ```{language}\n",
    "        {code}\n",
    "        ```\n",
    "        \n",
    "        Include edge cases and error handling tests.\n",
    "        \"\"\"\n",
    "        \n",
    "        try:\n",
    "            response = self.client.chat(\n",
    "                model=self.model_name,\n",
    "                messages=[{'role': 'user', 'content': prompt}]\n",
    "            )\n",
    "            return response['message']['content']\n",
    "        except Exception as e:\n",
    "            return f\"Error: {e}\"\n",
    "\n",
    "# Example usage\n",
    "if available_models:\n",
    "    code_assistant = CodeAssistant(available_models[0])\n",
    "    \n",
    "    sample_code = \"\"\"\n",
    "def fibonacci(n):\n",
    "    if n <= 1:\n",
    "        return n\n",
    "    else:\n",
    "        return fibonacci(n-1) + fibonacci(n-2)\n",
    "\n",
    "result = fibonacci(10)\n",
    "print(result)\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"üíª Code Assistant Example:\\n\")\n",
    "    \n",
    "    # Explain code\n",
    "    explanation = code_assistant.explain_code(sample_code)\n",
    "    print(f\"üìñ Code Explanation:\\n{explanation}\\n\")\n",
    "    \n",
    "    # Optimize code\n",
    "    optimization = code_assistant.optimize_code(sample_code)\n",
    "    print(f\"‚ö° Optimization Suggestions:\\n{optimization}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 3: Multi-turn Conversation Manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "class ConversationManager:\n",
    "    def __init__(self, model_name: str, save_conversations: bool = True):\n",
    "        self.model_name = model_name\n",
    "        self.client = ollama.Client()\n",
    "        self.conversations = {}\n",
    "        self.save_conversations = save_conversations\n",
    "        self.save_dir = Path(\"conversations\")\n",
    "        \n",
    "        if save_conversations:\n",
    "            self.save_dir.mkdir(exist_ok=True)\n",
    "    \n",
    "    def start_conversation(self, conversation_id: str, system_prompt: str = None) -> str:\n",
    "        \"\"\"Start a new conversation\"\"\"\n",
    "        messages = []\n",
    "        if system_prompt:\n",
    "            messages.append({\n",
    "                'role': 'system',\n",
    "                'content': system_prompt\n",
    "            })\n",
    "        \n",
    "        self.conversations[conversation_id] = {\n",
    "            'messages': messages,\n",
    "            'created_at': datetime.now().isoformat(),\n",
    "            'model': self.model_name\n",
    "        }\n",
    "        \n",
    "        return f\"Started conversation '{conversation_id}' with {self.model_name}\"\n",
    "    \n",
    "    def send_message(self, conversation_id: str, message: str) -> str:\n",
    "        \"\"\"Send a message in a conversation\"\"\"\n",
    "        if conversation_id not in self.conversations:\n",
    "            return f\"Conversation '{conversation_id}' not found. Start it first.\"\n",
    "        \n",
    "        # Add user message\n",
    "        self.conversations[conversation_id]['messages'].append({\n",
    "            'role': 'user',\n",
    "            'content': message,\n",
    "            'timestamp': datetime.now().isoformat()\n",
    "        })\n",
    "        \n",
    "        try:\n",
    "            # Get response\n",
    "            response = self.client.chat(\n",
    "                model=self.model_name,\n",
    "                messages=[\n",
    "                    {k: v for k, v in msg.items() if k in ['role', 'content']}\n",
    "                    for msg in self.conversations[conversation_id]['messages']\n",
    "                ]\n",
    "            )\n",
    "            \n",
    "            assistant_message = response['message']['content']\n",
    "            \n",
    "            # Add assistant response\n",
    "            self.conversations[conversation_id]['messages'].append({\n",
    "                'role': 'assistant',\n",
    "                'content': assistant_message,\n",
    "                'timestamp': datetime.now().isoformat()\n",
    "            })\n",
    "            \n",
    "            # Save conversation\n",
    "            if self.save_conversations:\n",
    "                self._save_conversation(conversation_id)\n",
    "            \n",
    "            return assistant_message\n",
    "            \n",
    "        except Exception as e:\n",
    "            return f\"Error: {e}\"\n",
    "    \n",
    "    def get_conversation_summary(self, conversation_id: str) -> str:\n",
    "        \"\"\"Get a summary of the conversation\"\"\"\n",
    "        if conversation_id not in self.conversations:\n",
    "            return f\"Conversation '{conversation_id}' not found.\"\n",
    "        \n",
    "        conv = self.conversations[conversation_id]\n",
    "        message_count = len([msg for msg in conv['messages'] if msg['role'] in ['user', 'assistant']])\n",
    "        \n",
    "        return f\"\"\"\n",
    "Conversation: {conversation_id}\n",
    "Model: {conv['model']}\n",
    "Created: {conv['created_at']}\n",
    "Messages: {message_count}\n",
    "        \"\"\".strip()\n",
    "    \n",
    "    def _save_conversation(self, conversation_id: str):\n",
    "        \"\"\"Save conversation to file\"\"\"\n",
    "        filename = self.save_dir / f\"{conversation_id}.json\"\n",
    "        with open(filename, 'w') as f:\n",
    "            json.dump(self.conversations[conversation_id], f, indent=2)\n",
    "    \n",
    "    def load_conversation(self, conversation_id: str) -> str:\n",
    "        \"\"\"Load conversation from file\"\"\"\n",
    "        filename = self.save_dir / f\"{conversation_id}.json\"\n",
    "        \n",
    "        if not filename.exists():\n",
    "            return f\"Conversation file '{filename}' not found.\"\n",
    "        \n",
    "        try:\n",
    "            with open(filename, 'r') as f:\n",
    "                self.conversations[conversation_id] = json.load(f)\n",
    "            return f\"Loaded conversation '{conversation_id}'\"\n",
    "        except Exception as e:\n",
    "            return f\"Error loading conversation: {e}\"\n",
    "    \n",
    "    def list_conversations(self) -> List[str]:\n",
    "        \"\"\"List all active conversations\"\"\"\n",
    "        return list(self.conversations.keys())\n",
    "\n",
    "# Example usage\n",
    "if available_models:\n",
    "    conv_manager = ConversationManager(available_models[0])\n",
    "    \n",
    "    print(\"üí¨ Conversation Manager Example:\\n\")\n",
    "    \n",
    "    # Start a conversation with a system prompt\n",
    "    system_prompt = \"You are a helpful Python programming tutor. Always provide clear explanations and examples.\"\n",
    "    print(conv_manager.start_conversation(\"python_tutorial\", system_prompt))\n",
    "    \n",
    "    # Have a conversation\n",
    "    response1 = conv_manager.send_message(\"python_tutorial\", \"What are Python decorators?\")\n",
    "    print(f\"\\nü§ñ: {response1[:200]}...\\n\")\n",
    "    \n",
    "    response2 = conv_manager.send_message(\"python_tutorial\", \"Can you show me a simple example?\")\n",
    "    print(f\"ü§ñ: {response2[:200]}...\\n\")\n",
    "    \n",
    "    # Get conversation summary\n",
    "    summary = conv_manager.get_conversation_summary(\"python_tutorial\")\n",
    "    print(f\"üìä Summary:\\n{summary}\")\n",
    "    \n",
    "    # List conversations\n",
    "    conversations = conv_manager.list_conversations()\n",
    "    print(f\"\\nüìã Active conversations: {conversations}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This notebook has covered comprehensive usage of the Ollama Python API, including:\n",
    "\n",
    "- ‚úÖ Basic setup and configuration\n",
    "- ‚úÖ Simple and advanced chat interactions\n",
    "- ‚úÖ Working with different models\n",
    "- ‚úÖ Streaming responses\n",
    "- ‚úÖ Advanced features (system prompts, parameters, embeddings)\n",
    "- ‚úÖ Error handling and best practices\n",
    "- ‚úÖ Real-world examples (document summarizer, code assistant, conversation manager)\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "1. **Experiment** with different models and parameters\n",
    "2. **Build** your own applications using these patterns\n",
    "3. **Explore** additional Ollama features and models\n",
    "4. **Implement** proper logging and monitoring for production use\n",
    "5. **Consider** rate limiting and resource management for high-volume applications\n",
    "\n",
    "### Useful Resources\n",
    "\n",
    "- [Ollama Documentation](https://ollama.ai/)\n",
    "- [Ollama Python Library](https://github.com/ollama/ollama-python)\n",
    "- [Available Models](https://ollama.ai/library)\n",
    "- [Model Cards and Documentation](https://ollama.ai/library)\n",
    "\n",
    "Happy coding with Ollama! üöÄ"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
