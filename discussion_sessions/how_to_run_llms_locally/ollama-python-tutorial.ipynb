{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aa637f02",
   "metadata": {},
   "source": [
    "# üöÄ Ollama Python API Tutorial\n",
    "\n",
    "**A comprehensive guide for the Slashdot Programming Club - IISER Kolkata**\n",
    "\n",
    "Welcome to this hands-on tutorial where you'll learn to harness the power of Ollama's Python API for building intelligent applications!\n",
    "\n",
    "## üéØ What You'll Learn\n",
    "\n",
    "- Setting up and connecting to Ollama\n",
    "- Making basic API calls\n",
    "- Implementing chat functionality\n",
    "- Building a conversational AI assistant\n",
    "- Advanced features like streaming responses\n",
    "- Best practices and error handling\n",
    "\n",
    "## üìã Prerequisites\n",
    "\n",
    "- Python 3.7+\n",
    "- Ollama installed and running\n",
    "- Basic understanding of Python\n",
    "- A curious mind! üß†"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db2acaf8",
   "metadata": {},
   "source": [
    "## üõ†Ô∏è Step 1: Environment Setup\n",
    "\n",
    "First, let's install the required packages and verify our setup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2be60b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install requests aiohttp python-dotenv rich\n",
    "\n",
    "# Optional: For advanced features\n",
    "!pip install asyncio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac2f5b16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import essential libraries\n",
    "import requests\n",
    "import json\n",
    "import asyncio\n",
    "import aiohttp\n",
    "from typing import List, Dict, Optional, AsyncGenerator\n",
    "from rich.console import Console\n",
    "from rich.panel import Panel\n",
    "from rich.text import Text\n",
    "from rich.markdown import Markdown\n",
    "\n",
    "# Initialize Rich console for beautiful output\n",
    "console = Console()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c03af53",
   "metadata": {},
   "source": [
    "## üîó Step 2: Basic Connection & Health Check\n",
    "\n",
    "Let's start by verifying that Ollama is running and accessible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bc33074",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ollama API Configuration\n",
    "OLLAMA_BASE_URL = \"http://localhost:11434\"\n",
    "\n",
    "def check_ollama_status():\n",
    "    \"\"\"Check if Ollama server is running and accessible.\"\"\"\n",
    "    try:\n",
    "        response = requests.get(f\"{OLLAMA_BASE_URL}/api/tags\", timeout=5)\n",
    "        if response.status_code == 200:\n",
    "            console.print(\"‚úÖ Ollama server is running!\", style=\"bold green\")\n",
    "            return True\n",
    "        else:\n",
    "            console.print(f\"‚ùå Ollama server returned status code: {response.status_code}\", style=\"red\")\n",
    "            return False\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        console.print(f\"‚ùå Failed to connect to Ollama: {e}\", style=\"red\")\n",
    "        return False\n",
    "\n",
    "# Test the connection\n",
    "check_ollama_status()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4da32f3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_available_models():\n",
    "    \"\"\"Fetch list of available models from Ollama.\"\"\"\n",
    "    try:\n",
    "        response = requests.get(f\"{OLLAMA_BASE_URL}/api/tags\")\n",
    "        if response.status_code == 200:\n",
    "            models_data = response.json()\n",
    "            models = [model['name'] for model in models_data.get('models', [])]\n",
    "            \n",
    "            console.print(Panel(\n",
    "                f\"üìö Available Models ({len(models)}): {', '.join(models) if models else 'None'}\",\n",
    "                title=\"ü§ñ Ollama Models\",\n",
    "                border_style=\"cyan\"\n",
    "            ))\n",
    "            return models\n",
    "        else:\n",
    "            console.print(f\"‚ùå Failed to fetch models: {response.status_code}\", style=\"red\")\n",
    "            return []\n",
    "    except Exception as e:\n",
    "        console.print(f\"‚ùå Error fetching models: {e}\", style=\"red\")\n",
    "        return []\n",
    "\n",
    "# Get available models\n",
    "available_models = get_available_models()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcd9e3cf",
   "metadata": {},
   "source": [
    "## üí¨ Step 3: Basic Generate API\n",
    "\n",
    "The Generate API is the simplest way to get completions from Ollama models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "242ba99e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OllamaGenerator:\n",
    "    \"\"\"Simple wrapper for Ollama Generate API.\"\"\"\n",
    "    \n",
    "    def __init__(self, base_url: str = \"http://localhost:11434\"):\n",
    "        self.base_url = base_url\n",
    "        self.endpoint = f\"{base_url}/api/generate\"\n",
    "    \n",
    "    def generate(self, model: str, prompt: str, **kwargs) -> str:\n",
    "        \"\"\"Generate text using the specified model and prompt.\"\"\"\n",
    "        \n",
    "        payload = {\n",
    "            \"model\": model,\n",
    "            \"prompt\": prompt,\n",
    "            \"stream\": False,  # Get complete response at once\n",
    "            **kwargs\n",
    "        }\n",
    "        \n",
    "        try:\n",
    "            console.print(f\"ü§î Thinking with {model}...\", style=\"yellow\")\n",
    "            \n",
    "            response = requests.post(\n",
    "                self.endpoint,\n",
    "                json=payload,\n",
    "                headers={\"Content-Type\": \"application/json\"}\n",
    "            )\n",
    "            \n",
    "            if response.status_code == 200:\n",
    "                result = response.json()\n",
    "                generated_text = result.get('response', '')\n",
    "                \n",
    "                # Display the result beautifully\n",
    "                console.print(Panel(\n",
    "                    generated_text,\n",
    "                    title=f\"ü§ñ {model} Response\",\n",
    "                    border_style=\"green\"\n",
    "                ))\n",
    "                \n",
    "                return generated_text\n",
    "            else:\n",
    "                console.print(f\"‚ùå API Error: {response.status_code}\", style=\"red\")\n",
    "                return \"\"\n",
    "                \n",
    "        except Exception as e:\n",
    "            console.print(f\"‚ùå Generation failed: {e}\", style=\"red\")\n",
    "            return \"\"\n",
    "\n",
    "# Example usage\n",
    "if available_models:\n",
    "    generator = OllamaGenerator()\n",
    "    \n",
    "    # Try with the first available model\n",
    "    model_name = available_models[0]\n",
    "    prompt = \"Write a haiku about programming:\"\n",
    "    \n",
    "    result = generator.generate(model_name, prompt)\n",
    "else:\n",
    "    console.print(\"‚ö†Ô∏è No models available. Download one with 'ollama pull llama2'\", style=\"yellow\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d772e1a3",
   "metadata": {},
   "source": [
    "## üí¨ Step 4: Chat API - Conversational Interface\n",
    "\n",
    "The Chat API is more powerful for conversations as it maintains context between messages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c784a4de",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OllamaChat:\n",
    "    \"\"\"Advanced chat interface with conversation memory.\"\"\"\n",
    "    \n",
    "    def __init__(self, model: str, base_url: str = \"http://localhost:11434\"):\n",
    "        self.model = model\n",
    "        self.base_url = base_url\n",
    "        self.endpoint = f\"{base_url}/api/chat\"\n",
    "        self.messages = []  # Conversation history\n",
    "        self.system_prompt = None\n",
    "    \n",
    "    def set_system_prompt(self, prompt: str):\n",
    "        \"\"\"Set the system prompt that defines the AI's behavior.\"\"\"\n",
    "        self.system_prompt = prompt\n",
    "        console.print(f\"üé≠ System prompt set: {prompt[:50]}...\", style=\"blue\")\n",
    "    \n",
    "    def chat(self, user_message: str, **kwargs) -> str:\n",
    "        \"\"\"Send a message and get a response while maintaining context.\"\"\"\n",
    "        \n",
    "        # Prepare messages\n",
    "        messages = []\n",
    "        \n",
    "        # Add system prompt if set\n",
    "        if self.system_prompt:\n",
    "            messages.append({\n",
    "                \"role\": \"system\",\n",
    "                \"content\": self.system_prompt\n",
    "            })\n",
    "        \n",
    "        # Add conversation history\n",
    "        messages.extend(self.messages)\n",
    "        \n",
    "        # Add current user message\n",
    "        messages.append({\n",
    "            \"role\": \"user\",\n",
    "            \"content\": user_message\n",
    "        })\n",
    "        \n",
    "        payload = {\n",
    "            \"model\": self.model,\n",
    "            \"messages\": messages,\n",
    "            \"stream\": False,\n",
    "            **kwargs\n",
    "        }\n",
    "        \n",
    "        try:\n",
    "            console.print(f\"üí≠ You: {user_message}\", style=\"cyan\")\n",
    "            console.print(f\"ü§î {self.model} is thinking...\", style=\"yellow\")\n",
    "            \n",
    "            response = requests.post(\n",
    "                self.endpoint,\n",
    "                json=payload,\n",
    "                headers={\"Content-Type\": \"application/json\"}\n",
    "            )\n",
    "            \n",
    "            if response.status_code == 200:\n",
    "                result = response.json()\n",
    "                ai_message = result['message']['content']\n",
    "                \n",
    "                # Add to conversation history\n",
    "                self.messages.append({\"role\": \"user\", \"content\": user_message})\n",
    "                self.messages.append({\"role\": \"assistant\", \"content\": ai_message})\n",
    "                \n",
    "                # Display response\n",
    "                console.print(Panel(\n",
    "                    ai_message,\n",
    "                    title=f\"ü§ñ {self.model}\",\n",
    "                    border_style=\"green\"\n",
    "                ))\n",
    "                \n",
    "                return ai_message\n",
    "            else:\n",
    "                console.print(f\"‚ùå Chat API Error: {response.status_code}\", style=\"red\")\n",
    "                return \"\"\n",
    "                \n",
    "        except Exception as e:\n",
    "            console.print(f\"‚ùå Chat failed: {e}\", style=\"red\")\n",
    "            return \"\"\n",
    "    \n",
    "    def clear_history(self):\n",
    "        \"\"\"Clear the conversation history.\"\"\"\n",
    "        self.messages = []\n",
    "        console.print(\"üßπ Conversation history cleared!\", style=\"blue\")\n",
    "    \n",
    "    def get_conversation_length(self) -> int:\n",
    "        \"\"\"Get the number of message exchanges.\"\"\"\n",
    "        return len(self.messages) // 2\n",
    "\n",
    "# Example: Create a helpful programming assistant\n",
    "if available_models:\n",
    "    chat = OllamaChat(model=available_models[0])\n",
    "    \n",
    "    # Set up the AI as a programming tutor\n",
    "    chat.set_system_prompt(\n",
    "        \"You are a friendly and knowledgeable programming tutor for the Slashdot Programming Club at IISER Kolkata. \"\n",
    "        \"Help students learn programming concepts, debug code, and explore new technologies. \"\n",
    "        \"Always be encouraging and provide practical examples.\"\n",
    "    )\n",
    "    \n",
    "    # Start a conversation\n",
    "    response1 = chat.chat(\"Hi! Can you explain what an API is in simple terms?\")\n",
    "    \n",
    "    # Follow up question (context is maintained)\n",
    "    response2 = chat.chat(\"Can you give me a Python example of using an API?\")\n",
    "    \n",
    "    console.print(f\"\\nüìä Conversation length: {chat.get_conversation_length()} exchanges\", style=\"blue\")\n",
    "else:\n",
    "    console.print(\"‚ö†Ô∏è No models available for chat demo.\", style=\"yellow\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15e2a495",
   "metadata": {},
   "source": [
    "## üåä Step 5: Streaming Responses\n",
    "\n",
    "For longer responses, streaming provides a better user experience by showing text as it's generated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1353654",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "class OllamaStreamer:\n",
    "    \"\"\"Handle streaming responses from Ollama.\"\"\"\n",
    "    \n",
    "    def __init__(self, base_url: str = \"http://localhost:11434\"):\n",
    "        self.base_url = base_url\n",
    "    \n",
    "    def stream_generate(self, model: str, prompt: str, **kwargs):\n",
    "        \"\"\"Stream text generation token by token.\"\"\"\n",
    "        \n",
    "        payload = {\n",
    "            \"model\": model,\n",
    "            \"prompt\": prompt,\n",
    "            \"stream\": True,\n",
    "            **kwargs\n",
    "        }\n",
    "        \n",
    "        try:\n",
    "            console.print(f\"üåä Streaming response from {model}...\", style=\"yellow\")\n",
    "            console.print(\"\\n\" + \"=\"*50, style=\"blue\")\n",
    "            \n",
    "            response = requests.post(\n",
    "                f\"{self.base_url}/api/generate\",\n",
    "                json=payload,\n",
    "                headers={\"Content-Type\": \"application/json\"},\n",
    "                stream=True\n",
    "            )\n",
    "            \n",
    "            full_response = \"\"\n",
    "            \n",
    "            for line in response.iter_lines():\n",
    "                if line:\n",
    "                    try:\n",
    "                        data = json.loads(line.decode('utf-8'))\n",
    "                        if 'response' in data:\n",
    "                            token = data['response']\n",
    "                            full_response += token\n",
    "                            print(token, end='', flush=True)\n",
    "                            \n",
    "                            # Add slight delay for visual effect\n",
    "                            time.sleep(0.02)\n",
    "                            \n",
    "                        if data.get('done', False):\n",
    "                            break\n",
    "                    except json.JSONDecodeError:\n",
    "                        continue\n",
    "            \n",
    "            console.print(\"\\n\" + \"=\"*50, style=\"blue\")\n",
    "            console.print(\"‚úÖ Streaming complete!\", style=\"green\")\n",
    "            return full_response\n",
    "            \n",
    "        except Exception as e:\n",
    "            console.print(f\"‚ùå Streaming failed: {e}\", style=\"red\")\n",
    "            return \"\"\n",
    "\n",
    "# Example: Stream a longer response\n",
    "if available_models:\n",
    "    streamer = OllamaStreamer()\n",
    "    \n",
    "    prompt = \"Write a detailed explanation of how machine learning works, including key concepts and real-world applications:\"\n",
    "    \n",
    "    result = streamer.stream_generate(available_models[0], prompt)\n",
    "else:\n",
    "    console.print(\"‚ö†Ô∏è No models available for streaming demo.\", style=\"yellow\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db59851b",
   "metadata": {},
   "source": [
    "## ‚ö° Step 6: Async Operations\n",
    "\n",
    "For applications that need to handle multiple requests concurrently, async operations are essential."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "866dd452",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AsyncOllamaClient:\n",
    "    \"\"\"Asynchronous Ollama client for concurrent operations.\"\"\"\n",
    "    \n",
    "    def __init__(self, base_url: str = \"http://localhost:11434\"):\n",
    "        self.base_url = base_url\n",
    "    \n",
    "    async def async_generate(self, session: aiohttp.ClientSession, model: str, prompt: str, **kwargs) -> str:\n",
    "        \"\"\"Generate text asynchronously.\"\"\"\n",
    "        \n",
    "        payload = {\n",
    "            \"model\": model,\n",
    "            \"prompt\": prompt,\n",
    "            \"stream\": False,\n",
    "            **kwargs\n",
    "        }\n",
    "        \n",
    "        try:\n",
    "            async with session.post(\n",
    "                f\"{self.base_url}/api/generate\",\n",
    "                json=payload,\n",
    "                headers={\"Content-Type\": \"application/json\"}\n",
    "            ) as response:\n",
    "                if response.status == 200:\n",
    "                    result = await response.json()\n",
    "                    return result.get('response', '')\n",
    "                else:\n",
    "                    return f\"Error: {response.status}\"\n",
    "        except Exception as e:\n",
    "            return f\"Exception: {e}\"\n",
    "    \n",
    "    async def batch_generate(self, model: str, prompts: List[str]) -> List[str]:\n",
    "        \"\"\"Generate responses for multiple prompts concurrently.\"\"\"\n",
    "        \n",
    "        async with aiohttp.ClientSession() as session:\n",
    "            tasks = [\n",
    "                self.async_generate(session, model, prompt)\n",
    "                for prompt in prompts\n",
    "            ]\n",
    "            \n",
    "            console.print(f\"üöÄ Running {len(tasks)} tasks concurrently...\", style=\"yellow\")\n",
    "            \n",
    "            start_time = time.time()\n",
    "            results = await asyncio.gather(*tasks)\n",
    "            end_time = time.time()\n",
    "            \n",
    "            console.print(f\"‚úÖ Completed {len(tasks)} tasks in {end_time - start_time:.2f} seconds\", style=\"green\")\n",
    "            \n",
    "            return results\n",
    "\n",
    "# Example: Concurrent text generation\n",
    "async def demo_async_operations():\n",
    "    if not available_models:\n",
    "        console.print(\"‚ö†Ô∏è No models available for async demo.\", style=\"yellow\")\n",
    "        return\n",
    "    \n",
    "    client = AsyncOllamaClient()\n",
    "    \n",
    "    # Multiple prompts to process concurrently\n",
    "    prompts = [\n",
    "        \"Explain recursion in one sentence:\",\n",
    "        \"What is the difference between a list and a tuple in Python?\",\n",
    "        \"Write a simple Python function to check if a number is prime:\",\n",
    "        \"What are the benefits of using Git for version control?\"\n",
    "    ]\n",
    "    \n",
    "    results = await client.batch_generate(available_models[0], prompts)\n",
    "    \n",
    "    # Display results\n",
    "    for i, (prompt, result) in enumerate(zip(prompts, results), 1):\n",
    "        console.print(Panel(\n",
    "            f\"**Question:** {prompt}\\n\\n**Answer:** {result}\",\n",
    "            title=f\"ü§ñ Result {i}\",\n",
    "            border_style=\"cyan\"\n",
    "        ))\n",
    "\n",
    "# Run the async demo\n",
    "await demo_async_operations()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1d54224",
   "metadata": {},
   "source": [
    "## üõ°Ô∏è Step 7: Error Handling & Best Practices\n",
    "\n",
    "Production-ready code needs robust error handling and configuration management."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5104303a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RobustOllamaClient:\n",
    "    \"\"\"Production-ready Ollama client with comprehensive error handling.\"\"\"\n",
    "    \n",
    "    def __init__(self, base_url: str = \"http://localhost:11434\", \n",
    "                 timeout: int = 30, max_retries: int = 3):\n",
    "        self.base_url = base_url.rstrip('/')\n",
    "        self.timeout = timeout\n",
    "        self.max_retries = max_retries\n",
    "        self.session = requests.Session()\n",
    "        \n",
    "        # Set default headers\n",
    "        self.session.headers.update({\n",
    "            'Content-Type': 'application/json',\n",
    "            'User-Agent': 'Slashdot-Ollama-Client/1.0'\n",
    "        })\n",
    "    \n",
    "    def _make_request(self, endpoint: str, payload: dict) -> dict:\n",
    "        \"\"\"Make a request with retry logic and error handling.\"\"\"\n",
    "        \n",
    "        url = f\"{self.base_url}{endpoint}\"\n",
    "        last_error = None\n",
    "        \n",
    "        for attempt in range(self.max_retries):\n",
    "            try:\n",
    "                console.print(f\"üîÑ Attempt {attempt + 1}/{self.max_retries}\", style=\"blue\")\n",
    "                \n",
    "                response = self.session.post(\n",
    "                    url,\n",
    "                    json=payload,\n",
    "                    timeout=self.timeout\n",
    "                )\n",
    "                \n",
    "                if response.status_code == 200:\n",
    "                    return response.json()\n",
    "                elif response.status_code == 404:\n",
    "                    raise ValueError(f\"Model '{payload.get('model')}' not found\")\n",
    "                elif response.status_code == 500:\n",
    "                    raise RuntimeError(\"Internal server error - check Ollama logs\")\n",
    "                else:\n",
    "                    raise RuntimeError(f\"HTTP {response.status_code}: {response.text}\")\n",
    "                    \n",
    "            except requests.exceptions.Timeout:\n",
    "                last_error = f\"Request timed out after {self.timeout} seconds\"\n",
    "                console.print(f\"‚è∞ {last_error}\", style=\"yellow\")\n",
    "                \n",
    "            except requests.exceptions.ConnectionError:\n",
    "                last_error = \"Cannot connect to Ollama server\"\n",
    "                console.print(f\"üîå {last_error}\", style=\"yellow\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                last_error = str(e)\n",
    "                console.print(f\"‚ùå {last_error}\", style=\"red\")\n",
    "                \n",
    "            if attempt < self.max_retries - 1:\n",
    "                wait_time = 2 ** attempt  # Exponential backoff\n",
    "                console.print(f\"‚è≥ Waiting {wait_time}s before retry...\", style=\"blue\")\n",
    "                time.sleep(wait_time)\n",
    "        \n",
    "        raise RuntimeError(f\"All {self.max_retries} attempts failed. Last error: {last_error}\")\n",
    "    \n",
    "    def safe_generate(self, model: str, prompt: str, **kwargs) -> Optional[str]:\n",
    "        \"\"\"Generate text with comprehensive error handling.\"\"\"\n",
    "        \n",
    "        # Validate inputs\n",
    "        if not model or not prompt:\n",
    "            console.print(\"‚ùå Model and prompt are required\", style=\"red\")\n",
    "            return None\n",
    "        \n",
    "        # Prepare payload with defaults\n",
    "        payload = {\n",
    "            \"model\": model,\n",
    "            \"prompt\": prompt,\n",
    "            \"stream\": False,\n",
    "            \"options\": {\n",
    "                \"temperature\": kwargs.get('temperature', 0.7),\n",
    "                \"top_p\": kwargs.get('top_p', 0.9),\n",
    "                \"max_tokens\": kwargs.get('max_tokens', 1000)\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        try:\n",
    "            console.print(f\"üéØ Generating with {model}...\", style=\"cyan\")\n",
    "            \n",
    "            result = self._make_request(\"/api/generate\", payload)\n",
    "            \n",
    "            response_text = result.get('response', '')\n",
    "            \n",
    "            # Log generation stats\n",
    "            stats = {\n",
    "                'eval_count': result.get('eval_count', 0),\n",
    "                'eval_duration': result.get('eval_duration', 0),\n",
    "                'prompt_eval_count': result.get('prompt_eval_count', 0)\n",
    "            }\n",
    "            \n",
    "            console.print(Panel(\n",
    "                f\"Generated {stats['eval_count']} tokens in {stats['eval_duration'] / 1e9:.2f}s\",\n",
    "                title=\"üìä Generation Stats\",\n",
    "                border_style=\"green\"\n",
    "            ))\n",
    "            \n",
    "            return response_text\n",
    "            \n",
    "        except Exception as e:\n",
    "            console.print(f\"üí• Generation failed: {e}\", style=\"red\")\n",
    "            return None\n",
    "    \n",
    "    def __del__(self):\n",
    "        \"\"\"Clean up session on deletion.\"\"\"\n",
    "        if hasattr(self, 'session'):\n",
    "            self.session.close()\n",
    "\n",
    "# Example: Robust client usage\n",
    "if available_models:\n",
    "    robust_client = RobustOllamaClient(\n",
    "        timeout=15,\n",
    "        max_retries=2\n",
    "    )\n",
    "    \n",
    "    # Test with various scenarios\n",
    "    test_prompts = [\n",
    "        \"Explain quantum computing in simple terms:\",\n",
    "        \"Write a Python function to reverse a string:\",\n",
    "        \"What are the key principles of good software design?\"\n",
    "    ]\n",
    "    \n",
    "    for prompt in test_prompts:\n",
    "        result = robust_client.safe_generate(\n",
    "            model=available_models[0],\n",
    "            prompt=prompt,\n",
    "            temperature=0.7\n",
    "        )\n",
    "        \n",
    "        if result:\n",
    "            console.print(Panel(\n",
    "                result[:200] + \"...\" if len(result) > 200 else result,\n",
    "                title=f\"‚úÖ Response Preview\",\n",
    "                border_style=\"green\"\n",
    "            ))\n",
    "        else:\n",
    "            console.print(\"‚ùå Failed to generate response\", style=\"red\")\n",
    "        \n",
    "        console.print(\"‚îÄ\" * 50, style=\"dim\")\n",
    "else:\n",
    "    console.print(\"‚ö†Ô∏è No models available for robust client demo.\", style=\"yellow\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38c87d0f",
   "metadata": {},
   "source": [
    "## üéØ Step 8: Putting It All Together - Interactive Demo\n",
    "\n",
    "Let's create a final interactive demonstration that showcases all the features we've learned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a95cb0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final demonstration\n",
    "if available_models:\n",
    "    console.print(\"\\nüéÆ Quick Demo - Ask the AI a question!\", style=\"bold cyan\")\n",
    "    \n",
    "    demo_chat = OllamaChat(model=available_models[0])\n",
    "    demo_chat.set_system_prompt(\n",
    "        \"You are a helpful AI assistant for the Slashdot Programming Club at IISER Kolkata. \"\n",
    "        \"Be friendly, informative, and encourage learning about programming and technology.\"\n",
    "    )\n",
    "    \n",
    "    demo_response = demo_chat.chat(\"What's the most exciting thing about AI programming?\")\n",
    "    \n",
    "    console.print(\"\\n‚ú® This concludes our comprehensive Ollama Python tutorial!\", style=\"bold green\")\n",
    "    console.print(\"You now have all the tools to build amazing AI applications!\", style=\"green\")\n",
    "else:\n",
    "    console.print(\"\\n‚ö†Ô∏è No models available for demo.\", style=\"yellow\")\n",
    "    console.print(\"Download a model with 'ollama pull llama2' to try the examples!\", style=\"cyan\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
